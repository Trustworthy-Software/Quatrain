Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-Math-34-HDRepair$$Fix ListPopulation . iterator ( )$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-HDRepair$$fixed erroneous loop$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-HDRepair$$Fix NaN - > org . apache . commons . math3 . complex . Complex$$1
Math-22$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time. The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned. [1] http://markmail.org/message/dxuxh7eybl7xejde$$patch1-Math-22-HDRepair$$Fix FDistribution and UniformRealDistribution . isSupportLowerBoundInclusive ( )$$1
Time-19$$Inconsistent interpretation of ambiguous time during DST$$The inconsistency appears for timezone Europe/London.  These three DateTime objects should all represent the same moment in time even if they are ambiguous. Now, it always returns the earlier instant (summer time) during an overlap.$$patch1-Time-19-HDRepair$$fixed typo .$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-HDRepair$$Use availableLocaleList ( ) instead of cAvailableLocaleSet$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-HDRepair$$Fix warning on int precision thingie in BaseSecantSolver$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-HDRepair$$Fix a numeric equality bug in UnivariateRealSolverUtils$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-HDRepair$$I had left it harwired to the edge ( and also in the inverse ) .$$0
Closure-61$$Closure removes needed code.$$None$$patch1-Closure-61-Developer$$Fix regex global references in Closure metadata$$1
Closure-95$$Use @public tag to prevent compression of symbol names$$None$$patch1-Closure-95-Developer$$fix this one .$$1
Closure-132$$if statement$$None$$patch1-Closure-132-Developer$$don ' t let the receiver have side effects on the left$$1
Closure-59$$Cannot exclude globalThis checks through command line$$None$$patch1-Closure-59-Developer$$"Revert "" Enable check for globalThis level """$$1
Closure-92$$bug with implicit namespaces across modules$$None$$patch1-Closure-92-Developer$$Fixing the warning for invalid symbol indexing .$$1
Closure-66$$@enum does not type correctly$$None$$patch1-Closure-66-Developer$$Set typeable = false for OBJECTLIT key nodes$$1
Closure-104$$Typos in externs/html5.js$$None$$patch1-Closure-104-Developer$$Fix warning$$1
Closure-50$$Optimisation: convert array.join(",") to array.join()$$None$$patch1-Closure-50-Developer$$added code change notification$$1
Closure-68$$Cryptic error message on invalid "@type function" annotation$$None$$patch1-Closure-68-Developer$$Restore look ahead indicator for TypeParameters$$1
Closure-103$$Compiler gives false error with respect to unreachable code$$None$$patch1-Closure-103-Developer$$Fix false positives in closure analysis$$1
Closure-57$$compiler crashes when  goog.provide used with non string$$None$$patch1-Closure-57-Developer$$Fix warning$$1
Closure-32$$Preserve doesn't preserve whitespace at start of line$$None$$patch1-Closure-32-Developer$$Reset lineStartChar after the * - 1 or * the * - 1 case .$$1
Closure-35$$assignment to object in conditional causes type error on function w/ record type return type$$None$$patch1-Closure-35-Developer$$Fix closure type inference$$1
Closure-102$$compiler assumes that 'arguments' can be shadowed$$None$$patch1-Closure-102-Developer$$Fixing the closure compiler warnings$$1
Closure-69$$Compiler should warn/error when instance methods are operated on$$None$$patch1-Closure-69-Developer$$Fix reported error in TypeCheck$$1
Closure-56$$Last warning or error in output is truncated$$None$$patch1-Closure-56-Developer$$fix possible NPE at startup$$1
Closure-105$$Array Join Munged Incorrectly$$None$$patch1-Closure-105-Developer$$Allow null string input$$1
Closure-51$$-0.0 becomes 0 even in whitespace mode$$None$$patch1-Closure-51-Developer$$Fix comparison to non - negative numbers$$1
Closure-58$$Online CC bug: report java error.$$None$$patch1-Closure-58-Developer$$Fix closure bit .$$1
Closure-133$$Exception when parsing erroneous jsdoc: /**@return {@code foo} bar   *    baz. */$$None$$patch1-Closure-133-Developer$$Reset unread token on getRemainingJSDocLine ( )$$1
Closure-67$$Advanced compilations renames a function and then deletes it, leaving a reference to a renamed but non-existent function$$None$$patch1-Closure-67-Developer$$Add an empty line$$1
Closure-93$$None$$None$$patch1-Closure-93-Developer$$Fixing the build .$$1
Closure-94$$closure-compiler @define annotation does not allow line to be split on 80 characters.$$None$$patch1-Closure-94-Developer$$Added some known valid characters .$$1
Closure-60$$void function () {}(); wrongly identified as having no side effects$$None$$patch1-Closure-60-Developer$$fixing side - effects in boolean - literals$$1
Closure-34$$StackOverflowError exception when running closure compiler (javascript attached)$$None$$patch1-Closure-34-Developer$$Fix unrolling of operators in Closure_34$$1
Closure-33$$weird object literal invalid property error on unrelated object prototype$$None$$patch1-Closure-33-Developer$$Handle anonymous constraints$$1
Closure-20$$String conversion optimization is incorrect$$None$$patch1-Closure-20-Developer$$closure compiler doesn ' t likeImmutableValue ( ) for inline code$$1
Closure-18$$Dependency sorting with closurePass set to false no longer works.$$None$$patch1-Closure-18-Developer$$fix the marking of dead code$$1
Closure-27$$Error trying to build try-catch block (AST)$$None$$patch1-Closure-27-Developer$$Updated information$$1
Closure-9$$Compiler fails to find amd module in a subdirectory$$None$$patch1-Closure-9-Developer$$Fix a warning$$1
Closure-11$$Record type invalid property not reported on function with @this annotation$$None$$patch1-Closure-11-Developer$$Fix warning$$1
Closure-7$$Bad type inference with goog.isFunction and friends$$None$$patch1-Closure-7-Developer$$Don ' t filter out subtypes of "" function "" unless the receiver type is a non -$$1
Closure-29$$closure compiler screws up a perfectly valid isFunction() implementation$$None$$patch1-Closure-29-Developer$$Added a set of properties that are valid on object literals$$1
Closure-16$$JSCompiler does not recursively resolve typedefs$$None$$patch1-Closure-16-Developer$$Fixing an warning$$1
Closure-129$$Casting a function before calling it produces bad code and breaks plugin code$$None$$patch1-Closure-129-Developer$$Fixing cruft$$1
Closure-42$$Simple "Whitespace only" compression removing "each" keyword from "for each (var x in arr)" loop$$None$$patch1-Closure-42-Developer$$said error reporter to have been logging for the for - each loop extension .$$1
Closure-89$$Compiler removes function properties that it should not$$None$$patch1-Closure-89-Developer$$fix # 1862$$1
Closure-116$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1-Closure-116-Developer$$Allow side effects on function declarations in empty functions$$1
Closure-45$$Assignment removed when used as an expression result to Array.push$$None$$patch1-Closure-45-Developer$$Allow vars to be marked as escaped in closure closures$$1
Closure-111$$goog.isArray doesn't hint compiler$$None$$patch1-Closure-111-Developer$$Allow array types in closure implementations$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-Developer$$Fix typo in data$$1
Closure-118$$Prototype method incorrectly removed$$None$$patch1-Closure-118-Developer$$removed over quoting$$1
Closure-87$$IE8 error: Object doesn't support this action$$None$$patch1-Closure-87-Developer$$Allow looping in JS files$$1
Closure-127$$Break in finally block isn't optimized properly$$None$$patch1-Closure-127-Developer$$added inFinally ( ) check for try / catch statement$$1
Closure-80$$Unexpected expression nodeDELPROP 1$$None$$patch1-Closure-80-Developer$$fix merge issue$$1
Closure-74$$Obvious optimizations don't works in "inline if"$$None$$patch1-Closure-74-Developer$$Fixed a bug in Closure_74 where NOT nodes were being reorganised as literals .$$1
Closure-120$$Overzealous optimization confuses variables$$None$$patch1-Closure-120-Developer$$fix # 1210$$1
Closure-6$$better 'this' type checking$$None$$patch1-Closure-6-Developer$$Allow one - line ifs$$1
Closure-28$$constant functions not inlined aggressively enough$$None$$patch1-Closure-28-Developer$$Adding constant to closure in inline cost estimate$$1
Closure-17$$@const dumps type cast information$$None$$patch1-Closure-17-Developer$$Fixed type - casting issues in TypedScopeCreator$$1
Closure-1$$function arguments should not be optimized away$$None$$patch1-Closure-1-Developer$$don ' t remove unused vars if we don ' t remove globals$$1
Closure-10$$Wrong code generated if mixing types in ternary operator$$None$$patch1-Closure-10-Developer$$fixing typo in mayBeString predicate$$1
Closure-19$$Type refining of 'this' raises IllegalArgumentException$$None$$patch1-Closure-19-Developer$$Fixing this case$$1
Closure-26$$ProcessCommonJSModules module$exports failures when checkTypes enabled$$None$$patch1-Closure-26-Developer$$Do not emit module exports twice in one go$$1
Closure-8$$Obfuscated code triggers TypeError in Firefox$$None$$patch1-Closure-8-Developer$$fix # 417$$1
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-21-Developer$$Do not check side effects of expr results with result < - > block$$1
Closure-75$$closure compiled swfobject error$$None$$patch1-Closure-75-Developer$$Allow null string input$$1
Closure-81$$An unnamed function statement statements should generate a parse error$$None$$patch1-Closure-81-Developer$$reported an error about the missing function type in the source code$$1
Closure-121$$Overzealous optimization confuses variables$$None$$patch1-Closure-121-Developer$$fix failing test$$1
Closure-119$$catch(e) yields JSC_UNDEFINED_NAME warning when e is used in catch in advanced mode$$None$$patch1-Closure-119-Developer$$Handle catch / error in Closure data$$1
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch1-Closure-86-Developer$$fix merge conflict resolution$$1
Closure-72$$Internal Compiler Error on Bullet$$None$$patch1-Closure-72-Developer$$Fix compiler error$$1
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-Developer$$Remove tryMinimizeExitPoints from Closure_MinimizeExitPoints$$1
Closure-44$$alert(/ / / / /)$$None$$patch1-Closure-44-Developer$$Allow forward slash in regexps$$1
Closure-110$$Allow @private top-level functions in goog.scope$$None$$patch1-Closure-110-Developer$$Fix possible NPE in closure scopeing info$$1
Closure-43$$@lends does not work unless class is defined beforehand$$None$$patch1-Closure-43-Developer$$Fix @ lends to not throw an error if lentObjectLiterals is null .$$1
Closure-128$$The compiler quotes the "0" keys in object literals$$None$$patch1-Closure-128-Developer$$Fix isSimpleNumber ( )$$1
Closure-88$$Incorrect assignment removal from expression in simple mode.$$None$$patch1-Closure-88-Developer$$Add a check for isVariableReadBeforeKill ( Node , String )$$1
Closure-117$$Wrong type name reported on missing property error.$$None$$patch1-Closure-117-Developer$$Fix getReadableJSTypeName for GETPROP .$$1
Closure-38$$Identifier minus a negative number needs a space between the "-"s$$None$$patch1-Closure-38-Developer$$added missing closing - > positive zero$$1
Closure-36$$goog.addSingletonGetter prevents unused class removal$$None$$patch1-Closure-36-Developer$$Fix inline with singleton methods$$1
Closure-31$$Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLY$$None$$patch1-Closure-31-Developer$$fix some JARs / folders that are not used .$$1
Closure-131$$unicode characters in property names result in invalid output$$None$$patch1-Closure-131-Developer$$Fix possible identifierInStringMatch$$1
Closure-91$$support @lends annotation$$None$$patch1-Closure-91-Developer$$Fix check for isObjectLitKey ( )$$1
Closure-65$$String escaping mishandles null byte$$None$$patch1-Closure-65-Developer$$Fix code generator$$1
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-Developer$$Fixed whitespace in source excerpt$$1
Closure-96$$Missing type-checks for var_args notation$$None$$patch1-Closure-96-Developer$$Allow var_args parameter access from closure_96$$1
Closure-109$$Constructor types that return all or unknown fail to parse$$None$$patch1-Closure-109-Developer$$Parser now uses new type AST .$$1
Closure-100$$Only assignment to "this" issues a "dangerous use of the global this object" warning.$$None$$patch1-Closure-100-Developer$$Added check for property access in closures$$1
Closure-54$$Prototype methods can't be used from the constructor in case prototype is explicitly defined.$$None$$patch1-Closure-54-Developer$$Fix wrong source formatting .$$1
Closure-107$$Variable names prefixed with MSG_ cause error with advanced optimizations$$None$$patch1-Closure-107-Developer$$shut up i18n warnings$$1
Closure-98$$bad variable inlining in closure$$None$$patch1-Closure-98-Developer$$Fixing possible loop in closure scope$$1
Closure-53$$compiler-20110811 crashes with index(1) must be less than size(1)$$None$$patch1-Closure-53-Developer$$Fix an issue with closure objects where empty collections are not used$$1
Closure-30$$Combining temporary strings are over-optimized in advanced build$$None$$patch1-Closure-30-Developer$$Do not traverse roots of Closure data$$1
Closure-37$$incomplete function definition crashes the compiler when ideMode is enabled$$None$$patch1-Closure-37-Developer$$Added check for expanded macros$$1
Closure-39$$externExport with @typedef can generate invalid externs$$None$$patch1-Closure-39-Developer$$Fix toStringHelper for Closure_39$$1
Closure-106$$Exception thrown from com.google.javascript.jscomp.CollapseProperties.addStubsForUndeclaredProperties$$None$$patch1-Closure-106-Developer$$Fixing Preconditions . checkNotNull ( description ) .$$1
Closure-99$$Prototypes declared with quotes produce a JSC_USED_GLOBAL_THIS warning.$$None$$patch1-Closure-99-Developer$$Fix check for prototype properties on interfaces ( and @ interface )$$1
Closure-52$$Converts string properties into numbers in literal object definitions$$None$$patch1-Closure-52-Developer$$Fixed false positives in getSimpleNumber ( )$$1
Closure-101$$--process_closure_primitives can't be set to false$$None$$patch1-Closure-101-Developer$$Fix closure pass by moving to the right place .$$1
Closure-55$$Exception when emitting code containing getters$$None$$patch1-Closure-55-Developer$$Allow closure nodes to be reduced by properties$$1
Closure-97$$Unsigned Shift Right (>>>) bug operating on negative numbers$$None$$patch1-Closure-97-Developer$$Fix 64 - bit long shift operator in Closure_97$$1
Closure-108$$precondition crash: goog.scope local with aliased in the type declaration$$None$$patch1-Closure-108-Developer$$Do not remove injectedDecls as we process the same kind of code when creating the alias .$$1
Closure-63$$None$$None$$patch1-Closure-63-Developer$$add whitespace to sourceExcerpt$$1
Closure-130$$arguments is moved to another scope$$None$$patch1-Closure-130-Developer$$don ' t inline if in externs$$1
Closure-64$$--language_in=ECMASCRIPT5_STRICT results in 1 'use strict' per input file$$None$$patch1-Closure-64-Developer$$Fix toSource ( ) method$$1
Closure-90$$@this emits warning when used with a typedef$$None$$patch1-Closure-90-Developer$$Allow null values for closure types$$1
Closure-46$$ClassCastException during TypeCheck pass$$None$$patch1-Closure-46-Developer$$Remove special treatment for property types .$$1
Closure-79$$RuntimeException when compiling with extern prototype$$None$$patch1-Closure-79-Developer$$Fix code change reported by Closure_79_VarCheck_s$$1
Closure-112$$Template types on methods incorrectly trigger inference of a template on the class if that template type is unknown$$None$$patch1-Closure-112-Developer$$Allow template types to be included in inferred by default$$1
Closure-41$$In ADVANCED mode, Compiler fails to warn about overridden methods with different signatures.$$None$$patch1-Closure-41-Developer$$fix merge issue$$1
Closure-115$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1-Closure-115-Developer$$disable side effects check for function arguments$$1
Closure-83$$Cannot see version with --version$$None$$patch1-Closure-83-Developer$$Fixed try catch block for parseArguments$$1
Closure-77$$\0 \x00 and \u0000 are translated to null character$$None$$patch1-Closure-77-Developer$$Fix code generator$$1
Closure-123$$Generates code with invalid for/in left-hand assignment$$None$$patch1-Closure-123-Developer$$Fix an issue with ' in ' after being told to use the context for the in operator$$1
Closure-48$$Type checking error when replacing a function with a stub after calling.$$None$$patch1-Closure-48-Developer$$Fix inferred flag for functions in unscoped qualified name .$$1
Closure-70$$unexpected typed coverage of less than 100%$$None$$patch1-Closure-70-Developer$$Fixing the warning about jsDoc parameter definition for typed scope creator$$1
Closure-84$$Invalid left-hand side of assignment not detected$$None$$patch1-Closure-84-Developer$$Fix an error in Closure data that is not valid assignment targets .$$1
Closure-124$$Different output from RestAPI and command line jar$$None$$patch1-Closure-124-Developer$$Fix download age$$1
Closure-24$$goog.scope doesn't properly check declared functions$$None$$patch1-Closure-24-Developer$$fix another broken link to ' alias ' report$$1
Closure-23$$tryFoldArrayAccess does not check for side effects$$None$$patch1-Closure-23-Developer$$Fix swapped loop$$1
Closure-4$$Converting from an interface type to a constructor which @implements itself causes stack overflow.$$None$$patch1-Closure-4-Developer$$Fix Cycles in Closure_4_NamedType_t$$1
Closure-15$$Switched order of "delete key" and "key in" statements changes semantic$$None$$patch1-Closure-15-Developer$$Added another check for delProp$$1
Closure-3$$optimization fails with variable in catch clause$$None$$patch1-Closure-3-Developer$$Fix possible ES6 error$$1
Closure-12$$Try/catch blocks incorporate code not inside original blocks$$None$$patch1-Closure-12-Developer$$Fix error in maybeReachingVariableUse$$1
Closure-85$$Reproduceable crash with switch statement$$None$$patch1-Closure-85-Developer$$Fix tryRemoveUnconditionalBranching ( )$$1
Closure-71$$no warnings when @private prop is redeclared on subclass$$None$$patch1-Closure-71-Developer$$Fix warning$$1
Closure-125$$IllegalStateException at com.google.javascript.rhino.jstype.FunctionType.getInstanceType$$None$$patch1-Closure-125-Developer$$Fix TypeCheck$$1
Closure-76$$Assignments within conditions are sometimes incorrectly removed$$None$$patch1-Closure-76-Developer$$fix the case$$1
Closure-82$$.indexOf fails to produce missing property warning$$None$$patch1-Closure-82-Developer$$isEmptyType ( ) is not a function type$$1
Closure-49$$Incorrect output if a function is assigned to a variable, and the function contains a variable with the same name$$None$$patch1-Closure-49-Developer$$Do not visit LP as varargs to "" varargs "" when move to LP"$$1
Closure-122$$Inconsistent handling of non-JSDoc comments$$None$$patch1-Closure-122-Developer$$Allow comments to be ignored .$$1
Closure-40$$smartNameRemoval causing compiler crash$$None$$patch1-Closure-40-Developer$$Fix possible NPE$$1
Closure-114$$Crash on the web closure compiler$$None$$patch1-Closure-114-Developer$$Fix merge conflict in ODS data$$1
Closure-47$$Original source line numbers are one-based in source maps.$$None$$patch1-Closure-47-Developer$$Fix source map offset for v3 source map entries .$$1
Closure-113$$Bug in require calls processing$$None$$patch1-Closure-113-Developer$$Allow for changes to provided$$1
Closure-78$$division by zero wrongly throws JSC_DIVIDE_BY_0_ERROR$$None$$patch1-Closure-78-Developer$$Fix PeepholeFoldConstants data$$1
Closure-2$$combining @interface and multiple @extends can crash compiler$$None$$patch1-Closure-2-Developer$$Fix typo in interfaceType data$$1
Closure-13$$true/false are not always replaced for !0/!1$$None$$patch1-Closure-13-Developer$$Fix PeepholeOptimizationsPass$$1
Closure-5$$Compiler ignores 'delete' statements, can break functionality.$$None$$patch1-Closure-5-Developer$$fix merge issue$$1
Closure-14$$bogus 'missing return' warning$$None$$patch1-Closure-14-Developer$$Fix CFA$$1
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-22-Developer$$Allow ' + ' after the closing parenthesis in closure - scoped code$$1
Closure-25$$anonymous object type inference behavior is different when calling constructors$$None$$patch1-Closure-25-Developer$$Updated scope for constructors with arguments$$1
Chart-20$$None$$None$$patch1-Chart-20-Developer$$Added missing closing parenthesis in ValueMarker$$1
Chart-18$$None$$None$$patch1-Chart-18-Developer$$Fix bugs in Controller and File ( ODS ) .$$1
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-Developer$$Fix empty range in chart_9_TimeSeries_t$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-Developer$$Fix human patching$$1
Chart-7$$None$$None$$patch1-Chart-7-Developer$$Fix bug in chart$$1
Chart-16$$Bug propgated from v1.0.5 on to present$$The method getRowCount() in class org.jfree.data.category.DefaultIntervalCategoryDataset says that it "Returns the number of series in the dataset (possibly zero)."  The implementation from v1.0.5 on no longer checks for a null condition (which would then return a zero) on the seriesKeys as it did in v1.0.4 and previous. This now throws a Null Pointer if seriesKeys never got initialized and the getRowCount() method is called.$$patch1-Chart-16-Developer$$Fix uninitialized variables in DefaultIntervalCategoryDataset .$$1
Chart-6$$None$$None$$patch1-Chart-6-Developer$$Fix chart6 shape list fixing$$1
Chart-17$$cloning of TimeSeries$$It's just a minor bug!  When I clone a TimeSeries which has no items, I get an IllegalArgumentException ("Requires start <= end"). But I don't think the user should be responsible for checking whether the TimeSeries has any items or not.$$patch1-Chart-17-Developer$$deep clone the timeline and timeline data lists .$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-Developer$$Fix nullability assertion .$$1
Chart-10$$None$$None$$patch1-Chart-10-Developer$$Fix build$$1
Chart-19$$None$$None$$patch1-Chart-19-Developer$$Fix a warning$$1
Chart-26$$None$$None$$patch1-Chart-26-Developer$$Add perfomance label to human patche$$1
Chart-8$$None$$None$$patch1-Chart-8-Developer$$Fix bug in week constructor$$1
Chart-21$$None$$None$$patch1-Chart-21-Developer$$Fixed a bug in chart bounds$$1
Chart-24$$None$$None$$patch1-Chart-24-Developer$$Fix bug in chart_24_GrayPaintScale$$1
Chart-23$$None$$None$$patch1-Chart-23-Developer$$Fix minor warning$$1
Chart-4$$None$$None$$patch1-Chart-4-Developer$$added missing test$$1
Chart-15$$None$$None$$patch1-Chart-15-Developer$$Fix an NPE if the dataset is null .$$1
Chart-3$$None$$None$$patch1-Chart-3-Developer$$Fix chart error$$1
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-Developer$$Fixed a bug in MultiplePiePlot .$$1
Chart-2$$Bugs in DatasetUtilities.iterateRangeBounds() methods$$None$$patch1-Chart-2-Developer$$Fix NaN values in XY dataset$$1
Chart-13$$None$$None$$patch1-Chart-13-Developer$$Fix chart border arrangement issue$$1
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-Developer$$Fix allowDuplicateXValues for XYSeries$$1
Chart-14$$None$$None$$patch1-Chart-14-Developer$$added human patches$$1
Chart-22$$None$$None$$patch1-Chart-22-Developer$$Fix removeRow ( ) where the column key was not found .$$1
Chart-25$$None$$None$$patch1-Chart-25-Developer$$Add missing fix from previous commit .$$1
Math-61$$Dangerous code in "PoissonDistributionImpl"$$In the following excerpt from class "PoissonDistributionImpl": PoissonDistributionImpl.java     public PoissonDistributionImpl(double p, NormalDistribution z) {         super();         setNormal(z);         setMean(p);     }   (1) Overridable methods are called within the constructor. (2) The reference "z" is stored and modified within the class. I've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the "setter" methods inside the constructor but I think that a more elegant solution would be to remove the "setters" altogether (i.e. make the classes immutable). Problem (2) can also create unexpected behaviour. Is it really necessary to pass the "NormalDistribution" object; can't it be always created within the class?$$patch1-Math-61-Developer$$Add missing import$$1
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1-Math-95-Developer$$Fix @@$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-Developer$$Fix typo in last commit$$1
Math-92$$MathUtils.binomialCoefficient(n,k) fails for large results$$Probably due to rounding errors, MathUtils.binomialCoefficient(n,k) fails for results near Long.MAX_VALUE. The existence of failures can be demonstrated by testing the recursive property:           assertEquals(MathUtils.binomialCoefficient(65,32) + MathUtils.binomialCoefficient(65,33),                  MathUtils.binomialCoefficient(66,33));   Or by directly using the (externally calculated and hopefully correct) expected value:           assertEquals(7219428434016265740L, MathUtils.binomialCoefficient(66,33));   I suggest a nonrecursive test implementation along the lines of MathUtilsTest.java     /**      * Exact implementation using BigInteger and the explicit formula      * (n, k) == ((k-1)*...*n) / (1*...*(n-k))      */ 	public static long binomialCoefficient(int n, int k) { 		if (k == 0 || k == n) 			return 1; 		BigInteger result = BigInteger.ONE; 		for (int i = k + 1; i <= n; i++) { 			result = result.multiply(BigInteger.valueOf(i)); 		} 		for (int i = 1; i <= n - k; i++) { 			result = result.divide(BigInteger.valueOf(i)); 		} 		if (result.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) > 0) { 			throw new ArithmeticException(                                 "Binomial coefficient overflow: " + n + ", " + k); 		} 		return result.longValue(); 	}   Which would allow you to test the expected values directly:           assertEquals(binomialCoefficient(66,33), MathUtils.binomialCoefficient(66,33));$$patch1-Math-92-Developer$$Fix a bug in binomial coefficient ( n , k )$$1
Math-66$$Bugs in "BrentOptimizer"$$I apologize for having provided a buggy implementation of Brent's optimization algorithm (class "BrentOptimizer" in package "optimization.univariate"). The unit tests didn't show that there was something wrong, although (from the "changes.xml" file) I discovered that, at the time, Luc had noticed something weird in the implementation's behaviour. Comparing with an implementation in Python, I could figure out the fixes. I'll modify "BrentOptimizer" and add a test. I also propose to change the name of the unit test class from "BrentMinimizerTest" to "BrentOptimizerTest".$$patch1-Math-66-Developer$$Fix a bug in the Math_66_BrentOptimizer class .$$1
Math-104$$Special functions not very accurate$$The Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I'd expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.$$patch1-Math-104-Developer$$Add back missing @@$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-Developer$$Fix REGULA_FALSI case .$$1
Math-68$$LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it$$LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.$$patch1-Math-68-Developer$$Added a convergence checker$$1
Math-103$$ConvergenceException in normal CDF$$NormalDistributionImpl::cumulativeProbability(double x) throws ConvergenceException if x deviates too much from the mean. For example, when x=+/-100, mean=0, sd=1. Of course the value of the CDF is hard to evaluate in these cases, but effectively it should be either zero or one.$$patch1-Math-103-Developer$$Fix max iterations reached in Math_103 NormalDistributionImpl$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-Developer$$Fix the build .$$1
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?$$patch1-Math-32-Developer$$Fixed a bug in Controller of Controller data .$$1
Math-35$$Need range checks for elitismRate in ElitisticListPopulation constructors.$$There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.$$patch1-Math-35-Developer$$ElitisticListPopulation constructor should set the elitism rate before setting the population limit .$$1
Math-102$$chiSquare(double[] expected, long[] observed) is returning incorrect test statistic$$ChiSquareTestImpl is returning incorrect chi-squared value. An implicit assumption of public double chiSquare(double[] expected, long[] observed) is that the sum of expected and observed are equal. That is, in the code: for (int i = 0; i < observed.length; i++)  {             dev = ((double) observed[i] - expected[i]);             sumSq += dev * dev / expected[i];         } this calculation is only correct if sum(observed)==sum(expected). When they are not equal then one must rescale the expected value by sum(observed) / sum(expected) so that they are. Ironically, it is an example in the unit test ChiSquareTestTest that highlights the error: long[] observed1 =  { 500, 623, 72, 70, 31 } ;         double[] expected1 =  { 485, 541, 82, 61, 37 } ;         assertEquals( "chi-square test statistic", 16.4131070362, testStatistic.chiSquare(expected1, observed1), 1E-10);         assertEquals("chi-square p-value", 0.002512096, testStatistic.chiSquareTest(expected1, observed1), 1E-9); 16.413 is not correct because the expected values do not make sense, they should be: 521.19403 581.37313  88.11940  65.55224  39.76119 so that the sum of expected equals 1296 which is the sum of observed. Here is some R code (r-project.org) which proves it: > o1 [1] 500 623  72  70  31 > e1 [1] 485 541  82  61  37 > chisq.test(o1,p=e1,rescale.p=TRUE)         Chi-squared test for given probabilities data:  o1  X-squared = 9.0233, df = 4, p-value = 0.06052 > chisq.test(o1,p=e1,rescale.p=TRUE)$observed [1] 500 623  72  70  31 > chisq.test(o1,p=e1,rescale.p=TRUE)$expected [1] 521.19403 581.37313  88.11940  65.55224  39.76119$$patch1-Math-102-Developer$$added rescale option to observed count$$1
Math-69$$PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon$$Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that. In MATH-201, the problem was described as such: > So in essence, the p-value returned by TTestImpl.tTest() is: >  > 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t)) >  > For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When  > cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because: >  > 1.0 - 1.0 + 0.0 = 0.0 The solution in MATH-201 was to modify the p-value calculation to this: > p = 2.0 * cumulativeProbability(-t) Here, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():   p = 2 * (1 - tDistribution.cumulativeProbability(t)); Directly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:   p = 2 * (tDistribution.cumulativeProbability(-t));$$patch1-Math-69-Developer$$PearsonsCorrelation tDistribution is not 1 - tDistribution$$1
Math-56$$MultidimensionalCounter.getCounts(int) returns wrong array of indices$$MultidimensionalCounter counter = new MultidimensionalCounter(2, 4); for (Integer i : counter) {     int[] x = counter.getCounts;     System.out.println(i + " " + Arrays.toString); } Output is: 0 [0, 0] 1 [0, 1] 2 [0, 2] 3 [0, 2]   <=== should be [0, 3] 4 [1, 0] 5 [1, 1] 6 [1, 2] 7 [1, 2]   <=== should be [1, 3]$$patch1-Math-56-Developer$$Fixed bug in Math_56_MultidimensionalCounter_t$$1
Math-105$$[math]  SimpleRegression getSumSquaredErrors$$getSumSquaredErrors returns -ve value. See test below: public void testSimpleRegression() { 		double[] y =  {  8915.102, 8919.302, 8923.502} ; 		double[] x =  { 1.107178495, 1.107264895, 1.107351295} ; 		double[] x2 =  { 1.107178495E2, 1.107264895E2, 1.107351295E2} ; 		SimpleRegression reg = new SimpleRegression(); 		for (int i = 0; i < x.length; i++)  { 			reg.addData(x[i],y[i]); 		} 		assertTrue(reg.getSumSquaredErrors() >= 0.0); // OK 		reg.clear(); 		for (int i = 0; i < x.length; i++)  { 			reg.addData(x2[i],y[i]); 		} 		assertTrue(reg.getSumSquaredErrors() >= 0.0); // FAIL 	}$$patch1-Math-105-Developer$$Add missing @@$$1
Math-51$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-51-Developer$$Add case for regula_FALSI to update the functionive value .$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-Developer$$Added tagging$$1
Math-67$$Method "getResult()" in "MultiStartUnivariateRealOptimizer"$$In "MultiStartUnivariateRealOptimizer" (package "optimization"), the method "getResult" returns the result of the last run of the "underlying" optimizer; this last result might not be the best one, in which case it will not correspond to the value returned by the "optimize" method. This is confusing and does not seem very useful. I think that "getResult" should be defined as    public double getResult() {     return optima[0]; }   and similarly  public double getFunctionValue() {     return optimaValues[0]; }$$patch1-Math-67-Developer$$Added missing @@$$1
Math-93$$MathUtils.factorial(n) fails for n >= 17$$The result of MathUtils.factorial( n ) for n = 17, 18, 19 is wrong, probably because of rounding errors in the double calculations. Replace the first line of MathUtilsTest.testFactorial() by         for (int i = 1; i <= 20; i++) { to check all valid arguments for the long result and see the failure. I suggest implementing a simple loop to multiply the long result - or even using a precomputed long[] - instead of adding logarithms.$$patch1-Math-93-Developer$$Fix a bug in factorial ( )$$1
Math-94$$MathUtils.gcd(u, v) fails when u and v both contain a high power of 2$$The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.         assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15))); Fix: Replace the test at the start of MathUtils.gcd()         if (u * v == 0) { by         if (u == 0 || v == 0) {$$patch1-Math-94-Developer$$Fix gcd ( )$$1
Math-60$$ConvergenceException in NormalDistributionImpl.cumulativeProbability()$$I get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity. For instance in the following code: 	@Test 	public void testCumulative() { 		final NormalDistribution nd = new NormalDistributionImpl(); 		for (int i = 0; i < 500; i++) { 			final double val = Math.exp; 			try  { 				System.out.println("val = " + val + " cumulative = " + nd.cumulativeProbability(val)); 			}  catch (MathException e)  { 				e.printStackTrace(); 				fail(); 			} 		} 	} In version 2.0, I get no exception.  My suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.$$patch1-Math-60-Developer$$Fix max iterations reached in Math_60_NormalDistributionImpl_t$$1
Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-Math-34-Developer$$Fix bug in HIVE - 1299$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-Developer$$maxUlps - > epsilon$$1
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-Math-20-Developer$$Fix the fix in CMAESOptimizer$$1
Math-18$$CMAESOptimizer with bounds fits finely near lower bound and coarsely near upper bound.$$When fitting with bounds, the CMAESOptimizer fits finely near the lower bound and coarsely near the upper bound.  This is because it internally maps the fitted parameter range into the interval [0,1].  The unit of least precision (ulp) between floating point numbers is much smaller near zero than near one.  Thus, fits have much better resolution near the lower bound (which is mapped to zero) than the upper bound (which is mapped to one).  I will attach a example program to demonstrate.$$patch1-Math-18-Developer$$Fix CMAESOptimizer test .$$1
Math-27$$Fraction percentageValue rare overflow$$The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value. The patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.$$patch1-Math-27-Developer$$Added missing @@$$1
Math-9$$Line.revert() is imprecise$$Line.revert() only maintains ~10 digits for the direction. This becomes an issue when the line's position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction. Also, is there a reason why Line is not immutable? It is just comprised of two vectors.$$patch1-Math-9-Developer$$Fixed test data$$1
Math-11$$MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd$$To reproduce:  Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);$$patch1-Math-11-Developer$$Fix a bug in MultivariateNormalDistribution$$1
Math-7$$event state not updated if an unrelated event triggers a RESET_STATE during ODE integration$$When an ODE solver manages several different event types, there are some unwanted side effects. If one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return. As a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20. The attached file reproduces the problem.$$patch1-Math-7-Developer$$Fix human patching$$1
Math-29$$Bugs in RealVector.ebeMultiply(RealVector) and ebeDivide(RealVector)$$OpenMapRealVector.ebeMultiply(RealVector) and OpenMapRealVector.ebeDivide(RealVector) return wrong values when one entry of the specified RealVector is nan or infinity. The bug is easy to understand. Here is the current implementation of ebeMultiply      public OpenMapRealVector ebeMultiply(RealVector v) {         checkVectorDimensions(v.getDimension());         OpenMapRealVector res = new OpenMapRealVector(this);         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             res.setEntry(iter.key(), iter.value() * v.getEntry(iter.key()));         }         return res;     }   The assumption is that for any double x, x * 0d == 0d holds, which is not true. The bug is easy enough to identify, but more complex to solve. The only solution I can come up with is to loop through all entries of v (instead of those entries which correspond to non-zero entries of this). I'm afraid about performance losses.$$patch1-Math-29-Developer$$Fix the for - loop of Math_29 OpenMapRealVector data$$1
Math-16$$FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts$$As reported by Jeff Hain: cosh(double) and sinh(double): Math.cosh(709.783) = 8.991046692770538E307 FastMath.cosh(709.783) = Infinity Math.sinh(709.783) = 8.991046692770538E307 FastMath.sinh(709.783) = Infinity ===> This is due to using exp( x )/2 for values of |x| above 20: the result sometimes should not overflow, but exp( x ) does, so we end up with some infinity. ===> for values of |x| >= StrictMath.log(Double.MAX_VALUE), exp will overflow, so you need to use that instead: for x positive: double t = exp(x*0.5); return (0.5*t)*t; for x negative: double t = exp(-x*0.5); return (-0.5*t)*t;$$patch1-Math-16-Developer$$Fix a bug in Math_16_FastMath_t$$1
Math-42$$Negative value with restrictNonNegative$$Problem: commons-math-2.2 SimplexSolver. A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call: SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true); Function 1 * x + 1 * y + 0 Constraints: 1 * x + 0 * y = 1 Result: x = 1; y = -1; Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.$$patch1-Math-42-Developer$$Fix the case for the objective function row being invalid after correction$$1
Math-89$$Bugs in Frequency API$$I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.$$patch1-Math-89-Developer$$Add throw argument for addValue ( )$$1
Math-45$$Integer overflow in OpenMapRealMatrix$$computeKey() has an integer overflow. Since it is a sparse matrix, this is quite easily encountered long before heap space is exhausted. The attached code demonstrates the problem, which could potentially be a security vulnerability (for example, if one was to use this matrix to store access control information). Workaround: never create an OpenMapRealMatrix with more cells than are addressable with an int.$$patch1-Math-45-Developer$$Add throw condition$$1
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-Math-73-Developer$$Added a message to the console if yMin * yMax > 0$$1
Math-87$$Basic variable is not found correctly in simplex tableau$$The last patch to SimplexTableau caused an automated test suite I'm running at work to go down a new code path and uncover what is hopefully the last bug remaining in the Simplex code. SimplexTableau was assuming an entry in the tableau had to be nonzero to indicate a basic variable, which is incorrect - the entry should have a value equal to 1.$$patch1-Math-87-Developer$$Improved the following$$1
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-Developer$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted ( ) .$$1
Math-74$$Wrong parameter for first step size guess for Embedded Runge Kutta methods$$In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator. Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...) The problem comes from the array "scale" that is used as a parameter in the call off initializeStep(..) Following the theory described by Hairer in his book "Solving Ordinary Differential Equations 1 : Nonstiff Problems", the scaling should be : sci = Atol i + |y0i| * Rtoli Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation "sci = Atol i + |y0i| * Rtoli  " when he performs the call to the same method initializeStep(..) In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user. But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...) To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator For exemple :  final double[] scale= new double[y0.length];;           if (vecAbsoluteTolerance == null) {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;               }             } else {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;               }             }           hNew = initializeStep(equations, forward, getOrder(), scale,                            stepStart, y, yDotK[0], yTmp, yDotK[1]); Sorry for the length of this message, looking forward to hearing from you soon Vincent Morand$$patch1-Math-74-Developer$$Fix the bug in EmbeddedRungeKuttaIntegator .$$1
Math-6$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.      @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }$$patch1-Math-6-Developer$$Fix infinite loop in Controller and Gremlin .$$1
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-Developer$$Using the fix from Bland ' s rule$$1
Math-17$$Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)$$In class org.apache.commons.math3.Dfp,  the method multiply(int n) is limited to 0 <= n <= 9999. This is not consistent with the general contract of FieldElement.multiply(int n), where there should be no limitation on the values of n.$$patch1-Math-17-Developer$$Fix the multiply ( ) method$$1
Math-1$$Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception$$An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple fraction.  For example: double d = 0.5000000001; Fraction f = new Fraction(d, 10); Patch with unit test on way.$$patch1-Math-1-Developer$$Add epsilon check in Math_1_Fraction$$1
Math-10$$DerivativeStructure.atan2(y,x) does not handle special cases properly$$The four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However, they give NaN for the value in all cases.$$patch1-Math-10-Developer$$Fix typo in Math_10_DSCompiler_t$$1
Math-19$$Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function$$If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.$$patch1-Math-19-Developer$$Add missing exception message$$1
Math-26$$Fraction(double, int) constructor strange behaviour$$The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest: 1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value 2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831. I have, as of yet, not found a solution. The constructor looks like this: public Fraction(double value, int maxDenominator)         throws FractionConversionException     {        this(value, 0, maxDenominator, 100);     }  Increasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest.  The problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find. This bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.  It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that "since fractions are always in lowest terms, numerators and can be compared directly for equality", so it seems like this is the intention.$$patch1-Math-26-Developer$$Fix overflow in Math_26_Fraction$$1
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch1-Math-8-Developer$$Fix the sample method to return Object [] instead of T [] .$$1
Math-21$$Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix$$The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples): Array2DRowRealMatrix { {0.0,0.0,0.0,0.0,0.0} , {0.0,0.013445532,0.01039469,0.009881156,0.010499559} , {0.0,0.01039469,0.023006616,0.008196856,0.010732709} , {0.0,0.009881156,0.008196856,0.019023866,0.009210099} , {0.0,0.010499559,0.010732709,0.009210099,0.019107243}} > cov(data1)    V1 V2 V3 V4 V5 V1 0 0.000000000 0.00000000 0.000000000 0.000000000 V2 0 0.013383931 0.01034401 0.009913271 0.010506733 V3 0 0.010344006 0.02309479 0.008374730 0.010759306 V4 0 0.009913271 0.00837473 0.019005488 0.009187287 V5 0 0.010506733 0.01075931 0.009187287 0.019021483 Array2DRowRealMatrix { {0.013445532,0.01039469,0.0,0.009881156,0.010499559} , {0.01039469,0.023006616,0.0,0.008196856,0.010732709} , {0.0,0.0,0.0,0.0,0.0}, {0.009881156,0.008196856,0.0,0.019023866,0.009210099}, {0.010499559,0.010732709,0.0,0.009210099,0.019107243}}  > cov(data2)             V1 V2 V3 V4 V5 V1 0.006922905 0.010507692 0 0.005817399 0.010330529 V2 0.010507692 0.023428918 0 0.008273152 0.010735568 V3 0.000000000 0.000000000 0 0.000000000 0.000000000 V4 0.005817399 0.008273152 0 0.004929843 0.009048759 V5 0.010330529 0.010735568 0 0.009048759 0.018683544   Array2DRowRealMatrix{ {0.013445532,0.01039469,0.009881156,0.010499559}, {0.01039469,0.023006616,0.008196856,0.010732709}, {0.009881156,0.008196856,0.019023866,0.009210099}, {0.010499559,0.010732709,0.009210099,0.019107243}}  > cov(data3)             V1          V2          V3          V4 V1 0.013445047 0.010478862 0.009955904 0.010529542 V2 0.010478862 0.022910522 0.008610113 0.011046353 V3 0.009955904 0.008610113 0.019250975 0.009464442 V4 0.010529542 0.011046353 0.009464442 0.019260317   I've traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):  CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0} , {0.0759577418122063,0.0876125188474239,0.0,0.0,0.0} , {0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0} , {0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0} ,{0.13822895138139477,0.0,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 5 CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0}, {0.07764443622513505,0.13029949164628746,0.0} , {0.0,0.0,0.0} , {0.06662930527909404,0.023203936694855674,0.0} ,{0.13822895138139477,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 3 CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785}, {0.07764443622513505,0.13029949164628746,0.0,0.0} , {0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0} ,{0.13822895138139477,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 4 Clearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don't know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the "rectangular" variant (also not at the links provided in the javadoc).$$patch1-Math-21-Developer$$Fix swapped indexing$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-Developer$$Fix typo in Pct method$$1
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-Developer$$fix a possible NPE in EigenDecompositionImpl , added a fix$$1
Math-86$$testing for symmetric positive definite matrix in CholeskyDecomposition$$I used this matrix:         double[][] cv = {  {0.40434286, 0.09376327, 0.30328980, 0.04909388} ,  {0.09376327, 0.10400408, 0.07137959, 0.04762857} ,  {0.30328980, 0.07137959, 0.30458776, 0.04882449},             {0.04909388, 0.04762857, 0.04882449, 0.07543265}         };  And it works fine, because it is symmetric positive definite  I tried this matrix:          double[][] cv = {             {0.40434286, -0.09376327, 0.30328980, 0.04909388},             {-0.09376327, 0.10400408, 0.07137959, 0.04762857},             {0.30328980, 0.07137959, 0.30458776, 0.04882449} ,             {0.04909388, 0.04762857, 0.04882449, 0.07543265}         }; And it should throw an exception but it does not.  I tested the matrix in R and R's cholesky decomposition method returns that the matrix is not symmetric positive definite. Obviously your code is not catching this appropriately. By the way (in my opinion) the use of exceptions to check these conditions is not the best design or use for exceptions.  If you are going to force the use to try and catch these exceptions at least provide methods  to test the conditions prior to the possibility of the exception.$$patch1-Math-86-Developer$$CholeskyDecompositionImpl_t can throw NotPositiveDefiniteMatrixException on diagonal elements$$1
Math-72$$Brent solver returns the wrong value if either bracket endpoint is root$$The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.$$patch1-Math-72-Developer$$Fix case$$1
Math-44$$Incomplete reinitialization with some events handling$$I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way. I can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.$$patch1-Math-44-Developer$$Fix bug in Math_44_AbstractIntegator_t$$1
Math-43$$Statistics.setVarianceImpl makes getStandardDeviation produce NaN$$Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:  int[] scores = {1, 2, 3, 4}; SummaryStatistics stats = new SummaryStatistics(); stats.setVarianceImpl(new Variance(false)); //use "population variance" for(int i : scores) {   stats.addValue(i); } double sd = stats.getStandardDeviation(); System.out.println(sd);   A workaround suggested by Mikkel is:    double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());$$patch1-Math-43-Developer$$Fix secondMoment$$1
Math-88$$Simplex Solver arrives at incorrect solution$$I have reduced the problem reported to me down to a minimal test case which I will attach.$$patch1-Math-88-Developer$$removed a redundant line$$1
Math-38$$Errors in BOBYQAOptimizer when numberOfInterpolationPoints is greater than 2*dim+1$$I've been having trouble getting BOBYQA to minimize a function (actually a non-linear least squares fit) so as one change I increased the number of interpolation points.  It seems that anything larger than 2*dim+1 causes an error (typically at line 1662                    interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt)); I'm guessing there is an off by one error in the translation from FORTRAN.  Changing the BOBYQAOptimizerTest as follows (increasing number of interpolation points by one) will cause failures. Bruce Index: src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java ===================================================================  src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(revision 1221065) +++ src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(working copy) @@ -258,7 +258,7 @@  //        RealPointValuePair result = optim.optimize(100000, func, goal, startPoint);          final double[] lB = boundaries == null ? null : boundaries[0];          final double[] uB = boundaries == null ? null : boundaries[1];  BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 1); +        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 2);          RealPointValuePair result = optim.optimize(maxEvaluations, func, goal, startPoint, lB, uB);  //        System.out.println(func.getClass().getName() + " = "   //              + optim.getEvaluations() + " f(");$$patch1-Math-38-Developer$$Fix NPE in Controller and InterpolationPoints$$1
Math-36$$BigFraction.doubleValue() returns Double.NaN for large numerators or denominators$$The current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator, the resulting quotient fails, even in cases where the result would be well inside Double's range. I have a patch to fix it, if I can figure out how to attach it here I will.$$patch1-Math-36-Developer$$Fix a bug in Controller / Math_36_BigFraction_t$$1
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch1-Math-31-Developer$$Fix Precision . equals ( dN , 0 . 0 , small ) .$$1
Math-91$$Fraction.comparTo returns 0 for some differente fractions$$If two different fractions evaluate to the same double due to limited precision, the compareTo methode returns 0 as if they were identical.  // value is roughly PI - 3.07e-18 Fraction pi1 = new Fraction(1068966896, 340262731);  // value is roughly PI + 1.936e-17 Fraction pi2 = new Fraction( 411557987, 131002976);  System.out.println(pi1.doubleValue() - pi2.doubleValue()); // exactly 0.0 due to limited IEEE754 precision System.out.println(pi1.compareTo(pi2)); // display 0 instead of a negative value$$patch1-Math-91-Developer$$Fix comparison warning in Math_91_Fraction$$1
Math-65$$weight versus sigma in AbstractLeastSquares$$In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.  Once corrected, getRMS() can even reduce  public double getRMS()  {return Math.sqrt(getChiSquare()/rows);}$$patch1-Math-65-Developer$$Fix case .$$1
Math-62$$Miscellaneous issues concerning the "optimization" package$$Revision 990792 contains changes triggered the following issues:  MATH-394 MATH-397 MATH-404  This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):  "BrentOptimizer": a specific convergence checker must be used. "LevenbergMarquardtOptimizer" also has specific convergence checks. Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical): 	 See "BrentOptimizer" and "LevenbergMarquardtOptimizer", the algorithm passes "points" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker). In "PowellOptimizer" the line search ("BrentOptimizer") tolerances depend on the tolerances within the main algorithm. Since tolerances come with "ConvergenceChecker" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals ("BrentOptimizer" field) to the enclosing class ("PowellOptimizer").   Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all. Class "DirectSearchOptimizer" (in package "optimization.direct") inherits from class "AbstractScalarOptimizer" (in package "optimization.general"). Some interfaces are defined in package "optimization" but their base implementations (abstract class that contain the boiler-plate code) are in package "optimization.general" (e.g. "DifferentiableMultivariateVectorialOptimizer" and "BaseAbstractVectorialOptimizer"). No check is performed to ensure the the convergence checker has been set (see e.g. "BrentOptimizer" and "PowellOptimizer"); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker. "NonLinearConjugateGradientOptimizer": Ugly workaround for the checked "ConvergenceException". Everywhere, we trail the checked "FunctionEvaluationException" although it is never used. There remains some duplicate code (such as the "multi-start loop" in the various "MultiStart..." implementations). The "ConvergenceChecker" interface is very general (the "converged" method can take any number of "...PointValuePair"). However there remains a "semantic" problem: One cannot be sure that the list of points means the same thing for the caller of "converged" and within the implementation of the "ConvergenceChecker" that was independently set. It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In "LevenbergMarquartdOptimizer" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations. In "AbstractLeastSquaresOptimizer" and "LevenbergMarquardtOptimizer", occurences of "OptimizationException" were replaced by the unchecked "ConvergenceException" but in some cases it might not be the most appropriate one. "MultiStartUnivariateRealOptimizer": in the other classes ("MultiStartMultivariate...") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval). The Javadoc utility raises warnings (see output of "mvn site") which I couldn't figure out how to correct. Some previously existing classes and interfaces have become no more than a specialisation of new "generics" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.$$patch1-Math-62-Developer$$Fix a bug in MultiStartUnivariateRealOptimizer . optimize ( f , goal ,$$1
Math-96$$Result of multiplying and equals for complex numbers is wrong$$Hi. The bug relates on complex numbers. The methods "multiply" and "equals" of the class Complex are involved. mathematic background:  (0,i) * (-1,0i) = (0,-i). little java program + output that shows the bug: -----------------------------------------------------------------------  import org.apache.commons.math.complex.*; public class TestProg {         public static void main(String[] args) {                  ComplexFormat f = new ComplexFormat();                 Complex c1 = new Complex(0,1);                 Complex c2 = new Complex(-1,0);                  Complex res = c1.multiply(c2);                 Complex comp = new Complex(0,-1);                  System.out.println("res:  "+f.format(res));                 System.out.println("comp: "+f.format(comp));                  System.out.println("res=comp: "+res.equals(comp));         } }   ----------------------------------------------------------------------- res:  -0 - 1i comp: 0 - 1i res=comp: false ----------------------------------------------------------------------- I think the "equals" should return "true". The problem could either be the "multiply" method that gives (-0,-1i) instead of (0,-1i), or if you think thats right, the equals method has to be modified. Good Luck Dieter$$patch1-Math-96-Developer$$Fix comparison issues with Math_96_Complex$$1
Math-100$$AbstractEstimator: getCovariances() and guessParametersErrors() crash when having bound parameters$$the two methods getCovariances() and guessParametersErrors() from org.apache.commons.math.estimation.AbstractEstimator crash with ArrayOutOfBounds exception when some of the parameters are bound. The reason is that the Jacobian is calculated only for the unbound parameters. in the code you loop through all parameters. line #166: final int cols = problem.getAllParameters().length; should be replaced by:  final int cols = problem.getUnboundParameters().length; (similar changes could be done in guessParametersErrors()) the dissadvantage of the above bug fix is that what is returned to the user is an array with smaller size than the number of all parameters. Alternatively, you can have some logic in the code which writes zeros for the elements of the covariance matrix corresponding to the bound parameters$$patch1-Math-100-Developer$$Fix NPE in Math_100_AbstractEstimator_t$$1
Math-54$$class Dfp toDouble method return -inf whan Dfp value is 0 "zero"$$I found a bug in the toDouble() method of the Dfp class. If the Dfp's value is 0 "zero", the toDouble() method returns a  negative infini. This is because the double value returned has an exposant equal to 0xFFF  and a significand is equal to 0. In the IEEE754 this is a -inf. To be equal to zero, the exposant and the significand must be equal to zero. A simple test case is : ---------------------------------------------- import org.apache.commons.math.dfp.DfpField; public class test { 	/**  @param args 	 */ 	public static void main(String[] args)  { 		DfpField field = new DfpField(100); 		System.out.println("toDouble value of getZero() ="+field.getZero().toDouble()+ 				"\ntoDouble value of newDfp(0.0) ="+ 				field.newDfp(0.0).toDouble()); 	} }  May be the simplest way to fix it is to test the zero equality at the begin of the toDouble() method, to be able to return the correctly signed zero ?$$patch1-Math-54-Developer$$Fix trade - off in Math_54_Dfp_s$$1
Math-98$$RealMatrixImpl#operate gets result vector dimensions wrong$$org.apache.commons.math.linear.RealMatrixImpl#operate tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix. Thus line 640 in RealMatrixImpl.java should read double[] out = new double[nRows]; instead of double[] out = new double[v.length];$$patch1-Math-98-Developer$$Fix a bug in the human_patches API , added test data$$1
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch1-Math-53-Developer$$Add the missing isNaN check$$1
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles$$patch1-Math-30-Developer$$Remove unnecessary ints$$1
Math-37$$[math] Complex Tanh for "big" numbers$$Hi, In Complex.java the tanh is computed with the following formula: tanh(a + bi) = sinh(2a)/(cosh(2a)+cos(2b)) + [sin(2b)/(cosh(2a)+cos(2b))]i The problem that I'm finding is that as soon as "a" is a "big" number, both sinh(2a) and cosh(2a) are infinity and then the method tanh returns in the real part NaN (infinity/infinity) when it should return 1.0. Wouldn't it be appropiate to add something as in the FastMath library??: if (real>20.0){       return createComplex(1.0, 0.0); } if (real<-20.0){       return createComplex(-1.0, 0.0); } Best regards, JBB$$patch1-Math-37-Developer$$Fix NPE in Math_37_Complex_t$$1
Math-39$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch1-Math-39-Developer$$Fixed test for forward / upper case$$1
Math-106$$[math] Function math.fraction.ProperFractionFormat.parse(String, ParsePosition) return illogical result$$Hello, I find illogical returned result from function "Fraction parse(String source,  ParsePostion pos)" (in class ProperFractionFormat of the Fraction Package) of  the Commons Math library. Please see the following code segment for more  details: " ProperFractionFormat properFormat = new ProperFractionFormat(); result = null; String source = "1 -1 / 2"; ParsePosition pos = new ParsePosition(0); //Test 1 : fail  public void testParseNegative(){    String source = "-1 -2 / 3";    ParsePosition pos = new ParsePosition(0);    Fraction actual = properFormat.parse(source, pos);    assertNull(actual); } // Test2: success public void testParseNegative(){    String source = "-1 -2 / 3";    ParsePosition pos = new ParsePosition(0);    Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3    assertEquals(1, source.getNumerator());    assertEquals(3, source.getDenominator()); } " Note: Similarly, when I passed in the following inputs:    input 2: (source = 1 2 / -3, pos = 0)   input 3: ( source =  -1 -2 / 3, pos = 0) Function "Fraction parse(String, ParsePosition)" returned Fraction 1/3 (means  the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs  above. I think the function does not handle parsing the numberator/ denominator  properly incase input string provide invalid numerator/denominator.  Thank you!$$patch1-Math-106-Developer$$Fix invalid closing / missing closing paren in ProperFractionFormat$$1
Math-99$$MathUtils.gcd(Integer.MIN_VALUE, 0) should throw an Exception instead of returning Integer.MIN_VALUE$$The gcd method should throw an Exception for gcd(Integer.MIN_VALUE, 0), like for gcd(Integer.MIN_VALUE, Integer.MIN_VALUE). The method should only return nonnegative results.$$patch1-Math-99-Developer$$Fix bug in Math_99_MathUtils_t$$1
Math-52$$numerical problems in rotation creation$$building a rotation from the following vector pairs leads to NaN: u1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377 u2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10 v1 = 1, 0, 0 v2 = 0, 0, 1 The constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:  <v1'|v1'> == <u1|u1>  <v2'|v2'> == <u2|u2>  <u1 |u2>  == <v1'|v2'> Once the (v1', v2') pair has been computed, we compute the cross product:   k = (v1' - u1)^(v2' - u2) and the scalar product:   c = <k | (u1^u2)> By construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)]. c should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm. However, there are numerical problems with the vector above with the way these computations are done, as shown by the following comparisons, showing the result we get from our Java code and the result we get from manual computation with the same formulas but with enhanced precision: commons math:   k = 38514476.5,            -84.,                           -1168590144 high precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208... and it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get: commons math    c = -1.2397173627587605E20 high precision: c =  558382746168463196.7079627... We have lost ALL significant digits in cancellations, and even the sign is wrong!$$patch1-Math-52-Developer$$Updated inPlaneThreshold variable$$1
Math-101$$java.lang.StringIndexOutOfBoundsException in ComplexFormat.parse(String source, ParsePosition pos)$$The parse(String source, ParsePosition pos) method in the ComplexFormat class does not check whether the imaginary character is set or not which produces StringIndexOutOfBoundsException in the substring method : (line 375 of ComplexFormat) ...         // parse imaginary character         int n = getImaginaryCharacter().length();         startIndex = pos.getIndex();         int endIndex = startIndex + n;         if (source.substring(startIndex, endIndex).compareTo(             getImaginaryCharacter()) != 0) { ... I encoutered this exception typing in a JTextFied with ComplexFormat set to look up an AbstractFormatter. If only the user types the imaginary part of the complex number first, he gets this exception. Solution: Before setting to n length of the imaginary character, check if the source contains it. My proposal: ...         int n = 0;         if (source.contains(getImaginaryCharacter()))         n = getImaginaryCharacter().length(); ...		  F.S.$$patch1-Math-101-Developer$$fix a bug in the source string to make it work reliably outside the source string$$1
Math-55$$Vector3D.crossProduct is sensitive to numerical cancellation$$Cross product implementation uses the naive formulas (y1 z2 - y2 z1, ...). These formulas fail when vectors are almost colinear, like in the following example:  Vector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1); Vector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1); System.out.println(Vector3D.crossProduct(v1, v2));   The previous code displays  { -1, 2, 0 }  instead of the correct answer  { -1, 2, 1 }$$patch1-Math-55-Developer$$Fix potential NPE in cross product test$$1
Math-97$$BrentSolver throws IllegalArgumentException$$I am getting this exception: java.lang.IllegalArgumentException: Function values at endpoints do not have different signs.  Endpoints: [-100000.0,1.7976931348623157E308]  Values: [0.0,-101945.04630982173] at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:99) at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:62) The exception should not be thrown with values  [0.0,-101945.04630982173] because 0.0 is positive. According to Brent Worden, the algorithm should stop and return 0 as the root instead of throwing an exception. The problem comes from this method:     public double solve(double min, double max) throws MaxIterationsExceededException,          FunctionEvaluationException {         clearResult();         verifyInterval(min, max);         double yMin = f.value(min);         double yMax = f.value(max);         // Verify bracketing         if (yMin * yMax >= 0)  {             throw new IllegalArgumentException             ("Function values at endpoints do not have different signs." +                     "  Endpoints: [" + min + "," + max + "]" +                      "  Values: [" + yMin + "," + yMax + "]");                }          // solve using only the first endpoint as initial guess         return solve(min, yMin, max, yMax, min, yMin);     } One way to fix it would be to add this code after the assignment of yMin and yMax:         if (yMin ==0 || yMax == 0)  {         	return 0;        	}$$patch1-Math-97-Developer$$Fixing bracketing in Math_97_BrentSolver_t$$1
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-Developer$$Add missing @@$$1
Math-64$$Inconsistent result from Levenberg-Marquardt$$Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost$$patch1-Math-64-Developer$$Fix a crash in the QTy function$$1
Math-90$$Bugs in Frequency API$$I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.$$patch1-Math-90-Developer$$Fix addValue ( )$$1
Math-46$$Division by zero$$In class Complex, division by zero always returns NaN. I think that it should return NaN only when the numerator is also ZERO, otherwise the result should be INF. See here.$$patch1-Math-46-Developer$$Add the fix on Math_46_Complex_t$$1
Math-79$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest { 	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ), 				new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size()); 	} }$$patch1-Math-79-Developer$$Fix OSGi release .$$1
Math-41$$One of Variance.evaluate() methods does not work correctly$$The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset. Similar method in Mean class seems to work. I did not check other methods taking the part of the array; they may have the same problem. Workaround: I had to shrink my arrays and use the method without the length.$$patch1-Math-41-Developer$$Added missing patch for human patch$$1
Math-83$$SimplexSolver not working as expected?$$I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints... Consider this LP: max: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5; r1: x0 + x2 + x4 = 23.0; r2: x1 + x3 + x5 = 23.0; r3: x0 >= 10.0; r4: x2 >= 8.0; r5: x4 >= 5.0; LPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0; The same LP expressed in Apache commons math is: LinearObjectiveFunction f = new LinearObjectiveFunction(new double[]  { 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 } , 0 ); Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[]  { 1, 0, 1, 0, 1, 0 } , Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[]  { 0, 1, 0, 1, 0, 1 } , Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[]  { 1, 0, 0, 0, 0, 0 } , Relationship.GEQ, 10.0)); constraints.add(new LinearConstraint(new double[]  { 0, 0, 1, 0, 0, 0 } , Relationship.GEQ, 8.0)); constraints.add(new LinearConstraint(new double[]  { 0, 0, 0, 0, 1, 0 } , Relationship.GEQ, 5.0)); RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true); that returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0; Is it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied... Am I using the interface wrongly?$$patch1-Math-83-Developer$$ignore objective functions$$1
Math-77$$getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)$$the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries. The current implementation in ArrayRealVector has a typo:      public double getLInfNorm() {         double max = 0;         for (double a : data) {             max += Math.max(max, Math.abs(a));         }         return max;     }   the += should just be an =. There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness). Worse, the implementation in OpenMapRealVector is not even positive semi-definite:          public double getLInfNorm() {         double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             max += iter.value();         }         return max;     }   I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():    public double getLInfNorm() {     double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null) {       norm = Math.max(norm, Math.abs(e.getValue()));     }     return norm;   }   Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.$$patch1-Math-77-Developer$$Remove intermittent loop$$1
Math-48$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-48-Developer$$Add a throw in case of still being able to work together$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-Developer$$Added beta solution .$$1
Math-84$$MultiDirectional optimzation loops forver if started at the correct solution$$MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.$$patch1-Math-84-Developer$$Fix a bug in iterateSimplex ( )$$1
Math-24$$"BrentOptimizer" not always reporting the best point$$BrentOptimizer (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.$$patch1-Math-24-Developer$$Added another attempt to improve the situation .$$1
Math-23$$"BrentOptimizer" not always reporting the best point$$BrentOptimizer (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.$$patch1-Math-23-Developer$$Added a couple of attempts to improve the score$$1
Math-4$$NPE when calling SubLine.intersection() with non-intersecting lines$$When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations. The attached patch fixes both implementations and adds the required test cases.$$patch1-Math-4-Developer$$Fixed NPE in Math_4_SubLine_t$$1
Math-15$$FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53$$As reported by Jeff Hain: pow(double,double): Math.pow(-1.0,5.000000000000001E15) = -1.0 FastMath.pow(-1.0,5.000000000000001E15) = 1.0 ===> This is due to considering that power is an even integer if it is >= 2^52, while you need to test that it is >= 2^53 for it. ===> replace "if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)" with "if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)" and that solves it.$$patch1-Math-15-Developer$$Fix typo in human_patches$$1
Math-3$$ArrayIndexOutOfBoundsException in MathArrays.linearCombination$$When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line: double prodHighNext = prodHigh[1]; linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.$$patch1-Math-3-Developer$$Revert MathArrays_t to scalar multiplication .$$1
Math-12$$GammaDistribution cloning broken$$Serializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples. Cause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field 'nextGaussian'. Solution: Make BitStreamGenerator implement Serializable as well. This probably affects other distributions as well.$$patch1-Math-12-Developer$$Fixed bug in BitsStreamGenerator$$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-Developer$$Fix a bug in the Math_85 test .$$1
Math-71$$ODE integrator goes past specified end of integration range$$End of integration range in ODE solving is handled as an event. In some cases, numerical accuracy in events detection leads to error in events location. The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.    public void testMissedEvent() throws IntegratorException, DerivativeException {           final double t0 = 1878250320.0000029;           final double t =  1878250379.9999986;           FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {                          public int getDimension() {                 return 1;             }                          public void computeDerivatives(double t, double[] y, double[] yDot)                 throws DerivativeException {                 yDot[0] = y[0] * 1.0e-6;             }         };          DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,                                                                                1.0e-10, 1.0e-10);          double[] y = { 1.0 };         integrator.setInitialStepSize(60.0);         double finalT = integrator.integrate(ode, t0, y, t, y);         Assert.assertEquals(t, finalT, 1.0e-6);     }$$patch1-Math-71-Developer$$Add the missing patch ( y0 )$$1
Math-76$$NaN singular value from SVD$$The following jython code Start code from org.apache.commons.math.linear import * Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]] A = Array2DRowRealMatrix(Alist) decomp = SingularValueDecompositionImpl(A) print decomp.getSingularValues() End code prints array('d', [11.218599757513008, 0.3781791648535976, nan]) The last singular value should be something very close to 0 since the matrix is rank deficient.  When i use the result from getSolver() to solve a system, i end  up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution. Does this SVD implementation require that the matrix be full rank?  If so, then i would expect an exception to be thrown from the constructor or one of the methods.$$patch1-Math-76-Developer$$Fix the for - loop of singular values in Bt , where B is not upper bidiagonal$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-Developer$$removed epsilon from tableau test$$1
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-Developer$$Fix a bug in ebeDivide ( RealVector )$$1
Math-40$$BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary$$In some cases, the aging feature in BracketingNthOrderBrentSolver fails. It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket. In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).$$patch1-Math-40-Developer$$Added fix for bracketing update$$1
Math-47$$Division by zero$$In class Complex, division by zero always returns NaN. I think that it should return NaN only when the numerator is also ZERO, otherwise the result should be INF. See here.$$patch1-Math-47-Developer$$Fix the case for Math_47_Complex_t$$1
Math-78$$during ODE integration, the last event in a pair of very close event may not be detected$$When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let's say this step spans from 90.0 to 153.0. The switching function switches once again in this step. If the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative. This bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.$$patch1-Math-78-Developer$$added epsilon step for g ( ta , gb )$$1
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-Developer$$Fix an issue with Math_2_HypergeometricDistribution . getNumericalMean ( ) that could$$1
Math-13$$new multivariate vector optimizers cannot be used with large number of weights$$When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large. This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.$$patch1-Math-13-Developer$$Fix NPE in AbstractLeastSquaresOptimizer$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-Developer$$Fixed a bug in Math_5_Complex_t$$1
Math-14$$new multivariate vector optimizers cannot be used with large number of weights$$When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large. This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.$$patch1-Math-14-Developer$$Fix NPE in Weight class$$1
Math-22$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time. The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned. [1] http://markmail.org/message/dxuxh7eybl7xejde$$patch1-Math-22-Developer$$Fixed case of Math_22 being non - inclusive .$$1
Math-25$$"HarmonicFitter.ParameterGuesser" sometimes fails to return sensible values$$The inner class "ParameterGuesser" in "HarmonicFitter" (package "o.a.c.m.optimization.fitting") fails to compute a usable guess for the "amplitude" parameter.$$patch1-Math-25-Developer$$Add a throw if the denominator of Math_25 is zero .$$1
Time-20$$Errors creating/parsing dates with specific time zones.$$The results are out of 572 time zones 130 fail and 30 throw exceptions.  The failures are the most interesting. When I query DateTimeZone to get its time zone ids I will get a time zone like America/Atka. When I take that id and create a date time with it its time zone id is America/Adak. It is like there are multiple list of time zones in Joda time and they are out of sync.$$patch1-Time-20-Developer$$Fix for Persian and Urdu language maps # 208$$1
Time-18$$GJChronology rejects valid Julian dates$$The 2nd statement fails with "org.joda.time.IllegalFieldValueException: Value 29 for dayOfMonth must be in the range [1,28]".  Given that I left the cutover date at the default (October 15, 1582), isn't 1500/02/29 a valid date in the GJChronology?$$patch1-Time-18-Developer$$Fixed bug in GJChronology . getDateTimeMillis + ( year , monthOfYear ,$$1
Time-27$$Different behaviour of PeriodFormatter$$When the hour of day is set to the ambiguous hour on the daylight to standard time transition in a given time zone the result is inconsistent for different time zones. Shoul the hour be set to the daylight hour or the standard hour for all time zones? I can't find anything that documents this behavior.  My test code below returns different results for different time zones. The very last assertion fails on the Australia time zone cutover.$$patch1-Time-27-Developer$$Fix broken parser / period formatting test$$1
Time-9$$Ensure there is a max/min valid offset$$DateTimeZone does not apply a max/min value for an offset. However the parse method is limited to 23:59. Make 23:59:59.999 the maximum.$$patch1-Time-9-Developer$$throw exception if offset is out of range$$1
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch1-Time-11-Developer$$Fixed loading of tzm files in HZO$$1
Time-7$$DateTimeFormat.parseInto sometimes miscalculates year (2.2)$$The following code (which can be added to org.joda.time.format.TestDateTimeFormatter) breaks, because the input mutable date time's millis appear to be mishandled and the year for the parse is changed to 1999.$$patch1-Time-7-Developer$$Fixed year parser bug$$1
Time-16$$DateTimeFormatter.parseInto broken when no year in format$$In Joda Time 2.0, the default year was set to 2000 so that Feb 29 could be parsed correctly. However, parseInto now overwrites the given instant's year with 2000 (or whatever iDefaultYear is set to). The correct behavior would seem to be to use the given instant's year instead of iDefaultYear. This does mean that Feb 29 might not be parseable if the instant's year is not a leap year, but in this case the caller asked for that in a sense.$$patch1-Time-16-Developer$$Fix parse error$$1
Time-6$$Questionable behaviour of GJChronology when dates pass 1BC$$I expect the following test to pass:  However, I never provided "0" for the year myself. I thought it was the job of the framework to skip over non-existent year 0 for me to return 1 BC?$$patch1-Time-6-Developer$$Fix bug # 1862$$1
Time-17$$Bug on withLaterOffsetAtOverlap method$$On the last two brackets we can see that withLaterOffsetAtOverlap is not undoing withEarlierOffsetAtOverlap as it should ( and not even working at all ).$$patch1-Time-17-Developer$$Fix broken HIVE - 593 integration test$$1
Time-1$$Partial.with fails with NPE$$Fails with yearOfCentury, year and yearOfEra. Probably because weekyear has a null range duration type.$$patch1-Time-1-Developer$$Fix build$$1
Time-10$$Days#daysBetween throw exception for MonthDay with 29 February$$Is there a way to avoid this happening? I understand fiddling around with the leap year, you're bound to get issues.$$patch1-Time-10-Developer$$Fixed a bug in Controller / Controller data .$$1
Time-19$$Inconsistent interpretation of ambiguous time during DST$$The inconsistency appears for timezone Europe/London.  These three DateTime objects should all represent the same moment in time even if they are ambiguous. Now, it always returns the earlier instant (summer time) during an overlap.$$patch1-Time-19-Developer$$Fixed test data$$1
Time-26$$.withHourOfDay() sets hour inconsistantly on DST transition.$$When the hour of day is set to the ambiguous hour on the daylight to standard time transition in a given time zone the result is inconsistent for different time zones. Shoul the hour be set to the daylight hour or the standard hour for all time zones? I can't find anything that documents this behavior.  My test code below returns different results for different time zones. The very last assertion fails on the Australia time zone cutover.$$patch1-Time-26-Developer$$Fix broken HIVE - 662 workflow$$1
Time-8$$DateTimeZone.forOffsetHoursMinutes cannot handle negative offset < 1 hour$$DateTimeZone.forOffsetHoursMinutes(h,m) cannot handle negative offset < 1 hour like -0:30 due to argument range checking. I used forOffsetMillis() instead.  This should probably be mentioned in the documentation or negative minutes be accepted.$$patch1-Time-8-Developer$$Fix incorrect exception message$$1
Time-21$$None$$None$$patch1-Time-21-Developer$$Fix broken HIVE5199 cache update .$$1
Time-24$$Incorrect date parsed when week and month used together$$It should print 2011-01-03 but it is printing 2010-01-04.$$patch1-Time-24-Developer$$Fix broken patch$$1
Time-23$$Incorrect mapping of the MET time zone$$This timezone is mapped to Asia/Tehran in DateTimeZone. It should be middle europena time.$$patch1-Time-23-Developer$$Fixed map . put ( )$$1
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch1-Time-4-Developer$$Add missing second argument$$1
Time-15$$possibly a bug in org.joda.time.field.FieldUtils.safeMultiply$$It seems to me that as currently written in joda-time-2.1.jar org.joda.time.field.FieldUtils.safeMultiply(long val1, int scalar) doesn't detect the overflow if the long val1 == Long.MIN_VALUE and the int scalar == -1.  The attached file demonstrates what I think is the bug and suggests a patch.  I looked at the Joda Time bugs list in SourceForge but couldn't see anything that looked relevant.$$patch1-Time-15-Developer$$Add a throw if we have a min value .$$1
Time-3$$addDays(0) changes value of MutableDateTime$$Upon DST transition from summer to winter time zone, adding the amount of zero days to a mutable date time object changes the value of the object. The methods addMonths and addYears show the same problem; addSeconds, addMinutes and addHours are ok.  I have tested with version 2.3. However, if I repeat the test with Joda 1.5.2, the invocation of addDays(0) does not change the date's value.$$patch1-Time-3-Developer$$Fixed Add method .$$1
Time-12$$Check Calendar.ERA in LocalDate.fromCalendarFields$$None$$patch1-Time-12-Developer$$Fixed year / month data issues$$1
Time-2$$Partial.with fails with NPE$$Fails with yearOfCentury, year and yearOfEra. Probably because weekyear has a null range duration type.$$patch1-Time-2-Developer$$Fix a bug in Controller and other frameworks$$1
Time-13$$Negative millis display incorrectly in Period.toString$$The last line should produce "PT-0.100S" instead of "PT0.100S".$$patch1-Time-13-Developer$$Handle the - 1 . 5 sign in the histogram$$1
Time-5$$none standard PeriodType without year throws exception$$I tried to get a Period only for months and weeks with following code:  This throws following exception:  Even removing the year component with .withYearsRemoved() throws the same exception:$$patch1-Time-5-Developer$$Fix bug in period implementation$$1
Time-14$$Unable to add days to a MonthDay set to the ISO leap date$$It's not possible to add days to a MonthDay set to the ISO leap date (February 29th). This is even more bizarre given the exact error message thrown.$$patch1-Time-14-Developer$$Fix bug in monthOfYearDateTimeField$$1
Time-22$$Duration.toPeriod with fixed time zones.$$I have a question concerning the conversion of a Duration to Period. I'm not sure if this is a bug, or if there is a different way to do this.  The basis of the problem, is that using Duration.toPeriod() uses the chronology of the default time zone to do the conversion. This can cause different results from a timezone with DST and one without. This can be reproduced easily with this test. In the joda code, Duration.toPeriod() uses a period constructor that takes the chronology, but null is passed in, so the chronology of the default time zone is used, which leads to this behavior.  The javadoc of toPeriod() states that only precise fields of hours, minutes, seconds, and millis will be converted. But for a fixed timezone, days and weeks are also precise, which is stated in the javadoc for toPeriod(Chronology chrono). In our app, we need consistent behavior regardless of the default time zone, which is to have all the extra hours put into the hours bucket. Since Duration is supposed to be a 'time zone independent' length of time, I don't think we should have to do any chronology manipulation to get this to work.$$patch1-Time-22-Developer$$Fix bug in BasePeriod class .$$1
Time-25$$DateTimeZone.getOffsetFromLocal error during DST transition$$This may be a failure of my understanding, but the comments in DateTimeZone.getOffsetFromLocal lead me to believe that if an ambiguous local time is given, the offset corresponding to the later of the two possible UTC instants will be returned - i.e. the greater offset.  This doesn't appear to tally with my experience. In fall 2009, America/Los_Angeles changed from -7 to -8 at 2am wall time on November 11. Thus 2am became 1am - so 1:30am is ambiguous. I would therefore expect that constructing a DateTime for November 11th, 1:30am would give an instant corresponding with the later value (i.e. 9:30am UTC).$$patch1-Time-25-Developer$$Fixed test data for offsetLocal < = 0$$1
Lang-61$$StrBuilder.replaceAll and StrBuilder.deleteAll can throw ArrayIndexOutOfBoundsException.$$StrBuilder.replaceAll and StrBuilder.deleteAll can thrown ArrayIndexOutOfBoundsException's. Here are a couple of additions to the StrBuilderTest class that demonstrate this problem: StrBuilder.deleteAll() - added to testDeleteAll_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.deleteAll("\n%BLAH%");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the following error: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.deleteImpl(StrBuilder.java:1114) 	at org.apache.commons.lang.text.StrBuilder.deleteAll(StrBuilder.java:1188) 	at org.apache.commons.lang.text.StrBuilderTest.testDeleteAll_String(StrBuilderTest.java:606) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) StrBuilder.replaceAll() - added to testReplaceAll_String_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.replaceAll("\n%BLAH%", "");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the exception: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.replaceImpl(StrBuilder.java:1256) 	at org.apache.commons.lang.text.StrBuilder.replaceAll(StrBuilder.java:1339) 	at org.apache.commons.lang.text.StrBuilderTest.testReplaceAll_String_String(StrBuilderTest.java:763) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)$$patch1-Lang-61-Developer$$Fix a bug in the string builder$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-Developer$$fix a bug in string builder$$1
Lang-50$$FastDateFormat getDateInstance() and getDateTimeInstance() assume Locale.getDefault() won't change$$The FastDateFormat getDateInstance() and getDateTimeInstance()  methods create the HashMap key from various items including the locale. If the locale is null, then it is not made part of the key, but the stored object is created using the current default locale. If the Locale is changed subsequently, then the wrong locale is applied. Patch for test case to follow.$$patch1-Lang-50-Developer$$Fix possible NPE$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-Developer$$added missing @@$$1
Lang-32$$Use of ThreadLocals in ToStringStyle and HashCodeBuilder trigger memory leaks in container environments$$The thread local in org.apache.commons.lang3.builder.ToStringStyle is created but never removed and no API is provided to remove it. If a webapp's use of LANG triggers the loading of this class, a reference chain will be created that will cause a memory leak on web application reload. See http://markmail.org/thread/uetw2fdrsqgbh2cv for more info.$$patch1-Lang-32-Developer$$Fix ThreadLocal ' s value check$$1
Lang-35$$ArrayUtils.add(T[] array, T element) can create unexpected ClassCastException$$ArrayUtils.add(T[] array, T element) can create an unexpected ClassCastException. For example, the following code compiles without a warning:  String[] sa = ArrayUtils.add(stringArray, aString);   and works fine, provided at least one of the parameters is non-null. However, if both parameters are null, the add() method returns an Object[] array, hence the Exception. If both parameters are null, it's not possible to determine the correct array type to return, so it seems to me this should be disallowed. I think the method ought to be changed to throw IllegalParameterException when both parameters are null.$$patch1-Lang-35-Developer$$Fixed a bug in ArrayUtils$$1
Lang-56$$FastDateFormat.mRules is not transient or serializable$$Reported by FindBugs. Either we need to make the Rule interface Serializable, or make mRules transient and add deserializing code to kick off init().$$patch1-Lang-56-Developer$$Fix bug$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-Developer$$Missing return statement on booleanUtils$$1
Lang-58$$NumberUtils.createNumber throws NumberFormatException for one digit long$$NumberUtils.createNumber throws a NumberFormatException when parsing "1l", "2l" .. etc... It works fine if you try to parse "01l" or "02l".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for "1l"$$patch1-Lang-58-Developer$$fix a bug in Lang_58_NumberUtils_t$$1
Lang-60$$StrBuilder contains usages of thisBuf.length when they should use size$$While fixing LANG-294 I noticed that there are two other places in StrBuilder that reference thisBuf.length and unless I'm mistaken they shouldn't.$$patch1-Lang-60-Developer$$Fix bug in StrBuilder . contains ( )$$1
Lang-34$$Use of ThreadLocals in ToStringStyle and HashCodeBuilder trigger memory leaks in container environments$$The thread local in org.apache.commons.lang3.builder.ToStringStyle is created but never removed and no API is provided to remove it. If a webapp's use of LANG triggers the loading of this class, a reference chain will be created that will cause a memory leak on web application reload. See http://markmail.org/thread/uetw2fdrsqgbh2cv for more info.$$patch1-Lang-34-Developer$$Fix null pointer check in toString style$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch1-Lang-33-Developer$$fix a bug in the array to null$$1
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch1-Lang-20-Developer$$Fix the bug of StringUtils parsing being called after I finished with the string utils$$1
Lang-18$$FastDateFormat formats year differently than SimpleDateFormat in Java 7$$Starting with Java 7 does SimpleDateFormat format a year pattern of 'Y' or 'YYY' as '2003' instead of '03' as in former Java releases. According Javadoc this pattern should have been always been formatted as number, therefore the new behavior seems to be a bug fix in the JDK. FastDateFormat is adjusted to behave the same.$$patch1-Lang-18-Developer$$Fix year field$$1
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-Developer$$Fix a bug in the NumberFormatUtils class .$$1
Lang-9$$FastDateParser does not handle unterminated quotes correctly$$FDP does not handled unterminated quotes the same way as SimpleDateFormat For example: Format: 'd'd' Date: d3 This should fail to parse the format and date but it actually works. The format is parsed as: Pattern: d(\p {IsNd} ++)$$patch1-Lang-9-Developer$$throw exception if pattern matcher regionEnd is not matched$$1
Lang-11$$RandomStringUtils throws confusing IAE when end <= start$$RandomUtils invokes Random#nextInt where n = end - start. If end <= start, then Random throws: java.lang.IllegalArgumentException: n must be positive This is confusing, and does not identify the source of the problem.$$patch1-Lang-11-Developer$$throw exception if end < start$$1
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1-Lang-7-Developer$$Handle spaces in string .$$1
Lang-29$$SystemUtils.getJavaVersionAsFloat throws StringIndexOutOfBoundsException on Android runtime/Dalvik VM$$Can be replicated in the Android emulator quite easily. Stack trace:   at org.apache.commons.lang.builder.ToStringBuilder.<clinit>(ToStringBuilder.java:98) E/AndroidRuntime( 1681): 	... 17 more E/AndroidRuntime( 1681): Caused by: java.lang.ExceptionInInitializerError E/AndroidRuntime( 1681): 	at org.apache.commons.lang.builder.ToStringStyle$MultiLineToStringStyle.<init>(ToStringStyle.java:2276) E/AndroidRuntime( 1681): 	at org.apache.commons.lang.builder.ToStringStyle.<clinit>(ToStringStyle.java:94) E/AndroidRuntime( 1681): 	... 18 more E/AndroidRuntime( 1681): Caused by: java.lang.StringIndexOutOfBoundsException E/AndroidRuntime( 1681): 	at java.lang.String.substring(String.java:1571) E/AndroidRuntime( 1681): 	at org.apache.commons.lang.SystemUtils.getJavaVersionAsFloat(SystemUtils.java:1153) E/AndroidRuntime( 1681): 	at org.apache.commons.lang.SystemUtils.<clinit>(SystemUtils.java:818)$$patch1-Lang-29-Developer$$Fix typo$$1
Lang-16$$NumberUtils does not handle upper-case hex: 0X and -0X$$NumberUtils.createNumber() should work equally for 0x1234 and 0X1234; currently 0X1234 generates a NumberFormatException Integer.decode() handles both upper and lower case hex.$$patch1-Lang-16-Developer$$missing patch$$1
Lang-42$$StringEscapeUtils.escapeHtml incorrectly converts unicode characters above U+00FFFF into 2 characters$$Characters that are represented as a 2 characters internaly by java are incorrectly converted by the function. The following test displays the problem quite nicely: import org.apache.commons.lang.*; public class J2 {     public static void main(String[] args) throws Exception {         // this is the utf8 representation of the character:         // COUNTING ROD UNIT DIGIT THREE         // in unicode         // codepoint: U+1D362         byte[] data = new byte[]  { (byte)0xF0, (byte)0x9D, (byte)0x8D, (byte)0xA2 } ;         //output is: &#55348;&#57186;         // should be: &#119650;         System.out.println("'" + StringEscapeUtils.escapeHtml(new String(data, "UTF8")) + "'");     } } Should be very quick to fix, feel free to drop me an email if you want a patch.$$patch1-Lang-42-Developer$$don ' t increment entity name in String entities$$1
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-Developer$$Fixed a bug in Lang_45_WordUtils where negative lower value was set$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-Developer$$Fix bug in CharSequenceTranslator$$1
Lang-28$$StringEscapeUtils.escapeXML() can't process UTF-16 supplementary characters$$Supplementary characters in UTF-16 are those whose code points are above 0xffff, that is, require more than 1 Java char to be encoded, as explained here: http://java.sun.com/developer/technicalArticles/Intl/Supplementary/ Currently, StringEscapeUtils.escapeXML() isn't aware of this coding scheme and treats each char as one character, which is not always right. A possible solution in class Entities would be:     public void escape(Writer writer, String str) throws IOException {         int len = str.length();         for (int i = 0; i < len; i++) {             int code = str.codePointAt;             String entityName = this.entityName(code);             if (entityName != null)  {                 writer.write('&');                 writer.write(entityName);                 writer.write(';');             }  else if (code > 0x7F)  {                     writer.write("&#");                     writer.write(code);                     writer.write(';');             }  else  {                     writer.write((char) code);             }              if (code > 0xffff)  {                     i++;             }         }     } Besides fixing escapeXML(), this will also affect HTML escaping functions. I guess that's a good thing, but please remember I have only tested escapeXML().$$patch1-Lang-28-Developer$$fix 64 bit detection failure$$1
Lang-17$$StringEscapeUtils.escapeXml(input) outputs wrong results when an input contains characters in Supplementary Planes.$$Hello. I use StringEscapeUtils.escapeXml(input) to escape special characters for XML. This method outputs wrong results when input contains characters in Supplementary Planes. String str1 = "\uD842\uDFB7" + "A"; String str2 = StringEscapeUtils.escapeXml(str1); // The value of str2 must be equal to the one of str1, // because str1 does not contain characters to be escaped. // However, str2 is diffrent from str1. System.out.println(URLEncoder.encode(str1, "UTF-16BE")); //%D8%42%DF%B7A System.out.println(URLEncoder.encode(str2, "UTF-16BE")); //%D8%42%DF%B7%FF%FD The cause of this problem is that the loop to translate input character by character is wrong. In CharSequenceTranslator.translate(CharSequence input, Writer out), loop counter "i" moves from 0 to Character.codePointCount(input, 0, input.length()), but it should move from 0 to input.length().$$patch1-Lang-17-Developer$$Fix bug in CharSequenceTranslator$$1
Lang-1$$NumberUtils does not handle Long Hex numbers$$NumberUtils.createLong() does not handle hex numbers, but createInteger() handles hex and octal. This seems odd. NumberUtils.createNumber() assumes that hex numbers can only be Integer. Again, why not handle bigger Hex numbers? == It is trivial to fix createLong() - just use Long.decode() instead of valueOf(). It's not clear why this was not done originally - the decode() method was added to both Integer and Long in Java 1.2. Fixing createNumber() is also fairly easy - if the hex string has more than 8 digits, use Long. Should we allow for leading zeros in an Integer?  If not, the length check is trivial.$$patch1-Lang-1-Developer$$Fix a bug in the hex number utils$$1
Lang-10$$FastDateParser does not handle white-space properly$$The SimpleDateFormat Javadoc does not treat white-space specially, however FastDateParser treats a single white-space as being any number of white-space characters. This means that FDP will parse dates that fail when parsed by SDP.$$patch1-Lang-10-Developer$$don ' t ignore white spaces in regex$$1
Lang-19$$StringIndexOutOfBoundsException when calling unescapeHtml4("&#03")$$When calling unescapeHtml4() on the String "&#03" (or any String that contains these characters) an Exception is thrown: Exception in thread "main" java.lang.StringIndexOutOfBoundsException: String index out of range: 4 	at java.lang.String.charAt(String.java:686) 	at org.apache.commons.lang3.text.translate.NumericEntityUnescaper.translate(NumericEntityUnescaper.java:49) 	at org.apache.commons.lang3.text.translate.AggregateTranslator.translate(AggregateTranslator.java:53) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:88) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:60) 	at org.apache.commons.lang3.StringEscapeUtils.unescapeHtml4(StringEscapeUtils.java:351)$$patch1-Lang-19-Developer$$fix small bug$$1
Lang-26$$FastDateFormat.format() outputs incorrect week of year because locale isn't respected$$FastDateFormat apparently doesn't respect the locale it was sent on creation when outputting week in year (e.g. "ww") in format(). It seems to use the settings of the system locale for firstDayOfWeek and minimalDaysInFirstWeek, which (depending on the year) may result in the incorrect week number being output. Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:  import java.util.Calendar; import java.util.Date; import java.util.Locale; import java.text.SimpleDateFormat;  import org.apache.commons.lang.time.FastDateFormat;  public class FastDateFormatWeekBugDemo {     public static void main(String[] args) {         Locale.setDefault(new Locale("en", "US"));         Locale locale = new Locale("sv", "SE");          Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome         cal.set(2010, 0, 1, 12, 0, 0);         Date d = cal.getTime();         System.out.println("Target date: " + d);          FastDateFormat fdf = FastDateFormat.getInstance("EEEE', week 'ww", locale);         SimpleDateFormat sdf = new SimpleDateFormat("EEEE', week 'ww", locale);         System.out.println("FastDateFormat:   " + fdf.format(d)); // will output "FastDateFormat:   fredag, week 01"         System.out.println("SimpleDateFormat: " + sdf.format(d)); // will output "SimpleDateFormat: fredag, week 53"     } }   If sv/SE is passed to Locale.setDefault() instead of en/US, both FastDateFormat and SimpleDateFormat output the correct week number.$$patch1-Lang-26-Developer$$Missing locale string for format ( )$$1
Lang-8$$FastDateFormat's "z" pattern does not respect timezone of Calendar instances passed to format()$$The work on LANG-462 has introduced a time zone formatting bug in FastDateFormat in commons-lang3. The problem can be seen by this snippet:  // Always prints timezone name of machine's default timezone, ignoring TZ // set on calendar, even though the printed time itself respects calendar's TZ. Calendar myCal = Calendar.getInstance(TimeZone.getTimeZone("US/Central")); System.out.println(FastDateFormat.getInstance("h:mma z").format(myCal));   If you happen to be in US/Central, this will print the right thing, but just try it with US/Eastern, US/Pacific, etc.  It will print the time in the correct timezone, but the timezone name at the end (the "z" pattern) will always be the system default timezone.  This is a regression against commons-lang 2.x. Basically, when the "forced time zone" code was removed, the TimeZoneNameRule class stopped respecting the Calendar instance's timezone, and instead now always uses the mTimeZone of the FastDateFormat instance itself (which is only supposed to be used when formatting timezone-less objects such as Date or long). The removal of the forced time zone stuff is surely the right thing to do (it was a mess).  I think the fix is to change the TimeZoneNameRule inner class to not take a TimeZone instance, but rather to use the TimeZone on the Calendar instance passed into appendTo(), just like TimeZoneNumberRule does.  Presumably then for efficiency, one would use the getTimeZoneDisplay() package-static method to quickly retrieve the required timezone's display name.$$patch1-Lang-8-Developer$$Fix bug$$1
Lang-21$$DateUtils.isSameLocalTime does not work correct$$Hi, I think I found a bug in the DateUtils class in the method isSameLocalTime. Example:  Calendar a = Calendar.getInstance(); a.setTimeInMillis(1297364400000L); Calendar b = Calendar.getInstance(); b.setTimeInMillis(1297321200000L); Assert.assertFalse(DateUtils.isSameLocalTime(a, b)); This is because the method compares  cal1.get(Calendar.HOUR) == cal2.get(Calendar.HOUR)  but I think it has to be  cal1.get(Calendar.HOUR_OF_DAY) == cal2.get(Calendar.HOUR_OF_DAY)$$patch1-Lang-21-Developer$$Fixed formatting issues$$1
Lang-44$$NumberUtils createNumber thows a StringIndexOutOfBoundsException when only an "l" is passed in.$$Seems to be similar to LANG-300, except that if you don't place a digit in front of the "l" or "L" it throws a StringIndexOutOfBoundsException instead.$$patch1-Lang-44-Developer$$fix a bug in StringUtils$$1
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-Developer$$don ' t put backslash at the end of a string in the pattern$$1
Lang-38$$DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations$$If a Calendar object is constructed in certain ways a call to Calendar.setTimeZone does not correctly change the Calendars fields.  Calling Calenar.getTime() seems to fix this problem.  While this is probably a bug in the JDK, it would be nice if DateFormatUtils was smart enough to detect/resolve this problem. For example, the following unit test fails:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";      // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)     // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);       FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }   However, this unit test passes:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);     cal.getTime();      FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }$$patch1-Lang-38-Developer$$Fixed weekdays for string output$$1
Lang-36$$NumberUtils.isNumber() Should Return True for Valid Number with a Trailing Decimal Place$$NumberUtils.isNumber() should return true for a valid number ending in a trailing decimal place; e.g., "2." should be considered a number because new BigDecimal("2.") works fine.  This could be done by adding the code below after line 1444, which is the if (chars[i] == 'e' || chars[i] == 'E') block. if (chars[i] == '.') {     if (hasDecPoint || hasExp)  {         // two decimal points or dec in exponent            return false;     }     return foundDigit; // single trailing decimal point after non-exponent is ok }$$patch1-Lang-36-Developer$$fix foundDigit = false for decimal point after non - exponent$$1
Lang-31$$StringUtils methods do not handle Unicode 2.0+ supplementary characters correctly.$$StringUtils.containsAny methods incorrectly matches Unicode 2.0+ supplementary characters. For example, define a test fixture to be the Unicode character U+20000 where U+20000 is written in Java source as "\uD840\uDC00" 	private static final String CharU20000 = "\uD840\uDC00"; 	private static final String CharU20001 = "\uD840\uDC01"; You can see Unicode supplementary characters correctly implemented in the JRE call: 	assertEquals(-1, CharU20000.indexOf(CharU20001)); But this is broken: 	assertEquals(false, StringUtils.containsAny(CharU20000, CharU20001)); 	assertEquals(false, StringUtils.containsAny(CharU20001, CharU20000)); This is fine: 	assertEquals(true, StringUtils.contains(CharU20000 + CharU20001, CharU20000)); 	assertEquals(true, StringUtils.contains(CharU20000 + CharU20001, CharU20001)); 	assertEquals(true, StringUtils.contains(CharU20000, CharU20000)); 	assertEquals(false, StringUtils.contains(CharU20000, CharU20001)); because the method calls the JRE to perform the match. More than you want to know:  http://java.sun.com/developer/technicalArticles/Intl/Supplementary/$$patch1-Lang-31-Developer$$capitalized search characters$$1
Lang-65$$[lang] DateUtils.truncate method is buggy when dealing with DST switching hours$$Try to truncate 2004-10-31 01:00:00 MDT by hour and you'll actually get 2004-10- 31 01:00:00 MST, which is one hour after the input hour.     // truncate 2004-10-31 01:00:00 MDT     Date oct31_01MDT = new Date(1099206000000L);         Date result = DateUtils.truncate(oct31_01MDT, Calendar.HOUR_OF_DAY);     assertEquals(oct31_01MDT, result);$$patch1-Lang-65-Developer$$Fixed for Persian and Urdu language maps # 1896$$1
Lang-62$$unescapeXml("&12345678;") should be "&12345678;"$$Following test (in EntitiesTest.java) fails:     public void testNumberOverflow() throws Exception  {         doTestUnescapeEntity("&#12345678;", "&#12345678;");         doTestUnescapeEntity("x&#12345678;y", "x&#12345678;y");         doTestUnescapeEntity("&#x12345678;", "&#x12345678;");         doTestUnescapeEntity("x&#x12345678;y", "x&#x12345678;y");     }  Maximim value for char is 0xFFFF, so &#12345678; is invalid entity reference, and so should be left as is.$$patch1-Lang-62-Developer$$fix a crash in parsing of string entity values from HIVE - 593$$1
Lang-54$$LocaleUtils.toLocale() rejects strings with only language+variant$$LocaleUtils.toLocale() throws an exception on strings containing a language and a variant but no country code. For example : fr__POSIX This string can be produced with the JDK by instanciating a Locale with an empty string for the country : new Locale("fr", "", "POSIX").toString(). According to the javadoc for the Locale class a variant is allowed with just a language code or just a country code. Commons Configuration handles this case in its PropertyConverter.toLocale() method. I'd like to replace our implementation by the one provided by LocaleUtils, but our tests fail due to this case.$$patch1-Lang-54-Developer$$Missing _ sign in locale format example$$1
Lang-53$$Dates.round() behaves incorrectly for minutes and seconds$$Get unexpected output for rounding by minutes or seconds. public void testRound() {     Calendar testCalendar = Calendar.getInstance(TimeZone.getTimeZone("GMT"));     testCalendar.set(2007, 6, 2, 8, 9, 50);     Date date = testCalendar.getTime();     System.out.println("Before round() " + date);     System.out.println("After round()  " + DateUtils.round(date, Calendar.MINUTE)); } --2.1 produces Before round() Mon Jul 02 03:09:50 CDT 2007 After round()  Mon Jul 02 03:10:00 CDT 2007  this is what I would expect --2.2 and 2.3 produces Before round() Mon Jul 02 03:09:50 CDT 2007 After round()  Mon Jul 02 03:01:00 CDT 2007  this appears to be wrong$$patch1-Lang-53-Developer$$Fix OSGi reported by @ jancborchardt$$1
Lang-30$$StringUtils methods do not handle Unicode 2.0+ supplementary characters correctly.$$StringUtils.containsAny methods incorrectly matches Unicode 2.0+ supplementary characters. For example, define a test fixture to be the Unicode character U+20000 where U+20000 is written in Java source as "\uD840\uDC00" 	private static final String CharU20000 = "\uD840\uDC00"; 	private static final String CharU20001 = "\uD840\uDC01"; You can see Unicode supplementary characters correctly implemented in the JRE call: 	assertEquals(-1, CharU20000.indexOf(CharU20001)); But this is broken: 	assertEquals(false, StringUtils.containsAny(CharU20000, CharU20001)); 	assertEquals(false, StringUtils.containsAny(CharU20001, CharU20000)); This is fine: 	assertEquals(true, StringUtils.contains(CharU20000 + CharU20001, CharU20000)); 	assertEquals(true, StringUtils.contains(CharU20000 + CharU20001, CharU20001)); 	assertEquals(true, StringUtils.contains(CharU20000, CharU20000)); 	assertEquals(false, StringUtils.contains(CharU20000, CharU20001)); because the method calls the JRE to perform the match. More than you want to know:  http://java.sun.com/developer/technicalArticles/Intl/Supplementary/$$patch1-Lang-30-Developer$$Improved containsAny ( )$$1
Lang-37$$ArrayUtils.addAll(T[] array1, T... array2) does not handle mixed types very well$$ArrayUtils.addAll(T[] array1, T... array2) does not handle mixed array types very well. The stack trace for  Number[] st = ArrayUtils.addAll(new Integer[] {1} , new Long[] {2L} ); starts: java.lang.ArrayStoreException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang3.ArrayUtils.addAll(ArrayUtils.java:2962) which is not all that obvious. It would be a lot clearer if the method threw an IlegalArgumentException or similar.$$patch1-Lang-37-Developer$$Fix broken patch$$1
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-Developer$$fix null values in string utils$$1
Lang-52$$StringEscapeUtils.escapeJavaScript() method did not escape '/' into '\/', it will make IE render page uncorrectly$$If Javascripts including'/', IE will parse the scripts uncorrectly, actually '/' should be escaped to '\/'. For example, document.getElementById("test").value = '<script>alert(\'aaa\');</script>';this expression will make IE render page uncorrect, it should be document.getElementById("test").value = '<script>alert(\'aaa\');<\/script>'; Btw, Spring's JavascriptEscape behavor is correct. Try  to run below codes, you will find the difference:   String s = "<script>alert('aaa');</script>";   String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);   System.out.println("Spring JS Escape : "+str);   str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);   System.out.println("Apache Common Lang JS Escape : "+ str);$$patch1-Lang-52-Developer$$Add backslash backslash$$1
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch1-Lang-55-Developer$$Add missing stop time assignment in Stopwatch$$1
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch1-Lang-63-Developer$$Fixed test for M$$1
Lang-64$$ValuedEnum.compareTo(Object other) not typesafe - it easily could be...$$int org.apache.commons.lang.enums.ValuedEnum.compareTo(Object other)  is not typesafe - if the int-values are the same, it will return "0" even for two totally different sub-classes of ValuedEnum$$patch1-Lang-64-Developer$$Fix a bug in ValuedEnum$$1
Lang-46$$StringEscapeUtils.escapeJava(String) escapes '/' characters$$Commons Lang 2.4 StringEscapeUtils.escapeJava(String) now escapes '/' characters, which is not a valid "escapable" character in Java strings.  I haven't tried the other Java escape/unescape methods to see if they have a similar problem, or that only Java "escapable" characters are escaped by escapeJava(String). This bug may have appeared as an unintended side-effect of the fix for LANG-363. Also the javadoc for escapeJava is now a little off, in that '/' should now be included in the sentence describing the differences between Java and Javascript strings, with respect to escaping rules. The following is a JUnit3 test demonstrating the bug. import junit.framework.TestCase; import org.apache.commons.lang.StringEscapeUtils; public class StringEscapeUtilsTest extends TestCase {     public void testEscapeJavaWithSlash()  {         final String input = "String with a slash (/) in it";                  final String expected = input;         final String actual   = StringEscapeUtils.escapeJava( input );          /**          * In 2.4 StringEscapeUtils.escapeJava(String) escapes '/' characters,          * which are not a valid character to escape in a Java string.            */         assertEquals( expected, actual );     } }$$patch1-Lang-46-Developer$$Don ' t escape backslash backslash when closing string separators , see EvilSeph .$$1
Lang-41$$ClassUtils.getShortClassName() will not work with an array;  it seems to add a semicolon to the end.$$A semicolon is introduced into the class name at the end for all arrays... String sArray[] = new String[2]; sArray[0] = "mark"; sArray[1] = "is cool"; String simpleString = "chris"; assertEquals("String", ClassUtils.getShortClassName(simpleString, null)); assertEquals("String;", ClassUtils.getShortClassName(sArray, null));$$patch1-Lang-41-Developer$$Fix reverse abbreviation map presence$$1
Lang-48$$EqualsBuilder don't compare BigDecimals correctly$$When comparing a BigDecimal, the comparing is made using equals, not compareTo, which is more appropriate in the case of BigDecimal.$$patch1-Lang-48-Developer$$Fixed a bug in equals builder$$1
Lang-24$$NumberUtils.isNumber(String)  is not right when the String is "1.1L"$$"1.1L"  is not a Java Number . but NumberUtils.isNumber(String) return true. perhaps change:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp;             }   to:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp && !hasDecPoint;             }$$patch1-Lang-24-Developer$$allow L with a decimal point$$1
Lang-23$$text.ExtendedMessageFormat doesn't override java.text.MessageFormat.equals(Object)$$Findbugs: Bug: org.apache.commons.lang3.text.ExtendedMessageFormat doesn't override java.text.MessageFormat.equals(Object) Pattern id: EQ_DOESNT_OVERRIDE_EQUALS, type: Eq, category: STYLE This class extends a class that defines an equals method and adds fields, but doesn't define an equals method itself. Thus, equality on instances of this class will ignore the identity of the subclass and the added fields. Be sure this is what is intended, and that you don't need to override the equals method. Even if you don't need to override the equals method, consider overriding it anyway to document the fact that the equals method for the subclass just return the result of invoking super.equals(o).$$patch1-Lang-23-Developer$$Fix a bug in the extender class$$1
Lang-4$$LookupTranslator accepts CharSequence as input, but fails to work with implementations other than String$$The core of org.apache.commons.lang3.text.translate is a HashMap<CharSequence, CharSequence> lookupMap. From the Javadoc of CharSequence (emphasis mine):  This interface does not refine the general contracts of the equals and hashCode methods. The result of comparing two objects that implement CharSequence is therefore, in general, undefined. Each object may be implemented by a different class, and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. It is therefore inappropriate to use arbitrary CharSequence instances as elements in a set or as keys in a map. The current implementation causes code such as the following to not work as expected:  CharSequence cs1 = "1 < 2"; CharSequence cs2 = CharBuffer.wrap("1 < 2".toCharArray());  System.out.println(StringEscapeUtils.ESCAPE_HTML4.translate(cs1)); System.out.println(StringEscapeUtils.ESCAPE_HTML4.translate(cs2));   ... which gives the following results (but should be identical):  1 &lt; 2 1 < 2   The problem, at a minimum, is that CharBuffer.equals is even documented in the Javadoc that:  A char buffer is not equal to any other type of object. ... so a lookup on a CharBuffer in the Map will always fail when compared against the String implementations that it contains. An obvious work-around is to instead use something along the lines of either of the following:  System.out.println(StringEscapeUtils.ESCAPE_HTML4.translate(cs2.toString())); System.out.println(StringEscapeUtils.escapeHtml4(cs2.toString()));   ... which forces everything back to a String.  However, this is not practical when working with large sets of data, which would require significant heap allocations and garbage collection concerns.  (As such, I was actually trying to use the translate method that outputs to a Writer - but simplified the above examples to omit this.) Another option that I'm considering is to use a custom CharSequence wrapper around a char[] that implements hashCode() and equals() to work with those implemented on String.  (However, this will be interesting due to the symmetric assumption - which is further interesting that String.equals is currently implemented using instanceof - even though String is final...)$$patch1-Lang-4-Developer$$String conversion to HashMap < CharSequence , CharSequence >$$1
Lang-15$$TypeUtils.getTypeArguments() misses type arguments for partially-assigned classes$$failing test code to add to TypeUtilsTest.testGetTypeArguments():  typeVarAssigns = TypeUtils.getTypeArguments(Other.class, This.class); Assert.assertEquals(2, typeVarAssigns.size()); Assert.assertEquals(String.class, typeVarAssigns.get(This.class.getTypeParameters()[0])); Assert.assertEquals(Other.class.getTypeParameters()[0], typeVarAssigns.get(This.class.getTypeParameters()[1]));   These should pass based on:   public interface This<K, V> { }  public class Other<T> implements This<String, T> { }   This case fails because the current code ignores the Other class due to its specifying its own type variables, which is obviously incorrect.  This report is extrapolated from an offline report received by Hen.$$patch1-Lang-15-Developer$$fix a bug in TypeUtils$$1
Lang-3$$Method createNumber from NumberUtils doesn't work for floating point numbers other than Float$$Method createNumber from NumberUtils is trying to parse a string with a floating point number always first as a Float, that will cause that if we send a string with a number that will need a Double or even a BigDecimal the number will be truncate to accommodate into the Float without an exception to be thrown, so in fact we will no be returning ever neither a Double nor a BigDecimal.$$patch1-Lang-3-Developer$$Fix typo in human_patches$$1
Lang-12$$RandomStringUtils.random(count, 0, 0, false, false, universe, random) always throws java.lang.ArrayIndexOutOfBoundsException$$In commons-lang 2.6 line 250 :  ch = chars[random.nextInt(gap) + start];  This line of code takes a random int to fetch a char in the chars array regardless of its size. (Besides start is useless here) Fixed version would be :  //ch = chars[random.nextInt(gap)%chars.length];  When user pass 0 as end or when the array is not null but empty this line ends up with an exception$$patch1-Lang-12-Developer$$Fix exception in Lang_12_RandomStringUtils_t$$1
Lang-49$$infinite loop in Fraction.reduce when numerator == 0$$Summary pretty much says it all.$$patch1-Lang-49-Developer$$fix # 1796$$1
Lang-40$$Fix case-insensitive string handling$$String.to*Case() is locale-sensitive, this is usually not intended for case-insensitive comparisions. Please see Common Bug #3 for details.$$patch1-Lang-40-Developer$$Improve StringUtils presence code$$1
Lang-47$$StrBuilder appendFixedWidth does not handle nulls$$Appending a null value with fixed width causes a null pointer exception if getNullText() has not been set.$$patch1-Lang-47-Developer$$Fix the build .$$1
Lang-2$$None$$None$$patch1-Lang-2-Developer$$Handle contains ( ) for Java 7$$1
Lang-13$$SerializationUtils throws ClassNotFoundException when cloning primitive classes$$If a serializable object contains a reference to a primitive class, e.g. int.class or int[].class, the SerializationUtils throw a ClassNotFoundException when trying to clone that object.  import org.apache.commons.lang3.SerializationUtils; import org.junit.Test;   public class SerializationUtilsTest {  	 	@Test 	public void primitiveTypeClassSerialization(){ 		Class<?> primitiveType = int.class; 		 		Class<?> clone = SerializationUtils.clone(primitiveType); 		assertEquals(primitiveType, clone); 	} }   The problem was already reported as a java bug http://bugs.sun.com/view_bug.do?bug_id=4171142 and ObjectInputStream is fixed since java version 1.4. The SerializationUtils problem arises because the SerializationUtils internally use the ClassLoaderAwareObjectInputStream that overrides the ObjectInputStream's resoleClass method without delegating to the super method in case of a ClassNotFoundException. I understand the intention of the ClassLoaderAwareObjectInputStream, but this implementation should also implement a fallback to the original implementation. For example:          protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {             String name = desc.getName();             try {                 return Class.forName(name, false, classLoader);             } catch (ClassNotFoundException ex) {             	try {             	     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());             	} catch (Exception e) { 		     return super.resolveClass(desc); 		}             }         }   Here is the code in ObjectInputStream that fixed the java bug.      protected Class<?> resolveClass(ObjectStreamClass desc) 	throws IOException, ClassNotFoundException     { 	String name = desc.getName(); 	try { 	    return Class.forName(name, false, latestUserDefinedLoader()); 	} catch (ClassNotFoundException ex) { 	    Class cl = (Class) primClasses.get(name); 	    if (cl != null) { 		return cl; 	    } else { 		throw ex; 	    } 	}     }$$patch1-Lang-13-Developer$$Fix possible NPE in ODS file$$1
Lang-5$$LocaleUtils.toLocale does not parse strings starting with an underscore$$Hi, Javadocs of Locale.toString() states that "If the language is missing, the string will begin with an underbar.". This is not handled in the LocaleUtils.toLocale method if it is meant to be the inversion method of Locale.toString(). The fix for the ticket 328 does not handle well the case "fr__P", which I found out during fixing the first bug. I am attaching the patch for both problems.$$patch1-Lang-5-Developer$$I am using the LocaleUtils class for Java 8 ( UK )$$1
Lang-14$$StringUtils equals() relies on undefined behavior$$Since the java.lang.CharSequence class was first introduced in 1.4, the JavaDoc block has contained the following note:  This interface does not refine the general contracts of the equals and hashCode methods. The result of comparing two objects that implement CharSequence is therefore, in general, undefined. Each object may be implemented by a different class, and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. When the signature of the StringUtils equals() method was changed from equals(String, String) to equals(CharSequence, CharSequence) in R920543, the implementation still relied on calling CharSequence#equals(Object) even though, in general, the result is undefined. One example where equals(Object) returns false even though, as CharSequences, two objects represent equal sequences is when one object is an instance of javax.lang.model.element.Name and the other object is a String.$$patch1-Lang-14-Developer$$fixed test data$$1
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch1-Lang-22-Developer$$Fix a bug in the greatest common divisor$$1
Lang-25$$Some Entitys like &Ouml; are not matched properly against its ISO8859-1 representation$$In EntityArrays  In  private static final String[][] ISO8859_1_ESCAPE  some matching is wrong, for example            {"\u00D7", "&Ouml;"}, //  - uppercase O, umlaut         {"\u00D8", "&times;"}, // multiplication sign   but this must be              {"\u00D6", "&Ouml;"}, //  - uppercase O, umlaut         {"\u00D7", "&times;"}, // multiplication sign   according to http://www.fileformat.info/info/unicode/block/latin_supplement/list.htm First look: u00CA is missing in the array and all following entries are matched wrong by an offset of 1. Found on http://stackoverflow.com/questions/4172784/bug-in-apache-commons-stringescapeutil/4172915#4172915$$patch1-Lang-25-Developer$$$$1
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-SketchFix$$Fix try / catch in minimizeExitPoints$$1
Closure-14$$bogus 'missing return' warning$$None$$patch1-Closure-14-SketchFix$$Fix finally map .$$1
Chart-20$$None$$None$$patch1-Chart-20-SketchFix$$Fix ValueMarker constructor to ignore the outline paint for now .$$1
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-SketchFix$$Fix empty range in TimeSeries$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-SketchFix$$fix an issue with TwoDimTest$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-SketchFix$$Fix nullability note in AbstractCategoryItemRenderer$$1
Chart-8$$None$$None$$patch1-Chart-8-SketchFix$$Missing constructor .$$1
Chart-24$$None$$None$$patch1-Chart-24-SketchFix$$Fix bug in GrayPaintScale$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-SketchFix$$Fix NaN in FastMath . max ( a , b )$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-SketchFix$$Fix renegation of baseSecantSolver$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-SketchFix$$reduce error in SimplexTableau$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-SketchFix$$fixed erroneous loop$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-SketchFix$$Fixed an error in the linear search .$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-SketchFix$$Fix NaN - > INF in Complex$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-SketchFix$$StrBuilder . append ( obj , width )$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-SketchFix$$Fix bug in CharSequenceTranslator$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-SketchFix$$Improved nullability in CategoryItemRenderer$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch2-Chart-1-SketchFix$$Fix CategoryItemRenderer to handle non - null datasets$$0
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-SketchFix$$Fix a bug in SimplexSolver . compareTo ( double , double , double , double ) .$$0
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-SketchFix$$fixed a typo in BisectionSolver . solve ( )$$0
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch2-Math-70-SketchFix$$fixed a typo in BisectionSolver . solve ( )$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-SketchFix$$Using less restrictive default value for minRatio$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch2-Math-82-SketchFix$$Fixed a bug in the SimplexSolver . getEntry ( ) method where the ratio was too high$$0
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-SketchFix$$Don ' t reverse the changes since we ' re about to manipulate the codepoint count$$0
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch2-Lang-6-SketchFix$$Use the writer null pointer for CharSequenceTranslator$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-DynaMoth$$FastMath . abs ( x ) doesn ' t allow false positives in baseSecantSolver$$1
Chart-18$$None$$None$$patch1-Chart-18-DynaMoth$$Add column remove after filtering$$0
Chart-15$$None$$None$$patch1-Chart-15-DynaMoth$$Exported new plot area for Android 4 . 0 .$$0
Chart-13$$None$$None$$patch1-Chart-13-DynaMoth$$Fix swapped line lengths for left block$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-DynaMoth$$don ' t sort XYSeries by default$$0
Chart-25$$None$$None$$patch1-Chart-25-DynaMoth$$StatisticalBarRenderer was throwing an error if the axes are not horizontal$$0
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?$$patch1-Math-32-DynaMoth$$Fixed euclidean2d tree attribute$$0
Math-105$$[math]  SimpleRegression getSumSquaredErrors$$getSumSquaredErrors returns -ve value. See test below: public void testSimpleRegression() { 		double[] y =  {  8915.102, 8919.302, 8923.502} ; 		double[] x =  { 1.107178495, 1.107264895, 1.107351295} ; 		double[] x2 =  { 1.107178495E2, 1.107264895E2, 1.107351295E2} ; 		SimpleRegression reg = new SimpleRegression(); 		for (int i = 0; i < x.length; i++)  { 			reg.addData(x[i],y[i]); 		} 		assertTrue(reg.getSumSquaredErrors() >= 0.0); // OK 		reg.clear(); 		for (int i = 0; i < x.length; i++)  { 			reg.addData(x2[i],y[i]); 		} 		assertTrue(reg.getSumSquaredErrors() >= 0.0); // FAIL 	}$$patch1-Math-105-DynaMoth$$fixed a small bug in SimpleRegression$$0
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-DynaMoth$$Fix cost relative tolerance in LevenbergMarquardtOptimizer . java$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-Math-20-DynaMoth$$Fix CMAESOptimizer ' s min max value .$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-DynaMoth$$EigenDecompositionImpl . java +++ added jacoco code$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch1-Math-8-DynaMoth$$DiscreteDistribution was acting up with a false log output$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-DynaMoth$$EigenDecompositionImpl . isFullRank = true ; removed extraneous check$$0
Math-41$$One of Variance.evaluate() methods does not work correctly$$The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset. Similar method in Mean class seems to work. I did not check other methods taking the part of the array; they may have the same problem. Workaround: I had to shrink my arrays and use the method without the length.$$patch1-Math-41-DynaMoth$$Fix the var test .$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-DynaMoth$$Fix a merge conflict in UnivariateRealSolverUtils$$0
Math-71$$ODE integrator goes past specified end of integration range$$End of integration range in ODE solving is handled as an event. In some cases, numerical accuracy in events detection leads to error in events location. The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.    public void testMissedEvent() throws IntegratorException, DerivativeException {           final double t0 = 1878250320.0000029;           final double t =  1878250379.9999986;           FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {                          public int getDimension() {                 return 1;             }                          public void computeDerivatives(double t, double[] y, double[] yDot)                 throws DerivativeException {                 yDot[0] = y[0] * 1.0e-6;             }         };          DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,                                                                                1.0e-10, 1.0e-10);          double[] y = { 1.0 };         integrator.setInitialStepSize(60.0);         double finalT = integrator.integrate(ode, t0, y, t, y);         Assert.assertEquals(t, finalT, 1.0e-6);     }$$patch1-Math-71-DynaMoth$$changed default value of initialStep to 1 . 0 if false$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-DynaMoth$$Added a false to minValue so it is not lost$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-DynaMoth$$Add epsilon option to Fix MapRealVector remove ( )$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch1-Time-11-DynaMoth$$fixed typo$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-DynaMoth$$missing if ($$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch1-Lang-63-DynaMoth$$Fix merge conflict for end . add ( field , newdiff )$$0
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-Elixir$$Fix empty range in TimeSeries$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-Elixir$$fix an issue with TwoDimTest$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-Elixir$$Fix nullability note in AbstractCategoryItemRenderer$$1
Chart-8$$None$$None$$patch1-Chart-8-Elixir$$Missing zone argument from week constructor$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-Elixir$$Fix NaN in FastMath . max ( )$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-Elixir$$Fix renegation of baseSecantSolver$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-Elixir$$Fix int overflow$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-Elixir$$GaussianFitter . fit ( ) now uses 1 . 18 . 0 ( and is using$$1
Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-Math-34-Elixir$$Fix ListPopulation . iterator ( )$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-Elixir$$reduce error in SimplexTableau$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-Elixir$$Fix Frequency . getPct ( Object )$$1
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles$$patch1-Math-30-Elixir$$changed int to double .$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-Elixir$$fixed erroneous loop$$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-Elixir$$Fix a false reporting of convergence$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-Elixir$$Fixed an error in the linear search .$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-Elixir$$Fix NaN - > INF in Complex$$1
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch1-Time-4-Elixir$$Fix partial constructor to validate values with chronoology$$1
Time-15$$possibly a bug in org.joda.time.field.FieldUtils.safeMultiply$$It seems to me that as currently written in joda-time-2.1.jar org.joda.time.field.FieldUtils.safeMultiply(long val1, int scalar) doesn't detect the overflow if the long val1 == Long.MIN_VALUE and the int scalar == -1.  The attached file demonstrates what I think is the bug and suggests a patch.  I looked at the Joda Time bugs list in SourceForge but couldn't see anything that looked relevant.$$patch1-Time-15-Elixir$$Fix fieldUtils$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-Elixir$$StrBuilder . append ( obj , width )$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-Elixir$$Fixed LocaleUtils # isAvailableLocale ( )$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch1-Lang-33-Elixir$$Fix NPE triggered by nullability exception$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-Elixir$$Fix bug in CharSequenceTranslator$$1
Lang-26$$FastDateFormat.format() outputs incorrect week of year because locale isn't respected$$FastDateFormat apparently doesn't respect the locale it was sent on creation when outputting week in year (e.g. "ww") in format(). It seems to use the settings of the system locale for firstDayOfWeek and minimalDaysInFirstWeek, which (depending on the year) may result in the incorrect week number being output. Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:  import java.util.Calendar; import java.util.Date; import java.util.Locale; import java.text.SimpleDateFormat;  import org.apache.commons.lang.time.FastDateFormat;  public class FastDateFormatWeekBugDemo {     public static void main(String[] args) {         Locale.setDefault(new Locale("en", "US"));         Locale locale = new Locale("sv", "SE");          Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome         cal.set(2010, 0, 1, 12, 0, 0);         Date d = cal.getTime();         System.out.println("Target date: " + d);          FastDateFormat fdf = FastDateFormat.getInstance("EEEE', week 'ww", locale);         SimpleDateFormat sdf = new SimpleDateFormat("EEEE', week 'ww", locale);         System.out.println("FastDateFormat:   " + fdf.format(d)); // will output "FastDateFormat:   fredag, week 01"         System.out.println("SimpleDateFormat: " + sdf.format(d)); // will output "SimpleDateFormat: fredag, week 53"     } }   If sv/SE is passed to Locale.setDefault() instead of en/US, both FastDateFormat and SimpleDateFormat output the correct week number.$$patch1-Lang-26-Elixir$$Missing locale argument$$1
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-Elixir$$don ' t append QUOTE if escaping is on$$1
Lang-38$$DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations$$If a Calendar object is constructed in certain ways a call to Calendar.setTimeZone does not correctly change the Calendars fields.  Calling Calenar.getTime() seems to fix this problem.  While this is probably a bug in the JDK, it would be nice if DateFormatUtils was smart enough to detect/resolve this problem. For example, the following unit test fails:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";      // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)     // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);       FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }   However, this unit test passes:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);     cal.getTime();      FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }$$patch1-Lang-38-Elixir$$Reset timeZone on parse ( ) .$$1
Lang-24$$NumberUtils.isNumber(String)  is not right when the String is "1.1L"$$"1.1L"  is not a Java Number . but NumberUtils.isNumber(String) return true. perhaps change:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp;             }   to:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp && !hasDecPoint;             }$$patch1-Lang-24-Elixir$$allow L with a decimal point$$1
Chart-17$$cloning of TimeSeries$$It's just a minor bug!  When I clone a TimeSeries which has no items, I get an IllegalArgumentException ("Requires start <= end"). But I don't think the user should be responsible for checking whether the TimeSeries has any items or not.$$patch1-Chart-17-Elixir$$Fix a bug in TimeSeries delete .$$0
Chart-3$$None$$None$$patch1-Chart-3-Elixir$$AddOrUpdate copy of TimeSeries$$0
Chart-13$$None$$None$$patch1-Chart-13-Elixir$$Fix border arrangement$$0
Math-104$$Special functions not very accurate$$The Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I'd expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.$$patch1-Math-104-Elixir$$reduce gamma term max iterations$$0
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?$$patch1-Math-32-Elixir$$Fixed a bug in PolygonsSet . getAttribute$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-Math-20-Elixir$$remove maxiterations as it is not compatible with Java 8$$0
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-Math-73-Elixir$$changed BrentSolver to solve with provided initial guess$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-Elixir$$Fixed a bug in EigenDecompositionImpl . java$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-Elixir$$Fix MathUtils . equals$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-Elixir$$Fix an issue with Double and Double .$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch1-Time-11-Elixir$$Stops bad behavior$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-Elixir$$missing closing paren in BooleanUtils$$0
Lang-58$$NumberUtils.createNumber throws NumberFormatException for one digit long$$NumberUtils.createNumber throws a NumberFormatException when parsing "1l", "2l" .. etc... It works fine if you try to parse "01l" or "02l".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for "1l"$$patch1-Lang-58-Elixir$$isDigits ( numeric ) will ignore - > long$$0
Lang-44$$NumberUtils createNumber thows a StringIndexOutOfBoundsException when only an "l" is passed in.$$Seems to be similar to LANG-300, except that if you don't place a digit in front of the "l" or "L" it throws a StringIndexOutOfBoundsException instead.$$patch1-Lang-44-Elixir$$Fix parseLong ( ) where - > parseLong ( ) works$$0
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-Elixir$$removed loop$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch1-Closure-92-PraPR$$Fix up whitespace$$1
Closure-93$$None$$None$$patch1-Closure-93-PraPR$$Remove lastIndexOf .$$1
Closure-18$$Dependency sorting with closurePass set to false no longer works.$$None$$patch1-Closure-18-PraPR$$Remove the redundant check on needsManagement ( )$$1
Closure-11$$Record type invalid property not reported on function with @this annotation$$None$$patch1-Closure-11-PraPR$$Fix TypeCheck . ILLEGAL_PROPERTY_ACCESS check$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-PraPR$$Remove spurious check for byte order .$$1
Closure-10$$Wrong code generated if mixing types in ternary operator$$None$$patch1-Closure-10-PraPR$$Remove mayBeStringHelper from NodeUtil . allResultsMatch ( ) .$$1
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch1-Closure-86-PraPR$$Allow new constructor to be used as a literal value .$$1
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-PraPR$$Remove deprecated try / catch block .$$1
Closure-31$$Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLY$$None$$patch1-Closure-31-PraPR$$Allow closure passes in the dependencies stream .$$1
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-PraPR$$Remove the unnecessary boolean condition$$1
Closure-63$$None$$None$$patch1-Closure-63-PraPR$$Remove the if / else .$$1
Closure-46$$ClassCastException during TypeCheck pass$$None$$patch1-Closure-46-PraPR$$Remove deprecated API in RecordType . getLeastSupertype .$$1
Closure-70$$unexpected typed coverage of less than 100%$$None$$patch1-Closure-70-PraPR$$Fix swapped - in case .$$1
Closure-14$$bogus 'missing return' warning$$None$$patch1-Closure-14-PraPR$$Fix control flow analysis$$1
Chart-20$$None$$None$$patch1-Chart-20-PraPR$$Remove redundant constructor .$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-PraPR$$Remove redundant line$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-PraPR$$Fix null pointer check in AbstractCategoryItemRenderer$$1
Chart-26$$None$$None$$patch1-Chart-26-PraPR$$Fix null pointer check .$$1
Chart-8$$None$$None$$patch1-Chart-8-PraPR$$Fix typo$$1
Chart-24$$None$$None$$patch1-Chart-24-PraPR$$Fix bounds$$1
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-PraPR$$Missing call to setDataset ( ) , fixes # 144$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-PraPR$$Fix max .$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-PraPR$$Fix BaseSecantSolver . java$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-PraPR$$Added missing param for fit ( )$$1
Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-Math-34-PraPR$$added iterator on chromosomes$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-PraPR$$Removing epsilon from column to drop list .$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-PraPR$$Fix getFrequency ( Object ) to return getCumPct ( Object )$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-PraPR$$fixed a bug in BisectionSolver$$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-PraPR$$remove erroneous loop$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-PraPR$$revert 8e704f627c7b03427ab1b71bb$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-PraPR$$Extend Complex . java for reference EInfinity$$1
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch1-Time-11-PraPR$$Stops verbose output$$1
Time-19$$Inconsistent interpretation of ambiguous time during DST$$The inconsistency appears for timezone Europe/London.  These three DateTime objects should all represent the same moment in time even if they are ambiguous. Now, it always returns the earlier instant (summer time) during an overlap.$$patch1-Time-19-PraPR$$fixed # 77$$1
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch1-Time-4-PraPR$$Fix partial constructor to not use iChronology directly in the partial constructor$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-PraPR$$Removed reverseEach method from StrBuilder$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-PraPR$$Updated LocaleUtils$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-PraPR$$Remove unused local variable$$1
Lang-10$$FastDateParser does not handle white-space properly$$The SimpleDateFormat Javadoc does not treat white-space specially, however FastDateParser treats a single white-space as being any number of white-space characters. This means that FDP will parse dates that fail when parsed by SDP.$$patch1-Lang-10-PraPR$$FastDateParser now ignores whitespaces$$1
Lang-26$$FastDateFormat.format() outputs incorrect week of year because locale isn't respected$$FastDateFormat apparently doesn't respect the locale it was sent on creation when outputting week in year (e.g. "ww") in format(). It seems to use the settings of the system locale for firstDayOfWeek and minimalDaysInFirstWeek, which (depending on the year) may result in the incorrect week number being output. Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:  import java.util.Calendar; import java.util.Date; import java.util.Locale; import java.text.SimpleDateFormat;  import org.apache.commons.lang.time.FastDateFormat;  public class FastDateFormatWeekBugDemo {     public static void main(String[] args) {         Locale.setDefault(new Locale("en", "US"));         Locale locale = new Locale("sv", "SE");          Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome         cal.set(2010, 0, 1, 12, 0, 0);         Date d = cal.getTime();         System.out.println("Target date: " + d);          FastDateFormat fdf = FastDateFormat.getInstance("EEEE', week 'ww", locale);         SimpleDateFormat sdf = new SimpleDateFormat("EEEE', week 'ww", locale);         System.out.println("FastDateFormat:   " + fdf.format(d)); // will output "FastDateFormat:   fredag, week 01"         System.out.println("SimpleDateFormat: " + sdf.format(d)); // will output "SimpleDateFormat: fredag, week 53"     } }   If sv/SE is passed to Locale.setDefault() instead of en/US, both FastDateFormat and SimpleDateFormat output the correct week number.$$patch1-Lang-26-PraPR$$FastDateFormat should use the locale$$1
Mockito-29$$fixed a verify() call example in @Captor javadoc.$$None$$patch1-Mockito-29-PraPR$$removed null check$$1
Mockito-38$$Generate change list separated by types using labels$$As discussed on the mailing list instead of one big list of "Improvements" the change list for the release is divided into change types based on labels. It is required to specify which labels should be considered separately. Some other labels can be excluded (like "question" or "refactoring"). There is also headerForOtherChanges method to override default "Other" header.$$patch1-Mockito-38-PraPR$$fixed NPE$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-CapGen$$fix merge issue$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch2-Chart-11-CapGen$$added p2 . getPathIterator ( null ) to fix looping over PathIterator$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-CapGen$$Fix nullability note in AbstractCategoryItemRenderer$$1
Chart-8$$None$$None$$patch1-Chart-8-CapGen$$Missing constructor .$$1
Chart-24$$None$$None$$patch1-Chart-24-CapGen$$Fix bug in GrayPaintScale$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-CapGen$$Fix NaN in FastMath . max ( a , b )$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-CapGen$$Fix int overflow$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-CapGen$$Added missing parameter .$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-CapGen$$reduce error in SimplexTableau$$1
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-CapGen$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted ( ) .$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-CapGen$$Fix getFrequency ( Object ) to return precise value instead of getCumPct ( Object )$$1
Math-65$$weight versus sigma in AbstractLeastSquares$$In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.  Once corrected, getRMS() can even reduce  public double getRMS()  {return Math.sqrt(getChiSquare()/rows);}$$patch1-Math-65-CapGen$$Fix residualsWeights .$$1
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch1-Math-53-CapGen$$Add the isNaN check to Complex . add ( )$$1
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch2-Math-53-CapGen$$Add the isNaN check to Complex . add ( )$$1
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles$$patch1-Math-30-CapGen$$changed int to double .$$1
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-CapGen$$Fix a bug in MathUtils . equals ( double , double , int )$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-CapGen$$fixed a typo in BisectionSolver$$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-CapGen$$Fix a false reporting of convergence$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-CapGen$$Fix NaN - > org . apache . commons . math3 . complex . Complex$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-CapGen$$StrBuilder . append ( obj , width )$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-CapGen$$Use the available locale list$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch2-Lang-57-CapGen$$Use new java . util . HashSet ( ) instead of cAvailableLocaleSet$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch3-Lang-57-CapGen$$Fixed bug in LocaleUtils$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-CapGen$$Fix bug in CharSequenceTranslator$$1
Lang-26$$FastDateFormat.format() outputs incorrect week of year because locale isn't respected$$FastDateFormat apparently doesn't respect the locale it was sent on creation when outputting week in year (e.g. "ww") in format(). It seems to use the settings of the system locale for firstDayOfWeek and minimalDaysInFirstWeek, which (depending on the year) may result in the incorrect week number being output. Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:  import java.util.Calendar; import java.util.Date; import java.util.Locale; import java.text.SimpleDateFormat;  import org.apache.commons.lang.time.FastDateFormat;  public class FastDateFormatWeekBugDemo {     public static void main(String[] args) {         Locale.setDefault(new Locale("en", "US"));         Locale locale = new Locale("sv", "SE");          Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome         cal.set(2010, 0, 1, 12, 0, 0);         Date d = cal.getTime();         System.out.println("Target date: " + d);          FastDateFormat fdf = FastDateFormat.getInstance("EEEE', week 'ww", locale);         SimpleDateFormat sdf = new SimpleDateFormat("EEEE', week 'ww", locale);         System.out.println("FastDateFormat:   " + fdf.format(d)); // will output "FastDateFormat:   fredag, week 01"         System.out.println("SimpleDateFormat: " + sdf.format(d)); // will output "SimpleDateFormat: fredag, week 53"     } }   If sv/SE is passed to Locale.setDefault() instead of en/US, both FastDateFormat and SimpleDateFormat output the correct week number.$$patch1-Lang-26-CapGen$$Use the locale for the compiler .$$1
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch3-Lang-43-CapGen$$don ' t append QUOTE if escaping is on$$1
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch8-Math-80-CapGen$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted ( ) .$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-CapGen$$Fixed a bug in EigenDecompositionImpl . java$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch6-Math-80-CapGen$$Fix EigenDecompositionImpl . java$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch7-Math-80-CapGen$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted ( ) .$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch9-Math-80-CapGen$$Fix EigenDecompositionImpl . java$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch5-Math-80-CapGen$$EigenDecompositionImpl implements EigenDecomposition$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch2-Math-80-CapGen$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted ( ) .$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch10-Math-80-CapGen$$EigenDecompositionImpl implements EigenDecomposition$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch3-Math-80-CapGen$$Fixed a bug in EigenDecompositionImpl . java$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch4-Math-80-CapGen$$EigenDecompositionImpl implements EigenDecomposition$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch8-Math-63-CapGen$$Add missing MathUtils . equals ( double )$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-CapGen$$Fix MathUtils . equals ( double , double , int )$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch6-Math-63-CapGen$$Fix MathUtils . equals ( double , double )$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch7-Math-63-CapGen$$Fix MathUtils . equals ( double , double )$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch5-Math-63-CapGen$$Fix MathUtils . equals ( double , double )$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch2-Math-63-CapGen$$Fix a bug in MathUtils . equals$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch3-Math-63-CapGen$$Fix MathUtils . equals ( double , double )$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch4-Math-63-CapGen$$Add missing MathUtils . equals ( )$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-CapGen$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch2-Math-85-CapGen$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch3-Math-85-CapGen$$Fix a false reporting of convergence$$0
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-CapGen$$Add a isNaN check$$0
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch2-Math-5-CapGen$$Add the missing isNaN in Complex . isNaN ( ) .$$0
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch3-Math-5-CapGen$$Add the isNaN check$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch8-Lang-59-CapGen$$Add 5 more space for appendFixedWidthPadRight ( )$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-CapGen$$StrBuilder should add 5 elements for appendFixedWidthPadRight ( )$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch6-Lang-59-CapGen$$Add some space .$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch12-Lang-59-CapGen$$added more ensureCapacity$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch7-Lang-59-CapGen$$StrBuilder should add more space for appendFixedWidthPadRight ( )$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch9-Lang-59-CapGen$$Add 4 + 4 space for appendFixedWidthPadRight ( )$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch13-Lang-59-CapGen$$added more ensureCapacity$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch11-Lang-59-CapGen$$Add 4 + 4 ensureCapacity ( )$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch5-Lang-59-CapGen$$added ensureCapacity ( ) for string builder$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch2-Lang-59-CapGen$$StrBuilder should add 4 + 4 entries for appendFixedWidthPadRight ( )$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch10-Lang-59-CapGen$$Add some space for StringBuilder . appendFixedWidthPadRight ( )$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch3-Lang-59-CapGen$$StrBuilder should add 4 + 4 + 4 + 4 + 4 + 4 + 4 + 4 +$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch4-Lang-59-CapGen$$added ensureCapacity ( ) for string builder$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-CapGen$$don ' t escape quotes ( we ' ll check them here , but we ' re adding them$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch2-Lang-43-CapGen$$Fix lost backslash$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch3-Lang-43-CapGen$$Missing next ( pos ) call$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-KaliA$$Fix tryMinimizeExitPoints$$1
Closure-115$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1-Closure-115-KaliA$$disable side effects check for functions with side effects$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-KaliA$$Fix renegation of base secant solver .$$1
Closure-61$$Closure removes needed code.$$None$$patch1-Closure-61-KaliA$$Allow closure trees to be expanded if they are not equals .$$0
Closure-132$$if statement$$None$$patch1-Closure-132-KaliA$$Allow null check in closure trees$$0
Closure-59$$Cannot exclude globalThis checks through command line$$None$$patch1-Closure-59-KaliA$$Allow changes through to pass through to fix stones$$0
Closure-50$$Optimisation: convert array.join(",") to array.join()$$None$$patch1-Closure-50-KaliA$$Add a missing if / else .$$0
Closure-68$$Cryptic error message on invalid "@type function" annotation$$None$$patch1-Closure-68-KaliA$$Fix possible duplicated parse of EOF$$0
Closure-67$$Advanced compilations renames a function and then deletes it, leaving a reference to a renamed but non-existent function$$None$$patch1-Closure-67-KaliA$$"add patch for "" RemoveUnusedPrototypeProperties "" to"$$0
Closure-33$$weird object literal invalid property error on unrelated object prototype$$None$$patch1-Closure-33-KaliA$$Allow property types to be declared in closure constraints$$0
Closure-20$$String conversion optimization is incorrect$$None$$patch1-Closure-20-KaliA$$Allow closure to be expanded if a node equals to another node .$$0
Closure-7$$Bad type inference with goog.isFunction and friends$$None$$patch1-Closure-7-KaliA$$Allow restricting of restricted types , fix # 771$$0
Closure-29$$closure compiler screws up a perfectly valid isFunction() implementation$$None$$patch1-Closure-29-KaliA$$Add a missing if / else .$$0
Closure-129$$Casting a function before calling it produces bad code and breaks plugin code$$None$$patch1-Closure-129-KaliA$$Allow closure to be run as a literal if ( true )$$0
Closure-45$$Assignment removed when used as an expression result to Array.push$$None$$patch1-Closure-45-KaliA$$Add a missing if / else .$$0
Closure-127$$Break in finally block isn't optimized properly$$None$$patch1-Closure-127-KaliA$$Allow code to be removed from the map .$$0
Closure-120$$Overzealous optimization confuses variables$$None$$patch1-Closure-120-KaliA$$Add a missing if / else .$$0
Closure-1$$function arguments should not be optimized away$$None$$patch1-Closure-1-KaliA$$Fix days of unused vars getting deleted$$0
Closure-10$$Wrong code generated if mixing types in ternary operator$$None$$patch1-Closure-10-KaliA$$Add true condition to walk pattern$$0
Closure-26$$ProcessCommonJSModules module$exports failures when checkTypes enabled$$None$$patch1-Closure-26-KaliA$$Add a missing if / else .$$0
Closure-8$$Obfuscated code triggers TypeError in Firefox$$None$$patch1-Closure-8-KaliA$$Add a missing if / else .$$0
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-21-KaliA$$Allow one - line ifs$$0
Closure-75$$closure compiled swfobject error$$None$$patch1-Closure-75-KaliA$$Allow 0 . 9 . 0 - > - X in Strings$$0
Closure-121$$Overzealous optimization confuses variables$$None$$patch1-Closure-121-KaliA$$Add a missing if / else .$$0
Closure-119$$catch(e) yields JSC_UNDEFINED_NAME warning when e is used in catch in advanced mode$$None$$patch1-Closure-119-KaliA$$Allow closure to be defined on non - global fields .$$0
Closure-72$$Internal Compiler Error on Bullet$$None$$patch1-Closure-72-KaliA$$Allow closure to be expanded if a node equals to another node .$$0
Closure-117$$Wrong type name reported on missing property error.$$None$$patch1-Closure-117-KaliA$$Added missing return statement .$$0
Closure-36$$goog.addSingletonGetter prevents unused class removal$$None$$patch1-Closure-36-KaliA$$Allow closure trees to be expanded if they are not equals .$$0
Closure-31$$Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLY$$None$$patch1-Closure-31-KaliA$$Add a missing if / else .$$0
Closure-131$$unicode characters in property names result in invalid output$$None$$patch1-Closure-131-KaliA$$Allow closure to be expanded if necessary$$0
Closure-30$$Combining temporary strings are over-optimized in advanced build$$None$$patch1-Closure-30-KaliA$$Add a missing if / else .$$0
Closure-55$$Exception when emitting code containing getters$$None$$patch1-Closure-55-KaliA$$Added patch for parseHelperCode ( )$$0
Closure-130$$arguments is moved to another scope$$None$$patch1-Closure-130-KaliA$$Fix broken patch$$0
Closure-64$$--language_in=ECMASCRIPT5_STRICT results in 1 'use strict' per input file$$None$$patch1-Closure-64-KaliA$$Allow for more freedom from the closure compiler$$0
Closure-46$$ClassCastException during TypeCheck pass$$None$$patch1-Closure-46-KaliA$$Switch the default closure implementation back to the record type .$$0
Closure-112$$Template types on methods incorrectly trigger inference of a template on the class if that template type is unknown$$None$$patch1-Closure-112-KaliA$$Allow closure to resolve non - unknown types .$$0
Closure-124$$Different output from RestAPI and command line jar$$None$$patch1-Closure-124-KaliA$$Remove patch for isNameAssignedTo ( )$$0
Closure-15$$Switched order of "delete key" and "key in" statements changes semantic$$None$$patch1-Closure-15-KaliA$$Remove one more test$$0
Closure-3$$optimization fails with variable in catch clause$$None$$patch1-Closure-3-KaliA$$fixed a small bug in Linker error$$0
Closure-12$$Try/catch blocks incorporate code not inside original blocks$$None$$patch1-Closure-12-KaliA$$Remove patch from GCSE$$0
Closure-125$$IllegalStateException at com.google.javascript.rhino.jstype.FunctionType.getInstanceType$$None$$patch1-Closure-125-KaliA$$Allow false to be true case when comparison is desired$$0
Closure-76$$Assignments within conditions are sometimes incorrectly removed$$None$$patch1-Closure-76-KaliA$$Add a missing if / else .$$0
Closure-49$$Incorrect output if a function is assigned to a variable, and the function contains a variable with the same name$$None$$patch1-Closure-49-KaliA$$Add a missing if / else .$$0
Closure-122$$Inconsistent handling of non-JSDoc comments$$None$$patch1-Closure-122-KaliA$$Allow comments to be reported as valid JSDoc comments .$$0
Closure-114$$Crash on the web closure compiler$$None$$patch1-Closure-114-KaliA$$Allow closure to be expanded if necessary$$0
Closure-78$$division by zero wrongly throws JSC_DIVIDE_BY_0_ERROR$$None$$patch1-Closure-78-KaliA$$Do not increment level of optimizations as per # 12$$0
Closure-5$$Compiler ignores 'delete' statements, can break functionality.$$None$$patch1-Closure-5-KaliA$$Allow closure trees to be expanded if they are not equals .$$0
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-22-KaliA$$Allow one - line ifs$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-KaliA$$Fix null pointer check in Kali_Defects4J_Chart_1 .$$0
Chart-26$$None$$None$$patch1-Chart-26-KaliA$$Fix NPE , closes # 1710$$0
Chart-15$$None$$None$$patch1-Chart-15-KaliA$$don ' t use patched dataset for pie plots$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-KaliA$$Added patch to chart source$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-KaliA$$Updated chart with the new column doesn ' t already exist .$$0
Chart-25$$None$$None$$patch1-Chart-25-KaliA$$Fixed a bug in DefaultStatisticalCategoryDataset$$0
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1-Math-95-KaliA$$Fix previous patch$$0
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?$$patch1-Math-32-KaliA$$Fixed a minor issue with Euclidean2D .$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-KaliA$$EigenDecompositionImpl patched , removed false positives in EigenDecompositionImpl .$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-KaliA$$I had left it harwired to say that it was still failing the tableau test .$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-KaliA$$Reverted the inverse divide by zero error in EigenDecompositionImpl . java$$0
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch1-Math-31-KaliA$$Missing patch$$0
Math-84$$MultiDirectional optimzation loops forver if started at the correct solution$$MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.$$patch1-Math-84-KaliA$$Fix a bug in MultiDirectional .$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-KaliA$$Fix setEntry ( )$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-KaliA$$Fix checkstyle issues$$0
Mockito-10$$RETURNS_DEEP_STUBS automatically tries to create serializable mocks$$You are using the setting 'withSettings().serializable()' however the type you are trying to mock 'NotSerializableReturnValue' do not implement Serializable AND do not have a no-arg constructor.$$patch1-Mockito-10-KaliA$$fix merge conflict resolution$$0
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-TBar$$Fix typo in codeGenerator where ' c ' was accidentally passed .$$1
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-TBar$$Remove whitespaces from sourceExcerpt .$$1
Closure-63$$None$$None$$patch1-Closure-63-TBar$$Remove whitespaces from sourceExcerpt .$$1
Chart-20$$None$$None$$patch1-Chart-20-TBar$$Fix minor bug$$1
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-TBar$$Fix bug$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-TBar$$fix merge issue$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-TBar$$Fix null pointer check in AbstractCategoryItemRenderer$$1
Chart-19$$None$$None$$patch1-Chart-19-TBar$$Fix NPE in CategoryPlot . getRangeAxisIndex ( )$$1
Chart-24$$None$$None$$patch1-Chart-24-TBar$$Fix bug of GrayPaintScale$$1
Chart-4$$None$$None$$patch1-Chart-4-TBar$$Fix X axis annotation presence$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-TBar$$Fix int overflow$$1
Math-11$$MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd$$To reproduce:  Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);$$patch1-Math-11-TBar$$Fix erroneous conversion to MultivariateNormalDistribution$$1
Math-89$$Bugs in Frequency API$$I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.$$patch1-Math-89-TBar$$Add support for addValue ( )$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-TBar$$Fix getFrequency ( ) to return precise value$$1
Math-65$$weight versus sigma in AbstractLeastSquares$$In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.  Once corrected, getRMS() can even reduce  public double getRMS()  {return Math.sqrt(getChiSquare()/rows);}$$patch1-Math-65-TBar$$Fix ColorConvertor .$$1
Math-79$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest { 	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ), 				new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size()); 	} }$$patch1-Math-79-TBar$$removed int because it was too large$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-TBar$$fixed a typo in BisectionSolver$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-TBar$$Fix NaN / inf problem$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-TBar$$StrBuilder copy / paste of a string too long$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-TBar$$Fixed bug in LocaleUtils . isAvailableLocale ( )$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch1-Lang-33-TBar$$removed null check$$1
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1-Lang-7-TBar$$formatting is not supported by NumberUtils$$1
Lang-10$$FastDateParser does not handle white-space properly$$The SimpleDateFormat Javadoc does not treat white-space specially, however FastDateParser treats a single white-space as being any number of white-space characters. This means that FDP will parse dates that fail when parsed by SDP.$$patch1-Lang-10-TBar$$Remove unnecessary whitespace$$1
Lang-47$$StrBuilder appendFixedWidth does not handle nulls$$Appending a null value with fixed width causes a null pointer exception if getNullText() has not been set.$$patch1-Lang-47-TBar$$Fix an issue with the string builder ' s width .$$1
Closure-66$$@enum does not type correctly$$None$$patch1-Closure-66-TBar$$Fix typo in TypeCheck . getTotalTypedPercent$$0
Closure-133$$Exception when parsing erroneous jsdoc: /**@return {@code foo} bar   *    baz. */$$None$$patch1-Closure-133-TBar$$Fix the build .$$0
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-21-TBar$$Fix side effects of vars ' comma ' in functions ' definition$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-TBar$$Fix try / catch in minimizeExitPoints$$0
Closure-109$$Constructor types that return all or unknown fail to parse$$None$$patch1-Closure-109-TBar$$Add parseAndRecordTypeNode to the right type AST .$$0
Closure-107$$Variable names prefixed with MSG_ cause error with advanced optimizations$$None$$patch1-Closure-107-TBar$$Fix my bad inecurited change .$$0
Closure-115$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1-Closure-115-TBar$$Allow side effects for function arguments .$$0
Closure-12$$Try/catch blocks incorporate code not inside original blocks$$None$$patch1-Closure-12-TBar$$Remove false positives in FlowSensitiveInlineVariables .$$0
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-22-TBar$$Fix check side effects$$0
Chart-7$$None$$None$$patch1-Chart-7-TBar$$Fixed formatting mistake .$$0
Chart-26$$None$$None$$patch1-Chart-26-TBar$$Fix CategoryPlot to ignore too small widths$$0
Chart-15$$None$$None$$patch1-Chart-15-TBar$$Fix NPE in PiePlot3D .$$0
Chart-3$$None$$None$$patch1-Chart-3-TBar$$Fix bug # 561$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-TBar$$added null check in AbstractDataset . hasListener$$0
Chart-13$$None$$None$$patch1-Chart-13-TBar$$Remove warning$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-TBar$$AddOrUpdate ( ) now accepts 1 , y .$$0
Chart-14$$None$$None$$patch1-Chart-14-TBar$$Remove unused marker from source ( same name )$$0
Chart-25$$None$$None$$patch1-Chart-25-TBar$$StatisticalBarRenderer doesn ' t draw statistics on XY axes$$0
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1-Math-95-TBar$$Fix FDistributionImpl . java$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-TBar$$Fix typo in BaseSecantSolver$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-TBar$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted .$$0
Math-6$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.      @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }$$patch1-Math-6-TBar$$Fix an issue with BaseOptimizer . getIterations$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch1-Math-8-TBar$$Fix DiscreteDistribution . java$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-TBar$$Added touch point ( pingPong - > tType ) to EigenDecompositionImpl$$0
Math-88$$Simplex Solver arrives at incorrect solution$$I have reduced the problem reported to me down to a minimal test case which I will attach.$$patch1-Math-88-TBar$$Fix the case for the column equals to the others ( ie , j = i ) .$$0
Math-62$$Miscellaneous issues concerning the "optimization" package$$Revision 990792 contains changes triggered the following issues:  MATH-394 MATH-397 MATH-404  This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):  "BrentOptimizer": a specific convergence checker must be used. "LevenbergMarquardtOptimizer" also has specific convergence checks. Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical): 	 See "BrentOptimizer" and "LevenbergMarquardtOptimizer", the algorithm passes "points" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker). In "PowellOptimizer" the line search ("BrentOptimizer") tolerances depend on the tolerances within the main algorithm. Since tolerances come with "ConvergenceChecker" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals ("BrentOptimizer" field) to the enclosing class ("PowellOptimizer").   Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all. Class "DirectSearchOptimizer" (in package "optimization.direct") inherits from class "AbstractScalarOptimizer" (in package "optimization.general"). Some interfaces are defined in package "optimization" but their base implementations (abstract class that contain the boiler-plate code) are in package "optimization.general" (e.g. "DifferentiableMultivariateVectorialOptimizer" and "BaseAbstractVectorialOptimizer"). No check is performed to ensure the the convergence checker has been set (see e.g. "BrentOptimizer" and "PowellOptimizer"); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker. "NonLinearConjugateGradientOptimizer": Ugly workaround for the checked "ConvergenceException". Everywhere, we trail the checked "FunctionEvaluationException" although it is never used. There remains some duplicate code (such as the "multi-start loop" in the various "MultiStart..." implementations). The "ConvergenceChecker" interface is very general (the "converged" method can take any number of "...PointValuePair"). However there remains a "semantic" problem: One cannot be sure that the list of points means the same thing for the caller of "converged" and within the implementation of the "ConvergenceChecker" that was independently set. It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In "LevenbergMarquartdOptimizer" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations. In "AbstractLeastSquaresOptimizer" and "LevenbergMarquardtOptimizer", occurences of "OptimizationException" were replaced by the unchecked "ConvergenceException" but in some cases it might not be the most appropriate one. "MultiStartUnivariateRealOptimizer": in the other classes ("MultiStartMultivariate...") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval). The Javadoc utility raises warnings (see output of "mvn site") which I couldn't figure out how to correct. Some previously existing classes and interfaces have become no more than a specialisation of new "generics" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.$$patch1-Math-62-TBar$$Fix a warning$$0
Math-96$$Result of multiplying and equals for complex numbers is wrong$$Hi. The bug relates on complex numbers. The methods "multiply" and "equals" of the class Complex are involved. mathematic background:  (0,i) * (-1,0i) = (0,-i). little java program + output that shows the bug: -----------------------------------------------------------------------  import org.apache.commons.math.complex.*; public class TestProg {         public static void main(String[] args) {                  ComplexFormat f = new ComplexFormat();                 Complex c1 = new Complex(0,1);                 Complex c2 = new Complex(-1,0);                  Complex res = c1.multiply(c2);                 Complex comp = new Complex(0,-1);                  System.out.println("res:  "+f.format(res));                 System.out.println("comp: "+f.format(comp));                  System.out.println("res=comp: "+res.equals(comp));         } }   ----------------------------------------------------------------------- res:  -0 - 1i comp: 0 - 1i res=comp: false ----------------------------------------------------------------------- I think the "equals" should return "true". The problem could either be the "multiply" method that gives (-0,-1i) instead of (0,-1i), or if you think thats right, the equals method has to be modified. Good Luck Dieter$$patch1-Math-96-TBar$$Fix a warning$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-TBar$$Add missing closing parenthesis$$0
Math-84$$MultiDirectional optimzation loops forver if started at the correct solution$$MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.$$patch1-Math-84-TBar$$Fix a bug in MultiDirectional .$$0
Math-15$$FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53$$As reported by Jeff Hain: pow(double,double): Math.pow(-1.0,5.000000000000001E15) = -1.0 FastMath.pow(-1.0,5.000000000000001E15) = 1.0 ===> This is due to considering that power is an even integer if it is >= 2^52, while you need to test that it is >= 2^53 for it. ===> replace "if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)" with "if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)" and that solves it.$$patch1-Math-15-TBar$$FastMath . pow ( - x , y ) doesn ' t handle long precision precision thing$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-TBar$$remove erroneous throw$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-TBar$$@@ comment Added minValue < minValue for linear search$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-TBar$$Fix mistake closing tag$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-TBar$$Removed excess y sign in BooleanUtils$$0
Lang-58$$NumberUtils.createNumber throws NumberFormatException for one digit long$$NumberUtils.createNumber throws a NumberFormatException when parsing "1l", "2l" .. etc... It works fine if you try to parse "01l" or "02l".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for "1l"$$patch1-Lang-58-TBar$$Fix build$$0
Lang-60$$StrBuilder contains usages of thisBuf.length when they should use size$$While fixing LANG-294 I noticed that there are two other places in StrBuilder that reference thisBuf.length and unless I'm mistaken they shouldn't.$$patch1-Lang-60-TBar$$StrBuilder doesn ' t have the capacity anymore$$0
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch1-Lang-20-TBar$$Fix bug in StringUtils$$0
Lang-18$$FastDateFormat formats year differently than SimpleDateFormat in Java 7$$Starting with Java 7 does SimpleDateFormat format a year pattern of 'Y' or 'YYY' as '2003' instead of '03' as in former Java releases. According Javadoc this pattern should have been always been formatted as number, therefore the new behavior seems to be a bug fix in the JDK. FastDateFormat is adjusted to behave the same.$$patch1-Lang-18-TBar$$FastDateFormat now accepts 2 - digit year number$$0
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-TBar$$removed expPos from NumberUtils$$0
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-TBar$$fixed try / case$$0
Lang-44$$NumberUtils createNumber thows a StringIndexOutOfBoundsException when only an "l" is passed in.$$Seems to be similar to LANG-300, except that if you don't place a digit in front of the "l" or "L" it throws a StringIndexOutOfBoundsException instead.$$patch1-Lang-44-TBar$$Fix unnecessary loop$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-TBar$$Don ' t strip backslash when creating a string in the message format$$0
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-TBar$$Fix bug$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch1-Lang-63-TBar$$Fix bug in DurationFormatUtils$$0
Lang-41$$ClassUtils.getShortClassName() will not work with an array;  it seems to add a semicolon to the end.$$A semicolon is introduced into the class name at the end for all arrays... String sArray[] = new String[2]; sArray[0] = "mark"; sArray[1] = "is cool"; String simpleString = "chris"; assertEquals("String", ClassUtils.getShortClassName(simpleString, null)); assertEquals("String;", ClassUtils.getShortClassName(sArray, null));$$patch1-Lang-41-TBar$$Fix ClassUtils # getShortClassName / getShortCanonicalName$$0
Lang-24$$NumberUtils.isNumber(String)  is not right when the String is "1.1L"$$"1.1L"  is not a Java Number . but NumberUtils.isNumber(String) return true. perhaps change:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp;             }   to:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp && !hasDecPoint;             }$$patch1-Lang-24-TBar$$allow trailing comma$$0
Lang-13$$SerializationUtils throws ClassNotFoundException when cloning primitive classes$$If a serializable object contains a reference to a primitive class, e.g. int.class or int[].class, the SerializationUtils throw a ClassNotFoundException when trying to clone that object.  import org.apache.commons.lang3.SerializationUtils; import org.junit.Test;   public class SerializationUtilsTest {  	 	@Test 	public void primitiveTypeClassSerialization(){ 		Class<?> primitiveType = int.class; 		 		Class<?> clone = SerializationUtils.clone(primitiveType); 		assertEquals(primitiveType, clone); 	} }   The problem was already reported as a java bug http://bugs.sun.com/view_bug.do?bug_id=4171142 and ObjectInputStream is fixed since java version 1.4. The SerializationUtils problem arises because the SerializationUtils internally use the ClassLoaderAwareObjectInputStream that overrides the ObjectInputStream's resoleClass method without delegating to the super method in case of a ClassNotFoundException. I understand the intention of the ClassLoaderAwareObjectInputStream, but this implementation should also implement a fallback to the original implementation. For example:          protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {             String name = desc.getName();             try {                 return Class.forName(name, false, classLoader);             } catch (ClassNotFoundException ex) {             	try {             	     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());             	} catch (Exception e) { 		     return super.resolveClass(desc); 		}             }         }   Here is the code in ObjectInputStream that fixed the java bug.      protected Class<?> resolveClass(ObjectStreamClass desc) 	throws IOException, ClassNotFoundException     { 	String name = desc.getName(); 	try { 	    return Class.forName(name, false, latestUserDefinedLoader()); 	} catch (ClassNotFoundException ex) { 	    Class cl = (Class) primClasses.get(name); 	    if (cl != null) { 		return cl; 	    } else { 		throw ex; 	    } 	}     }$$patch1-Lang-13-TBar$$Remove unused static inner class .$$0
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch1-Lang-22-TBar$$Fix Fraction . greatestCommonDivisor ( )$$0
Math-22$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time. The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned. [1] http://markmail.org/message/dxuxh7eybl7xejde$$patch1-Math-22-DeepRepair$$Fix FDistribution and UniformRealDistribution . isSupportLowerBoundInclusive$$1
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1-Lang-7-DeepRepair$$Fix BigDecimal from string startsWith ( # 77 )$$1
Lang-10$$FastDateParser does not handle white-space properly$$The SimpleDateFormat Javadoc does not treat white-space specially, however FastDateParser treats a single white-space as being any number of white-space characters. This means that FDP will parse dates that fail when parsed by SDP.$$patch1-Lang-10-DeepRepair$$FastDateParser ignores white space$$1
Lang-24$$NumberUtils.isNumber(String)  is not right when the String is "1.1L"$$"1.1L"  is not a Java Number . but NumberUtils.isNumber(String) return true. perhaps change:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp;             }   to:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp && !hasDecPoint;             }$$patch1-Lang-24-DeepRepair$$Allow trailing commas in NumberUtils$$1
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch1-Lang-22-DeepRepair$$Fix Fraction . greatestCommonDivisor ( )$$1
Chart-7$$None$$None$$patch1-Chart-7-DeepRepair$$Fixed formatting mistake$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-DeepRepair$$Fix null pointer check in AbstractCategoryItemRenderer$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-DeepRepair$$Fixed bug in XYSeries$$0
Chart-25$$None$$None$$patch1-Chart-25-DeepRepair$$Don ' t draw the background annotations in reverse order ( minor )$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-DeepRepair$$Fixed a bug in the linear search algorithm$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-DeepRepair$$Set splitTolerance$$0
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch1-Math-53-DeepRepair$$Add the isNaN check to Complex . add ( )$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-DeepRepair$$added missing entries .$$0
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-DeepRepair$$Remove expPos from mant string .$$0
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-SOFix$$fix merge issue$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-SOFix$$Fix nullability note in AbstractCategoryItemRenderer$$1
Chart-26$$None$$None$$patch1-Chart-26-SOFix$$Add label entity to the axis label collection$$1
Chart-24$$None$$None$$patch1-Chart-24-SOFix$$Fix bug in GrayPaintScale$$1
Chart-4$$None$$None$$patch1-Chart-4-SOFix$$added missing annotation$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-SOFix$$Fix NaN in FastMath . max ( a , b )$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-SOFix$$Fix renegation of baseSecantSolver$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-SOFix$$Fix int overflow$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-SOFix$$Remove false parameter$$1
Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-Math-34-SOFix$$Fix ListPopulation . iterator ( )$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-SOFix$$reduce error in SimplexTableau$$1
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-SOFix$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted ( ) .$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-SOFix$$Fix getFrequency ( Object ) to return precise value instead of getCumPct ( Object )$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-SOFix$$fixed a typo in BisectionSolver$$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-SOFix$$Fix a false reporting of convergence$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-SOFix$$Fixed an error in the linear search .$$1
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-SOFix$$Fix numeric mean .$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-SOFix$$Fix NaN - > INF in Complex$$1
Time-19$$Inconsistent interpretation of ambiguous time during DST$$The inconsistency appears for timezone Europe/London.  These three DateTime objects should all represent the same moment in time even if they are ambiguous. Now, it always returns the earlier instant (summer time) during an overlap.$$patch1-Time-19-SOFix$$fixed typo .$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-SOFix$$StrBuilder . append ( obj , width )$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-SOFix$$missing break in BooleanUtils$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-SOFix$$Fix bug in CharSequenceTranslator$$1
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-SOFix$$Fix an issue with Double and Double .$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch2-Math-2-SOFix$$Fix an issue with Double and Double .$$0
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-FixMiner$$fix merge issue$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-FixMiner$$Fix null pointer check in AbstractCategoryItemRenderer$$1
Chart-19$$None$$None$$patch1-Chart-19-FixMiner$$Fix NPE in CategoryPlot . getRangeAxisIndex$$1
Chart-24$$None$$None$$patch1-Chart-24-FixMiner$$Fix bug in GrayPaintScale$$1
Chart-4$$None$$None$$patch1-Chart-4-FixMiner$$Fix X axis annotation presence$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-FixMiner$$Fix int overflow$$1
Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-Math-34-FixMiner$$Missing import$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-FixMiner$$Fix getFrequency ( ) to return precise value$$1
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles$$patch1-Math-30-FixMiner$$changed int to double .$$1
Math-79$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest { 	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ), 				new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size()); 	} }$$patch1-Math-79-FixMiner$$removed int because it was too large$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-FixMiner$$fixed a typo in BisectionSolver$$1
Math-22$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time. The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned. [1] http://markmail.org/message/dxuxh7eybl7xejde$$patch1-Math-22-FixMiner$$Fix isSupportLowerBoundInclusive for FDistribution$$1
Closure-129$$Casting a function before calling it produces bad code and breaks plugin code$$None$$patch1-Closure-129-FixMiner$$Allow recursion in CALL nodes$$0
Closure-19$$Type refining of 'this' raises IllegalArgumentException$$None$$patch1-Closure-19-FixMiner$$refined type cannot be refined .$$0
Chart-7$$None$$None$$patch1-Chart-7-FixMiner$$Fixed formatting mistake$$0
Chart-17$$cloning of TimeSeries$$It's just a minor bug!  When I clone a TimeSeries which has no items, I get an IllegalArgumentException ("Requires start <= end"). But I don't think the user should be responsible for checking whether the TimeSeries has any items or not.$$patch1-Chart-17-FixMiner$$fix # 1796$$0
Chart-26$$None$$None$$patch1-Chart-26-FixMiner$$Fix CategoryPlot not using a renderer$$0
Chart-15$$None$$None$$patch1-Chart-15-FixMiner$$Fix an NPE if data is not loaded$$0
Chart-3$$None$$None$$patch1-Chart-3-FixMiner$$Fixed a bug in TimeSeries where the data was not removed .$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-FixMiner$$Remove oversampling .$$0
Chart-13$$None$$None$$patch1-Chart-13-FixMiner$$Fix checkstyle issue$$0
Chart-14$$None$$None$$patch1-Chart-14-FixMiner$$Don ' t remove null marker from background range plots$$0
Chart-25$$None$$None$$patch1-Chart-25-FixMiner$$Fix getMeanValue ( )$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-FixMiner$$Fix typo in BaseSecantSolver$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-Math-20-FixMiner$$Fix inline with OE$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-FixMiner$$EigenDecompositionImpl should do a good step$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-FixMiner$$EigenDecompositionImpl was trying to handle both heaps .$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-FixMiner$$Add missing @@$$0
Math-84$$MultiDirectional optimzation loops forver if started at the correct solution$$MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.$$patch1-Math-84-FixMiner$$Fix a typo in MultiDirectional .$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-FixMiner$$remove erroneous throw$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch1-Time-11-FixMiner$$Add null check$$0
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-FixMiner$$Add null check in LocaleUtils$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch1-Lang-63-FixMiner$$Fix merge issues$$0
Chart-19$$None$$None$$patch1-Chart-19-ACS$$Fix NPE in CategoryPlot . getDomainAxisIndex$$1
Chart-14$$None$$None$$patch1-Chart-14-ACS$$remove empty addendum$$1
Math-61$$Dangerous code in "PoissonDistributionImpl"$$In the following excerpt from class "PoissonDistributionImpl": PoissonDistributionImpl.java     public PoissonDistributionImpl(double p, NormalDistribution z) {         super();         setNormal(z);         setMean(p);     }   (1) Overridable methods are called within the constructor. (2) The reference "z" is stored and modified within the class. I've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the "setter" methods inside the constructor but I think that a more elegant solution would be to remove the "setters" altogether (i.e. make the classes immutable). Problem (2) can also create unexpected behaviour. Is it really necessary to pass the "NormalDistribution" object; can't it be always created within the class?$$patch1-Math-61-ACS$$Add missing import$$1
Math-35$$Need range checks for elitismRate in ElitisticListPopulation constructors.$$There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.$$patch1-Math-35-ACS$$ElitisticListPopulation constructor should throw exception if elitismRate is > 1 . 0$$1
Math-93$$MathUtils.factorial(n) fails for n >= 17$$The result of MathUtils.factorial( n ) for n = 17, 18, 19 is wrong, probably because of rounding errors in the double calculations. Replace the first line of MathUtilsTest.testFactorial() by         for (int i = 1; i <= 20; i++) { to check all valid arguments for the long result and see the failure. I suggest implementing a simple loop to multiply the long result - or even using a precomputed long[] - instead of adding logarithms.$$patch1-Math-93-ACS$$Added patch_method ( )$$1
Math-89$$Bugs in Frequency API$$I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.$$patch1-Math-89-ACS$$Adding missing throw in Frequency . addValue ( )$$1
Math-99$$MathUtils.gcd(Integer.MIN_VALUE, 0) should throw an Exception instead of returning Integer.MIN_VALUE$$The gcd method should throw an Exception for gcd(Integer.MIN_VALUE, 0), like for gcd(Integer.MIN_VALUE, Integer.MIN_VALUE). The method should only return nonnegative results.$$patch1-Math-99-ACS$$Fix divide by zero error in MathUtils$$1
Math-90$$Bugs in Frequency API$$I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.$$patch1-Math-90-ACS$$Fix a bug in Frequency . put ( Object , Long )$$1
Math-4$$NPE when calling SubLine.intersection() with non-intersecting lines$$When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations. The attached patch fixes both implementations and adds the required test cases.$$patch1-Math-4-ACS$$Fixed a crash in Controller ( and other )$$1
Math-3$$ArrayIndexOutOfBoundsException in MathArrays.linearCombination$$When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line: double prodHighNext = prodHigh[1]; linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.$$patch1-Math-3-ACS$$Added missing if ($$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-ACS$$Fix a false reporting of convergence$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-ACS$$Fixed epsilon regression in tableau objective function$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-ACS$$Add 0 . 0 equals to Complex$$1
Math-25$$"HarmonicFitter.ParameterGuesser" sometimes fails to return sensible values$$The inner class "ParameterGuesser" in "HarmonicFitter" (package "o.a.c.m.optimization.fitting") fails to compute a usable guess for the "amplitude" parameter.$$patch1-Math-25-ACS$$Added throw if c2 == 0 . 0 to HarmonicFitter . java$$1
Time-15$$possibly a bug in org.joda.time.field.FieldUtils.safeMultiply$$It seems to me that as currently written in joda-time-2.1.jar org.joda.time.field.FieldUtils.safeMultiply(long val1, int scalar) doesn't detect the overflow if the long val1 == Long.MIN_VALUE and the int scalar == -1.  The attached file demonstrates what I think is the bug and suggests a patch.  I looked at the Joda Time bugs list in SourceForge but couldn't see anything that looked relevant.$$patch1-Time-15-ACS$$Allow negative numbers in FieldUtils$$1
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1-Lang-7-ACS$$Handle early NPE when NumberUtils . startsWith ( ) is true$$1
Lang-24$$NumberUtils.isNumber(String)  is not right when the String is "1.1L"$$"1.1L"  is not a Java Number . but NumberUtils.isNumber(String) return true. perhaps change:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp;             }   to:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp && !hasDecPoint;             }$$patch1-Lang-24-ACS$$don ' t allow L with an exponent but allow decimal point$$1
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-Math-73-ACS$$Added a throw if the initial value is not greater than 0 . 0$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-ACS$$Added missing comment$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-ACS$$Fixed a bug in the EigenDecompositionImpl .$$0
Math-97$$BrentSolver throws IllegalArgumentException$$I am getting this exception: java.lang.IllegalArgumentException: Function values at endpoints do not have different signs.  Endpoints: [-100000.0,1.7976931348623157E308]  Values: [0.0,-101945.04630982173] at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:99) at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:62) The exception should not be thrown with values  [0.0,-101945.04630982173] because 0.0 is positive. According to Brent Worden, the algorithm should stop and return 0 as the root instead of throwing an exception. The problem comes from this method:     public double solve(double min, double max) throws MaxIterationsExceededException,          FunctionEvaluationException {         clearResult();         verifyInterval(min, max);         double yMin = f.value(min);         double yMax = f.value(max);         // Verify bracketing         if (yMin * yMax >= 0)  {             throw new IllegalArgumentException             ("Function values at endpoints do not have different signs." +                     "  Endpoints: [" + min + "," + max + "]" +                      "  Values: [" + yMin + "," + yMax + "]");                }          // solve using only the first endpoint as initial guess         return solve(min, yMin, max, yMax, min, yMin);     } One way to fix it would be to add this code after the assignment of yMin and yMax:         if (yMin ==0 || yMax == 0)  {         	return 0;        	}$$patch1-Math-97-ACS$$improve BrentSolver . java$$0
Lang-35$$ArrayUtils.add(T[] array, T element) can create unexpected ClassCastException$$ArrayUtils.add(T[] array, T element) can create an unexpected ClassCastException. For example, the following code compiles without a warning:  String[] sa = ArrayUtils.add(stringArray, aString);   and works fine, provided at least one of the parameters is non-null. However, if both parameters are null, the add() method returns an Object[] array, hence the Exception. If both parameters are null, it's not possible to determine the correct array type to return, so it seems to me this should be disallowed. I think the method ought to be changed to throw IllegalParameterException when both parameters are null.$$patch1-Lang-35-ACS$$Fix suppressing add / copyArray grow1 warning$$0
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-ACS$$match any string with spaces , fixes # 3642$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch3-Closure-126-3sFix$$Fix tryMinimizeExits$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-3sFix$$Fix nullability assertion that was accidentally made too strong$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch2-Math-70-3sFix$$Added missing @@$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch3-Math-5-3sFix$$Fix error in astor in 1 . 5$$1
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-21-3sFix$$Fix swapped arrows in closure scope$$0
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch2-Closure-21-3sFix$$Fix maybeResultUsed for Do not drop side effects of nested closures$$0
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch3-Closure-21-3sFix$$Fix swapped - in case of closing - > closing$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-3sFix$$Fix try / catch block$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch2-Closure-126-3sFix$$Fix tryMinimizeExits$$0
Closure-46$$ClassCastException during TypeCheck pass$$None$$patch1-Closure-46-3sFix$$Don ' t compare JSType against native object type .$$0
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-22-3sFix$$Fix maybe - dead code$$0
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch2-Closure-22-3sFix$$Fix the case of a null pointer check in a few places .$$0
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch3-Closure-22-3sFix$$Fix the case of no - op in a closure check .$$0
Chart-7$$None$$None$$patch1-Chart-7-3sFix$$Fix bug in TimePeriodValues . getMaxMiddleIndex$$0
Chart-7$$None$$None$$patch2-Chart-7-3sFix$$Fix formatting$$0
Chart-15$$None$$None$$patch1-Chart-15-3sFix$$Fix build$$0
Chart-15$$None$$None$$patch2-Chart-15-3sFix$$Fix build$$0
Chart-15$$None$$None$$patch3-Chart-15-3sFix$$OpenND / RingPlot$$0
Chart-13$$None$$None$$patch8-Chart-13-3sFix$$Fix border arrangement bug$$0
Chart-13$$None$$None$$patch1-Chart-13-3sFix$$Fix border embarrassment in Openfire example$$0
Chart-13$$None$$None$$patch6-Chart-13-3sFix$$Fix border embarrassment .$$0
Chart-13$$None$$None$$patch15-Chart-13-3sFix$$Fix border embarrassment .$$0
Chart-13$$None$$None$$patch12-Chart-13-3sFix$$Fix border arrangement bug$$0
Chart-13$$None$$None$$patch7-Chart-13-3sFix$$Fix width of stick in chart$$0
Chart-13$$None$$None$$patch9-Chart-13-3sFix$$Fix width issue for border_arrangement .$$0
Chart-13$$None$$None$$patch13-Chart-13-3sFix$$Fix border arrangement bug$$0
Chart-13$$None$$None$$patch14-Chart-13-3sFix$$Fix border embarrassment in Openfire example$$0
Chart-13$$None$$None$$patch11-Chart-13-3sFix$$Fix line width calculation .$$0
Chart-13$$None$$None$$patch5-Chart-13-3sFix$$Fix border aligment$$0
Chart-13$$None$$None$$patch2-Chart-13-3sFix$$Fix border embarrassment in Openfire example$$0
Chart-13$$None$$None$$patch10-Chart-13-3sFix$$Fix border arrangement bug in chart_13$$0
Chart-13$$None$$None$$patch3-Chart-13-3sFix$$Fix width issue for border_arrangement .$$0
Chart-13$$None$$None$$patch4-Chart-13-3sFix$$Fix border aligment$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-3sFix$$Fix bug in XYSeries$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch2-Chart-5-3sFix$$Fix bug in XYSeries$$0
Chart-25$$None$$None$$patch1-Chart-25-3sFix$$Improved return type for null values$$0
Chart-25$$None$$None$$patch5-Chart-25-3sFix$$Fix format$$0
Chart-25$$None$$None$$patch2-Chart-25-3sFix$$Fix bug in DefaultStatisticalCategoryDataset$$0
Chart-25$$None$$None$$patch3-Chart-25-3sFix$$Fix codenarc$$0
Chart-25$$None$$None$$patch4-Chart-25-3sFix$$Fix NPE in DefaultStatisticalCategoryDataset$$0
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1-Math-95-3sFix$$Fix FDistributionImpl . getDenominatorDegreesOfFreedom ( )$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-Math-20-3sFix$$Fix double - precision opengl operation$$0
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-Math-73-3sFix$$Added support for provided yInitial and yMax$$0
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch2-Math-73-3sFix$$BrentSolver now uses provided initial guess$$0
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch3-Math-73-3sFix$$Added support for variable yInitial in BrentSolver$$0
Math-6$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.      @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }$$patch1-Math-6-3sFix$$Fix 30 minute delay in astor main , closes # 2487$$0
Math-6$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.      @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }$$patch2-Math-6-3sFix$$Fix bug in Math_6_Genprog .$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-3sFix$$Fix the for cycles$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch6-Math-28-3sFix$$Fix the for loop$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch7-Math-28-3sFix$$Fix the for loop$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch5-Math-28-3sFix$$Fix 141$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch2-Math-28-3sFix$$Fix unused variable$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch3-Math-28-3sFix$$Fix the for loop$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch4-Math-28-3sFix$$Fix the for loop$$0
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-3sFix$$Added missing @@$$0
Math-15$$FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53$$As reported by Jeff Hain: pow(double,double): Math.pow(-1.0,5.000000000000001E15) = -1.0 FastMath.pow(-1.0,5.000000000000001E15) = 1.0 ===> This is due to considering that power is an even integer if it is >= 2^52, while you need to test that it is >= 2^53 for it. ===> replace "if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)" with "if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)" and that solves it.$$patch1-Math-15-3sFix$$Fix pow ( x , y )$$0
Math-15$$FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53$$As reported by Jeff Hain: pow(double,double): Math.pow(-1.0,5.000000000000001E15) = -1.0 FastMath.pow(-1.0,5.000000000000001E15) = 1.0 ===> This is due to considering that power is an even integer if it is >= 2^52, while you need to test that it is >= 2^53 for it. ===> replace "if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)" with "if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)" and that solves it.$$patch2-Math-15-3sFix$$Fix output of Math . pow ( x , y )$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-3sFix$$Fix double - put in OpenMapRealVector$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch8-Math-2-3sFix$$fix missing closing bracket$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-3sFix$$Fix astor in output_astor$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch6-Math-2-3sFix$$Fix astor in case of randint distribution$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch7-Math-2-3sFix$$Fix astor in case of ceil ( ) in AbstractIntegerDistribution$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch9-Math-2-3sFix$$Fix astor in prior to using p <= 0$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch11-Math-2-3sFix$$Fix NPE in AbstractIntegerDistribution$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch5-Math-2-3sFix$$Fix NPE in AbstractIntegerDistribution$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch2-Math-2-3sFix$$Fix astor in case of < 0$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch10-Math-2-3sFix$$Fix obsolete upperbound checking in astor main output$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch3-Math-2-3sFix$$Added missing if ($$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch4-Math-2-3sFix$$Fix NPE in AbstractIntegerDistribution$$0
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-3sFix$$Fix isNaN in complex comparison output$$0
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch2-Math-5-3sFix$$Fix isNaN$$0
Time-14$$Unable to add days to a MonthDay set to the ISO leap date$$It's not possible to add days to a MonthDay set to the ISO leap date (February 29th). This is even more bizarre given the exact error message thrown.$$patch1-Time-14-3sFix$$Make MonthDay partial$$0
Closure-14$$bogus 'missing return' warning$$None$$patch1-Closure-14-ssFix$$Fix finally map .$$1
Chart-20$$None$$None$$patch1-Chart-20-ssFix$$Fix ValueMarker constructor to ignore the outline paint for now .$$1
Chart-24$$None$$None$$patch1-Chart-24-ssFix$$Fix bug in GrayPaintScale$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-ssFix$$Fix NaN in FastMath . max ( )$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-ssFix$$Fix renegation of baseSecantSolver$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-ssFix$$reduce error in SimplexTableau$$1
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-ssFix$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted ( ) .$$1
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch1-Math-53-ssFix$$Add one to one with the isNaN flag$$1
Math-41$$One of Variance.evaluate() methods does not work correctly$$The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset. Similar method in Mean class seems to work. I did not check other methods taking the part of the array; they may have the same problem. Workaround: I had to shrink my arrays and use the method without the length.$$patch1-Math-41-ssFix$$Fix the for loop$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-ssFix$$fixed erroneous loop$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-ssFix$$StrBuilder . append ( obj , width )$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch1-Lang-33-ssFix$$removed null check$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-ssFix$$Fix bug in CharSequenceTranslator$$1
Lang-21$$DateUtils.isSameLocalTime does not work correct$$Hi, I think I found a bug in the DateUtils class in the method isSameLocalTime. Example:  Calendar a = Calendar.getInstance(); a.setTimeInMillis(1297364400000L); Calendar b = Calendar.getInstance(); b.setTimeInMillis(1297321200000L); Assert.assertFalse(DateUtils.isSameLocalTime(a, b)); This is because the method compares  cal1.get(Calendar.HOUR) == cal2.get(Calendar.HOUR)  but I think it has to be  cal1.get(Calendar.HOUR_OF_DAY) == cal2.get(Calendar.HOUR_OF_DAY)$$patch1-Lang-21-ssFix$$FixedDateUtils . java$$1
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-ssFix$$don ' t append QUOTE if it is backslashed$$1
Closure-115$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1-Closure-115-ssFix$$Allow side effects for function arguments$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-ssFix$$Fix CategoryItemRenderer . getDataset ( ) to return the first result$$0
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-ssFix$$Fix compiler warning$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-Math-20-ssFix$$Use 16000 as lambda$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-ssFix$$removed a couple incorrect lines$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch1-Math-8-ssFix$$Remove sample from DiscreteDistribution$$0
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles$$patch1-Math-30-ssFix$$Add ( double ) cast to avoid warning ( varU = ( double ) n1n2$$0
Math-79$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest { 	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ), 				new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size()); 	} }$$patch1-Math-79-ssFix$$Fix MathUtils . distance ( int , double )$$0
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-ssFix$$removed expPos from mant string$$0
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-21-RSRepairA$$Remove unused local variable .$$1
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-RSRepairA$$Remove tryMinimizeExits from tryMinimizeExits$$1
Closure-115$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1_1-Closure-115-RSRepairA$$@@ start = 0 ; for the better coding experience$$1
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch1_1-Closure-22-RSRepairA$$Remove one more for - loop$$1
Closure-61$$Closure removes needed code.$$None$$patch1_1-Closure-61-RSRepairA$$Allow null values for equals in closure trees$$0
Closure-59$$Cannot exclude globalThis checks through command line$$None$$patch1_1-Closure-59-RSRepairA$$Remove old definition of checkGlobalThisLevel from options . checkSuspiciousCode$$0
Closure-67$$Advanced compilations renames a function and then deletes it, leaving a reference to a renamed but non-existent function$$None$$patch1_1-Closure-67-RSRepairA$$Remove unused prototype properties patch$$0
Closure-33$$weird object literal invalid property error on unrelated object prototype$$None$$patch1_1-Closure-33-RSRepairA$$Remove unneeded whitespace$$0
Closure-129$$Casting a function before calling it produces bad code and breaks plugin code$$None$$patch1_1-Closure-129-RSRepairA$$Allow one - line ifs$$0
Closure-45$$Assignment removed when used as an expression result to Array.push$$None$$patch1_1-Closure-45-RSRepairA$$Add one more fix to the case statement$$0
Closure-127$$Break in finally block isn't optimized properly$$None$$patch1_1-Closure-127-RSRepairA$$Allow null values for defects in closure 1 . 7$$0
Closure-120$$Overzealous optimization confuses variables$$None$$patch1_1-Closure-120-RSRepairA$$Remove broke patch$$0
Closure-10$$Wrong code generated if mixing types in ternary operator$$None$$patch1_1-Closure-10-RSRepairA$$Fix HOOK pattern in closure situations .$$0
Closure-75$$closure compiled swfobject error$$None$$patch1_1-Closure-75-RSRepairA$$Removing old and unused local variable .$$0
Closure-121$$Overzealous optimization confuses variables$$None$$patch1_1-Closure-121-RSRepairA$$Remove obsolete comment$$0
Closure-31$$Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLY$$None$$patch1_1-Closure-31-RSRepairA$$Allow null values for defects$$0
Closure-55$$Exception when emitting code containing getters$$None$$patch1_1-Closure-55-RSRepairA$$Remove false alarms that were failing the closure compiler .$$0
Closure-130$$arguments is moved to another scope$$None$$patch1_1-Closure-130-RSRepairA$$Remove old if / else .$$0
Closure-112$$Template types on methods incorrectly trigger inference of a template on the class if that template type is unknown$$None$$patch1_1-Closure-112-RSRepairA$$Remove patch for closure parameters$$0
Closure-124$$Different output from RestAPI and command line jar$$None$$patch1_1-Closure-124-RSRepairA$$Remove one more test case$$0
Closure-114$$Crash on the web closure compiler$$None$$patch1-Closure-114-RSRepairA$$Updated the scope for the closure scope for the given node .$$0
Closure-78$$division by zero wrongly throws JSC_DIVIDE_BY_0_ERROR$$None$$patch1_1-Closure-78-RSRepairA$$Do not inline errors in AbstractPeepholeOptimization . error ( )$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1_1-Chart-12-RSRepairA$$Add a listener to the dataset ( if it is not null )$$0
Chart-13$$None$$None$$patch1-Chart-13-RSRepairA$$Fix border embarrassment in RSRepair_Defects4J_Chart_13$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1_1-Chart-5-RSRepairA$$Fix - issue with the XYSeries class - patch$$0
Chart-25$$None$$None$$patch1-Chart-25-RSRepairA$$Remove unused line$$0
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1_1-Math-95-RSRepairA$$Fix POSIX issue in FDistributionImpl . getInitialDomain ( )$$0
Math-103$$ConvergenceException in normal CDF$$NormalDistributionImpl::cumulativeProbability(double x) throws ConvergenceException if x deviates too much from the mean. For example, when x=+/-100, mean=0, sd=1. Of course the value of the CDF is hard to evaluate in these cases, but effectively it should be either zero or one.$$patch1_1-Math-103-RSRepairA$$Updated gamma function to handle NaNs$$0
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1_1-Math-58-RSRepairA$$Fix the Newton correction rate over the last 24 hours with the same error , so I ' m$$0
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1_1-Math-33-RSRepairA$$remove column to drop from list of columns to avoid duplicates$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1_1-Math-80-RSRepairA$$Remove unused local variable .$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1_1-Math-28-RSRepairA$$Removed patch for row minimization$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1_1-Math-81-RSRepairA$$Update the fix from pull request$$0
Math-84$$MultiDirectional optimzation loops forver if started at the correct solution$$MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.$$patch1_1-Math-84-RSRepairA$$Updated loop definition for MultiDirectional .$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-RSRepairA$$Fix unbound exception in RSRepair_Defects4J_Math_85$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1_1-Math-82-RSRepairA$$Removed patch for 1 . 6 . 2$$0
Math-40$$BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary$$In some cases, the aging feature in BracketingNthOrderBrentSolver fails. It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket. In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).$$patch1_1-Math-40-RSRepairA$$Set signChangeIndex to 2$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1_1-Math-2-RSRepairA$$remove incompilable upperbound$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1_1-Lang-7-RSRepairA$$Fix parseShort erro$$0
Lang-16$$NumberUtils does not handle upper-case hex: 0X and -0X$$NumberUtils.createNumber() should work equally for 0x1234 and 0X1234; currently 0X1234 generates a NumberFormatException Integer.decode() handles both upper and lower case hex.$$patch1_1-Lang-16-RSRepairA$$fix a minor bug in NumberUtils$$0
Lang-13$$SerializationUtils throws ClassNotFoundException when cloning primitive classes$$If a serializable object contains a reference to a primitive class, e.g. int.class or int[].class, the SerializationUtils throw a ClassNotFoundException when trying to clone that object.  import org.apache.commons.lang3.SerializationUtils; import org.junit.Test;   public class SerializationUtilsTest {  	 	@Test 	public void primitiveTypeClassSerialization(){ 		Class<?> primitiveType = int.class; 		 		Class<?> clone = SerializationUtils.clone(primitiveType); 		assertEquals(primitiveType, clone); 	} }   The problem was already reported as a java bug http://bugs.sun.com/view_bug.do?bug_id=4171142 and ObjectInputStream is fixed since java version 1.4. The SerializationUtils problem arises because the SerializationUtils internally use the ClassLoaderAwareObjectInputStream that overrides the ObjectInputStream's resoleClass method without delegating to the super method in case of a ClassNotFoundException. I understand the intention of the ClassLoaderAwareObjectInputStream, but this implementation should also implement a fallback to the original implementation. For example:          protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {             String name = desc.getName();             try {                 return Class.forName(name, false, classLoader);             } catch (ClassNotFoundException ex) {             	try {             	     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());             	} catch (Exception e) { 		     return super.resolveClass(desc); 		}             }         }   Here is the code in ObjectInputStream that fixed the java bug.      protected Class<?> resolveClass(ObjectStreamClass desc) 	throws IOException, ClassNotFoundException     { 	String name = desc.getName(); 	try { 	    return Class.forName(name, false, latestUserDefinedLoader()); 	} catch (ClassNotFoundException ex) { 	    Class cl = (Class) primClasses.get(name); 	    if (cl != null) { 		return cl; 	    } else { 		throw ex; 	    } 	}     }$$patch1_1-Lang-13-RSRepairA$$Fix possible NPE in RSRepairDefects4J_Lang_13$$0
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-Cardumen$$Fix swapped diffs in patch$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-Cardumen$$Added missing throws$$1
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-Cardumen$$Add null check$$0
Closure-63$$None$$None$$patch1-Closure-63-Cardumen$$Add null check$$0
Chart-6$$None$$None$$patch1-Chart-6-Cardumen$$Added fix to chart .$$0
Chart-24$$None$$None$$patch1-Chart-24-Cardumen$$Fix bug in chart$$0
Chart-13$$None$$None$$patch1-Chart-13-Cardumen$$Fix arrange ( ) method$$0
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1-Math-95-Cardumen$$fix a minor bug in Controllers$$0
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-Math-73-Cardumen$$Fixed erroneous Brent algorithm$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-Cardumen$$Fix bug in Controllers / Websocket Session$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-Cardumen$$Fix erroneous test$$0
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-Cardumen$$Fix NaN in Object . equals ( )$$0
Lang-44$$NumberUtils createNumber thows a StringIndexOutOfBoundsException when only an "l" is passed in.$$Seems to be similar to LANG-300, except that if you don't place a digit in front of the "l" or "L" it throws a StringIndexOutOfBoundsException instead.$$patch1-Lang-44-Nopol2015$$Fix build$$1
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch1-Lang-55-Nopol2015$$Add missing stopTime if it is not running .$$1
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-Nopol2015$$don ' t allow duplicate x values in XYSeries$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-Nopol2015$$Fix renegation in BaseSecantSolver .$$0
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-Nopol2015$$Fix the case for better orthotoxical fit in LevenbergMarquardtOptimizer . java$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-Nopol2015$$Fix a minor increase in the number of iterations since the lowerBound was too high$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-Nopol2015$$Allow remove of default value if it is set$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch1-Time-11-Nopol2015$$Fix bug in DateTimeZoneBuilder$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-Nopol2015$$removed null check$$0
Lang-58$$NumberUtils.createNumber throws NumberFormatException for one digit long$$NumberUtils.createNumber throws a NumberFormatException when parsing "1l", "2l" .. etc... It works fine if you try to parse "01l" or "02l".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for "1l"$$patch1-Lang-58-Nopol2015$$Handle negative exponentiblity in StringUtils . java$$0
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-Nopol2015$$added missing loop$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch1-Closure-92-Hercules$$Fix accidental use of lastIndexOf$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-Hercules$$Fix typo in code snippet$$1
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch1-Closure-86-Hercules$$fix broken patch$$1
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-Hercules$$Fixed typo in code snippet$$1
Closure-109$$Constructor types that return all or unknown fail to parse$$None$$patch1-Closure-109-Hercules$$updated hercules patch$$1
Closure-4$$Converting from an interface type to a constructor which @implements itself causes stack overflow.$$None$$patch1-Closure-4-Hercules$$Added fix for cycles of closures$$1
Closure-78$$division by zero wrongly throws JSC_DIVIDE_BY_0_ERROR$$None$$patch1-Closure-78-Hercules$$disabled division by zero patch$$1
Closure-14$$bogus 'missing return' warning$$None$$patch1-Closure-14-Hercules$$added fix$$1
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-Hercules$$Fix bug in chart 9$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-Hercules$$fix bug in Hercules fixed markets$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-Hercules$$Fixed nullability assertion that was accidentally made too strong$$1
Chart-19$$None$$None$$patch1-Chart-19-Hercules$$Fix a bug in chart 19$$1
Chart-8$$None$$None$$patch1-Chart-8-Hercules$$Fixed week constructor$$1
Chart-3$$None$$None$$patch1-Chart-3-Hercules$$added fix$$1
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-Hercules$$Fixed a bug in MultiplePiePlot .$$1
Chart-14$$None$$None$$patch1-Chart-14-Hercules$$add fix$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-Hercules$$Fix float . max ( a , b )$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-Hercules$$Removed int because it wasn ' t used .$$1
Math-35$$Need range checks for elitismRate in ElitisticListPopulation constructors.$$There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.$$patch1-Math-35-Hercules$$ElitisticListPopulation constructor should set elitismRate before setting populationLimit .$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-Hercules$$updated hercules patch$$1
Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-Math-34-Hercules$$Fix bug in Hercules fixed$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-Hercules$$Updated patch1 - Math - 33 - Hercules . fixed$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-Hercules$$Fix getOwnPct return type$$1
Math-72$$Brent solver returns the wrong value if either bracket endpoint is root$$The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.$$patch1-Math-72-Hercules$$Fix the bug in Math - 72 - Hercules . fixed$$1
Math-43$$Statistics.setVarianceImpl makes getStandardDeviation produce NaN$$Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:  int[] scores = {1, 2, 3, 4}; SummaryStatistics stats = new SummaryStatistics(); stats.setVarianceImpl(new Variance(false)); //use "population variance" for(int i : scores) {   stats.addValue(i); } double sd = stats.getStandardDeviation(); System.out.println(sd);   A workaround suggested by Mikkel is:    double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());$$patch1-Math-43-Hercules$$added missing if ($$1
Math-98$$RealMatrixImpl#operate gets result vector dimensions wrong$$org.apache.commons.math.linear.RealMatrixImpl#operate tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix. Thus line 640 in RealMatrixImpl.java should read double[] out = new double[nRows]; instead of double[] out = new double[v.length];$$patch1-Math-98-Hercules$$Fix the array to work with BigDecimal ( or other BigDecimal ) .$$1
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles$$patch1-Math-30-Hercules$$changed int to double$$1
Math-46$$Division by zero$$In class Complex, division by zero always returns NaN. I think that it should return NaN only when the numerator is also ZERO, otherwise the result should be INF. See here.$$patch1-Math-46-Hercules$$Add fix in MATH - 657$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-Hercules$$updated hercules patch$$1
Math-24$$"BrentOptimizer" not always reporting the best point$$BrentOptimizer (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.$$patch1-Math-24-Hercules$$It is better for testing$$1
Math-4$$NPE when calling SubLine.intersection() with non-intersecting lines$$When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations. The attached patch fixes both implementations and adds the required test cases.$$patch1-Math-4-Hercules$$Extend fix for 1 . 8 . 0 - FIX$$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-Hercules$$Fix the if / else .$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-Hercules$$removed patch$$1
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-Hercules$$added fix$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-Hercules$$Fixed typo in hercules . fixed$$1
Math-25$$"HarmonicFitter.ParameterGuesser" sometimes fails to return sensible values$$The inner class "ParameterGuesser" in "HarmonicFitter" (package "o.a.c.m.optimization.fitting") fails to compute a usable guess for the "amplitude" parameter.$$patch1-Math-25-Hercules$$Add a throw if the pair of arguments are not compatible with 1 . 0 .$$1
Time-26$$.withHourOfDay() sets hour inconsistantly on DST transition.$$When the hour of day is set to the ambiguous hour on the daylight to standard time transition in a given time zone the result is inconsistent for different time zones. Shoul the hour be set to the daylight hour or the standard hour for all time zones? I can't find anything that documents this behavior.  My test code below returns different results for different time zones. The very last assertion fails on the Australia time zone cutover.$$patch1-Time-26-Hercules$$Handle bug in ISO 639 - 2$$1
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch1-Time-4-Hercules$$Fix partial constructor to work with joda - time$$1
Time-15$$possibly a bug in org.joda.time.field.FieldUtils.safeMultiply$$It seems to me that as currently written in joda-time-2.1.jar org.joda.time.field.FieldUtils.safeMultiply(long val1, int scalar) doesn't detect the overflow if the long val1 == Long.MIN_VALUE and the int scalar == -1.  The attached file demonstrates what I think is the bug and suggests a patch.  I looked at the Joda Time bugs list in SourceForge but couldn't see anything that looked relevant.$$patch1-Time-15-Hercules$$Add fix for safeMultiply ( long , int )$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-Hercules$$Fix bug$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-Hercules$$Fixed bug in isAvailableLocale$$1
Lang-60$$StrBuilder contains usages of thisBuf.length when they should use size$$While fixing LANG-294 I noticed that there are two other places in StrBuilder that reference thisBuf.length and unless I'm mistaken they shouldn't.$$patch1-Lang-60-Hercules$$Fix bug$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch1-Lang-33-Hercules$$added fix$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-Hercules$$Fix bug in Hercules fixed$$1
Lang-26$$FastDateFormat.format() outputs incorrect week of year because locale isn't respected$$FastDateFormat apparently doesn't respect the locale it was sent on creation when outputting week in year (e.g. "ww") in format(). It seems to use the settings of the system locale for firstDayOfWeek and minimalDaysInFirstWeek, which (depending on the year) may result in the incorrect week number being output. Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:  import java.util.Calendar; import java.util.Date; import java.util.Locale; import java.text.SimpleDateFormat;  import org.apache.commons.lang.time.FastDateFormat;  public class FastDateFormatWeekBugDemo {     public static void main(String[] args) {         Locale.setDefault(new Locale("en", "US"));         Locale locale = new Locale("sv", "SE");          Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome         cal.set(2010, 0, 1, 12, 0, 0);         Date d = cal.getTime();         System.out.println("Target date: " + d);          FastDateFormat fdf = FastDateFormat.getInstance("EEEE', week 'ww", locale);         SimpleDateFormat sdf = new SimpleDateFormat("EEEE', week 'ww", locale);         System.out.println("FastDateFormat:   " + fdf.format(d)); // will output "FastDateFormat:   fredag, week 01"         System.out.println("SimpleDateFormat: " + sdf.format(d)); // will output "SimpleDateFormat: fredag, week 53"     } }   If sv/SE is passed to Locale.setDefault() instead of en/US, both FastDateFormat and SimpleDateFormat output the correct week number.$$patch1-Lang-26-Hercules$$Missing locale .$$1
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-Hercules$$don ' t add QUOTE to fix$$1
Lang-38$$DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations$$If a Calendar object is constructed in certain ways a call to Calendar.setTimeZone does not correctly change the Calendars fields.  Calling Calenar.getTime() seems to fix this problem.  While this is probably a bug in the JDK, it would be nice if DateFormatUtils was smart enough to detect/resolve this problem. For example, the following unit test fails:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";      // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)     // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);       FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }   However, this unit test passes:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);     cal.getTime();      FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }$$patch1-Lang-38-Hercules$$Fixed formatting of date with timeZone$$1
Lang-24$$NumberUtils.isNumber(String)  is not right when the String is "1.1L"$$"1.1L"  is not a Java Number . but NumberUtils.isNumber(String) return true. perhaps change:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp;             }   to:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp && !hasDecPoint;             }$$patch1-Lang-24-Hercules$$don ' t allow L with a decimal point$$1
Lang-47$$StrBuilder appendFixedWidth does not handle nulls$$Appending a null value with fixed width causes a null pointer exception if getNullText() has not been set.$$patch1-Lang-47-Hercules$$Fix whitespace in string . setLength ( )$$1
Closure-133$$Exception when parsing erroneous jsdoc: /**@return {@code foo} bar   *    baz. */$$None$$patch1-Closure-133-Hercules$$don ' t look ahead for empty lines$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-Hercules$$Fix the try / finally blocks used in the AST .$$0
Closure-107$$Variable names prefixed with MSG_ cause error with advanced optimizations$$None$$patch1-Closure-107-Hercules$$HHH - 9733 - Set typeBasedOptimizationOptions ( ) to the right level so it$$0
Closure-46$$ClassCastException during TypeCheck pass$$None$$patch1-Closure-46-Hercules$$don ' t include regexps in closures$$0
Closure-12$$Try/catch blocks incorporate code not inside original blocks$$None$$patch1-Closure-12-Hercules$$Allow apply of control flow graphs$$0
Chart-6$$None$$None$$patch1-Chart-6-Hercules$$Updated chart with fixed shape$$0
Chart-17$$cloning of TimeSeries$$It's just a minor bug!  When I clone a TimeSeries which has no items, I get an IllegalArgumentException ("Requires start <= end"). But I don't think the user should be responsible for checking whether the TimeSeries has any items or not.$$patch1-Chart-17-Hercules$$Fix bug in delete ( )$$0
Math-104$$Special functions not very accurate$$The Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I'd expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.$$patch1-Math-104-Hercules$$fixed too high epsilon delay in java 1 . 0 Math$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-Hercules$$Fix typo in reference manual$$0
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?$$patch1-Math-32-Hercules$$added fix$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-Math-20-Hercules$$added back missing patch$$0
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-Math-73-Hercules$$update hercules fixed spot$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-Hercules$$Fixed a bug in Math - 80 - Hercules .$$0
Math-101$$java.lang.StringIndexOutOfBoundsException in ComplexFormat.parse(String source, ParsePosition pos)$$The parse(String source, ParsePosition pos) method in the ComplexFormat class does not check whether the imaginary character is set or not which produces StringIndexOutOfBoundsException in the substring method : (line 375 of ComplexFormat) ...         // parse imaginary character         int n = getImaginaryCharacter().length();         startIndex = pos.getIndex();         int endIndex = startIndex + n;         if (source.substring(startIndex, endIndex).compareTo(             getImaginaryCharacter()) != 0) { ... I encoutered this exception typing in a JTextFied with ComplexFormat set to look up an AbstractFormatter. If only the user types the imaginary part of the complex number first, he gets this exception. Solution: Before setting to n length of the imaginary character, check if the source contains it. My proposal: ...         int n = 0;         if (source.contains(getImaginaryCharacter()))         n = getImaginaryCharacter().length(); ...		  F.S.$$patch1-Math-101-Hercules$$Removed symbolic link to error patch .$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-Hercules$$Fix bug in equals$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-Hercules$$Updated numeric average patch$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch1-Time-11-Hercules$$Add string comparison to zone names$$0
Time-14$$Unable to add days to a MonthDay set to the ISO leap date$$It's not possible to add days to a MonthDay set to the ISO leap date (February 29th). This is even more bizarre given the exact error message thrown.$$patch1-Time-14-Hercules$$Missing addWrapPartial method from DateField . add ( ) .$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-Hercules$$removed a couple incorrect patching attempts$$0
Lang-58$$NumberUtils.createNumber throws NumberFormatException for one digit long$$NumberUtils.createNumber throws a NumberFormatException when parsing "1l", "2l" .. etc... It works fine if you try to parse "01l" or "02l".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for "1l"$$patch1-Lang-58-Hercules$$Handle isDigits ( string ) with patch 1 - Lang - 58 - Hercules . fixed$$0
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-Hercules$$fixed year in StringUtils$$0
Lang-44$$NumberUtils createNumber thows a StringIndexOutOfBoundsException when only an "l" is passed in.$$Seems to be similar to LANG-300, except that if you don't place a digit in front of the "l" or "L" it throws a StringIndexOutOfBoundsException instead.$$patch1-Lang-44-Hercules$$Fix null - penalty in long precision spotting$$0
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-Hercules$$added fix$$0
Math-79$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest { 	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ), 				new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size()); 	} }$$patch1-Math-79-LSRepair$$Fix MathUtils . distance ( ) , reported by Marin Dzhigarov .$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-LSRepair$$Optimize toBoolean ( )$$1
Lang-54$$LocaleUtils.toLocale() rejects strings with only language+variant$$LocaleUtils.toLocale() throws an exception on strings containing a language and a variant but no country code. For example : fr__POSIX This string can be produced with the JDK by instanciating a Locale with an empty string for the country : new Locale("fr", "", "POSIX").toString(). According to the javadoc for the Locale class a variant is allowed with just a language code or just a country code. Commons Configuration handles this case in its PropertyConverter.toLocale() method. I'd like to replace our implementation by the one provided by LocaleUtils, but our tests fail due to this case.$$patch1-Lang-54-LSRepair$$locale parsing was throwing an exception$$1
Chart-18$$None$$None$$patch1-Chart-18-LSRepair$$Fixed help for snapshotconverter$$0
Chart-6$$None$$None$$patch1-Chart-6-LSRepair$$Fix Added missing equals method$$0
Chart-17$$cloning of TimeSeries$$It's just a minor bug!  When I clone a TimeSeries which has no items, I get an IllegalArgumentException ("Requires start <= end"). But I don't think the user should be responsible for checking whether the TimeSeries has any items or not.$$patch1-Chart-17-LSRepair$$Fixed a bug with not throwing the clone exception$$0
Chart-10$$None$$None$$patch1-Chart-10-LSRepair$$added System . exit ( 0 ) to method generateToolTipFragment$$0
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1-Math-95-LSRepair$$removed erroneous legacy change .$$0
Math-93$$MathUtils.factorial(n) fails for n >= 17$$The result of MathUtils.factorial( n ) for n = 17, 18, 19 is wrong, probably because of rounding errors in the double calculations. Replace the first line of MathUtilsTest.testFactorial() by         for (int i = 1; i <= 20; i++) { to check all valid arguments for the long result and see the failure. I suggest implementing a simple loop to multiply the long result - or even using a precomputed long[] - instead of adding logarithms.$$patch1-Math-93-LSRepair$$removed erroneous legacy call in MathUtils$$0
Math-11$$MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd$$To reproduce:  Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);$$patch1-Math-11-LSRepair$$Fix density method$$0
Math-16$$FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts$$As reported by Jeff Hain: cosh(double) and sinh(double): Math.cosh(709.783) = 8.991046692770538E307 FastMath.cosh(709.783) = Infinity Math.sinh(709.783) = 8.991046692770538E307 FastMath.sinh(709.783) = Infinity ===> This is due to using exp( x )/2 for values of |x| above 20: the result sometimes should not overflow, but exp( x ) does, so we end up with some infinity. ===> for values of |x| >= StrictMath.log(Double.MAX_VALUE), exp will overflow, so you need to use that instead: for x positive: double t = exp(x*0.5); return (0.5*t)*t; for x negative: double t = exp(-x*0.5); return (-0.5*t)*t;$$patch1-Math-16-LSRepair$$Fix typo in cosh ( )$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-LSRepair$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted ( ) .$$0
Math-99$$MathUtils.gcd(Integer.MIN_VALUE, 0) should throw an Exception instead of returning Integer.MIN_VALUE$$The gcd method should throw an Exception for gcd(Integer.MIN_VALUE, 0), like for gcd(Integer.MIN_VALUE, Integer.MIN_VALUE). The method should only return nonnegative results.$$patch1-Math-99-LSRepair$$Fix a bug in MathUtils . gcd ( )$$0
Lang-60$$StrBuilder contains usages of thisBuf.length when they should use size$$While fixing LANG-294 I noticed that there are two other places in StrBuilder that reference thisBuf.length and unless I'm mistaken they shouldn't.$$patch1-Lang-60-LSRepair$$StrBuilder . contains ( ) now returns true if the builder contains the character$$0
Lang-29$$SystemUtils.getJavaVersionAsFloat throws StringIndexOutOfBoundsException on Android runtime/Dalvik VM$$Can be replicated in the Android emulator quite easily. Stack trace:   at org.apache.commons.lang.builder.ToStringBuilder.<clinit>(ToStringBuilder.java:98) E/AndroidRuntime( 1681): 	... 17 more E/AndroidRuntime( 1681): Caused by: java.lang.ExceptionInInitializerError E/AndroidRuntime( 1681): 	at org.apache.commons.lang.builder.ToStringStyle$MultiLineToStringStyle.<init>(ToStringStyle.java:2276) E/AndroidRuntime( 1681): 	at org.apache.commons.lang.builder.ToStringStyle.<clinit>(ToStringStyle.java:94) E/AndroidRuntime( 1681): 	... 18 more E/AndroidRuntime( 1681): Caused by: java.lang.StringIndexOutOfBoundsException E/AndroidRuntime( 1681): 	at java.lang.String.substring(String.java:1571) E/AndroidRuntime( 1681): 	at org.apache.commons.lang.SystemUtils.getJavaVersionAsFloat(SystemUtils.java:1153) E/AndroidRuntime( 1681): 	at org.apache.commons.lang.SystemUtils.<clinit>(SystemUtils.java:818)$$patch1-Lang-29-LSRepair$$Fix toJavaVersionInt from String to boolean$$0
Lang-62$$unescapeXml("&12345678;") should be "&12345678;"$$Following test (in EntitiesTest.java) fails:     public void testNumberOverflow() throws Exception  {         doTestUnescapeEntity("&#12345678;", "&#12345678;");         doTestUnescapeEntity("x&#12345678;y", "x&#12345678;y");         doTestUnescapeEntity("&#x12345678;", "&#x12345678;");         doTestUnescapeEntity("x&#x12345678;y", "x&#x12345678;y");     }  Maximim value for char is 0xFFFF, so &#12345678; is invalid entity reference, and so should be left as is.$$patch1-Lang-62-LSRepair$$Don ' t ignore entity values in HTML entities , since they are never unescaped$$0
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch1-Lang-55-LSRepair$$Add refresh files after stop$$0
Lang-41$$ClassUtils.getShortClassName() will not work with an array;  it seems to add a semicolon to the end.$$A semicolon is introduced into the class name at the end for all arrays... String sArray[] = new String[2]; sArray[0] = "mark"; sArray[1] = "is cool"; String simpleString = "chris"; assertEquals("String", ClassUtils.getShortClassName(simpleString, null)); assertEquals("String;", ClassUtils.getShortClassName(sArray, null));$$patch1-Lang-41-LSRepair$$Fix ClassUtils # getShortClassName ( )$$0
Lang-40$$Fix case-insensitive string handling$$String.to*Case() is locale-sensitive, this is usually not intended for case-insensitive comparisions. Please see Common Bug #3 for details.$$patch1-Lang-40-LSRepair$$fixed StringUtils . containsIgnoreCase ( )$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch1-Closure-92-CoCoNut$$Fixing the indexing of the closure ' s namespace .$$1
Closure-18$$Dependency sorting with closurePass set to false no longer works.$$None$$patch1-Closure-18-CoCoNut$$closure pass now fixes the build .$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-CoCoNut$$Fix typo in codeGenerator$$1
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch1-Closure-86-CoCoNut$$Add missing copy of NodeUtil$$1
Closure-38$$Identifier minus a negative number needs a space between the "-"s$$None$$patch1-Closure-38-CoCoNut$$Improved javadoc comment .$$1
Closure-31$$Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLY$$None$$patch1-Closure-31-CoCoNut$$don ' t skip building all passes when building$$1
Closure-46$$ClassCastException during TypeCheck pass$$None$$patch1-Closure-46-CoCoNut$$Improved method return type for JSType . getLeastSupertype ( JSType )$$1
Closure-70$$unexpected typed coverage of less than 100%$$None$$patch1-Closure-70-CoCoNut$$Fix copy constructor$$1
Closure-40$$smartNameRemoval causing compiler crash$$None$$patch1-Closure-40-CoCoNut$$Fix copy nag$$1
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-CoCoNut$$Fix an issue with TimeSeries . isEmpty ( ) .$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-CoCoNut$$added missing copy$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-CoCoNut$$Fix null checking in AbstractCategoryItemRenderer$$1
Chart-26$$None$$None$$patch1-Chart-26-CoCoNut$$Fix NPE in Axis label copy .$$1
Chart-24$$None$$None$$patch1-Chart-24-CoCoNut$$Fix bug in GrayPaintScale . getPaint ( double )$$1
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-CoCoNut$$Missing warning$$1
Chart-14$$None$$None$$patch1-Chart-14-CoCoNut$$Allow user to specify which technologies apply to a background range marker$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-CoCoNut$$Fix NaN in FastMath . max ( a , b )$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-CoCoNut$$KMeansPlusPlusClusterer copy constructor$$1
Math-56$$MultidimensionalCounter.getCounts(int) returns wrong array of indices$$MultidimensionalCounter counter = new MultidimensionalCounter(2, 4); for (Integer i : counter) {     int[] x = counter.getCounts;     System.out.println(i + " " + Arrays.toString); } Output is: 0 [0, 0] 1 [0, 1] 2 [0, 2] 3 [0, 2]   <=== should be [0, 3] 4 [1, 0] 5 [1, 1] 6 [1, 2] 7 [1, 2]   <=== should be [1, 3]$$patch1-Math-56-CoCoNut$$Added missing gen folder$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-CoCoNut$$Added copy of GaussianFitter . fit ( ) .$$1
Math-94$$MathUtils.gcd(u, v) fails when u and v both contain a high power of 2$$The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.         assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15))); Fix: Replace the test at the start of MathUtils.gcd()         if (u * v == 0) { by         if (u == 0 || v == 0) {$$patch1-Math-94-CoCoNut$$Fix gcd ( )$$1
Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-Math-34-CoCoNut$$added iterator on non - empty chromosomes list$$1
Math-27$$Fraction percentageValue rare overflow$$The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value. The patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.$$patch1-Math-27-CoCoNut$$Added Fraction copy of Fraction . percentageValue ( )$$1
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-CoCoNut$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted ( ) .$$1
Math-65$$weight versus sigma in AbstractLeastSquares$$In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.  Once corrected, getRMS() can even reduce  public double getRMS()  {return Math.sqrt(getChiSquare()/rows);}$$patch1-Math-65-CoCoNut$$Added a copy of the residuals function .$$1
Math-98$$RealMatrixImpl#operate gets result vector dimensions wrong$$org.apache.commons.math.linear.RealMatrixImpl#operate tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix. Thus line 640 in RealMatrixImpl.java should read double[] out = new double[nRows]; instead of double[] out = new double[v.length];$$patch1-Math-98-CoCoNut$$Fix BigMatrixImpl to work with data . length opengles .$$1
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles$$patch1-Math-30-CoCoNut$$long not int ( not long )$$1
Math-90$$Bugs in Frequency API$$I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.$$patch1-Math-90-CoCoNut$$Fixing rat phase$$1
Math-77$$getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)$$the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries. The current implementation in ArrayRealVector has a typo:      public double getLInfNorm() {         double max = 0;         for (double a : data) {             max += Math.max(max, Math.abs(a));         }         return max;     }   the += should just be an =. There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness). Worse, the implementation in OpenMapRealVector is not even positive semi-definite:          public double getLInfNorm() {         double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             max += iter.value();         }         return max;     }   I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():    public double getLInfNorm() {     double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null) {       norm = Math.max(norm, Math.abs(e.getValue()));     }     return norm;   }   Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.$$patch1-Math-77-CoCoNut$$Fix error in getLInfNorm .$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-CoCoNut$$Improved the garbage collection profile of TwoDimTableau .$$1
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-CoCoNut$$Remove extraneous whitespace$$1
Math-22$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time. The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned. [1] http://markmail.org/message/dxuxh7eybl7xejde$$patch1-Math-22-CoCoNut$$Fix FDistribution and UniformRealDistribution .$$1
Time-19$$Inconsistent interpretation of ambiguous time during DST$$The inconsistency appears for timezone Europe/London.  These three DateTime objects should all represent the same moment in time even if they are ambiguous. Now, it always returns the earlier instant (summer time) during an overlap.$$patch1-Time-19-CoCoNut$$fixed typo .$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-CoCoNut$$StrBuilder copy ( ) uses string copy ( ) for the copy constructor$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-CoCoNut$$made copy ( ) not static$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-CoCoNut$$Added missing return statement$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch1-Lang-33-CoCoNut$$removed null check$$1
Lang-29$$SystemUtils.getJavaVersionAsFloat throws StringIndexOutOfBoundsException on Android runtime/Dalvik VM$$Can be replicated in the Android emulator quite easily. Stack trace:   at org.apache.commons.lang.builder.ToStringBuilder.<clinit>(ToStringBuilder.java:98) E/AndroidRuntime( 1681): 	... 17 more E/AndroidRuntime( 1681): Caused by: java.lang.ExceptionInInitializerError E/AndroidRuntime( 1681): 	at org.apache.commons.lang.builder.ToStringStyle$MultiLineToStringStyle.<init>(ToStringStyle.java:2276) E/AndroidRuntime( 1681): 	at org.apache.commons.lang.builder.ToStringStyle.<clinit>(ToStringStyle.java:94) E/AndroidRuntime( 1681): 	... 18 more E/AndroidRuntime( 1681): Caused by: java.lang.StringIndexOutOfBoundsException E/AndroidRuntime( 1681): 	at java.lang.String.substring(String.java:1571) E/AndroidRuntime( 1681): 	at org.apache.commons.lang.SystemUtils.getJavaVersionAsFloat(SystemUtils.java:1153) E/AndroidRuntime( 1681): 	at org.apache.commons.lang.SystemUtils.<clinit>(SystemUtils.java:818)$$patch1-Lang-29-CoCoNut$$Fix toJavaVersionInt from String to float$$1
Lang-10$$FastDateParser does not handle white-space properly$$The SimpleDateFormat Javadoc does not treat white-space specially, however FastDateParser treats a single white-space as being any number of white-space characters. This means that FDP will parse dates that fail when parsed by SDP.$$patch1-Lang-10-CoCoNut$$FastDateParser copy ( ) didn ' t ignore whitespace in date parsing$$1
Lang-26$$FastDateFormat.format() outputs incorrect week of year because locale isn't respected$$FastDateFormat apparently doesn't respect the locale it was sent on creation when outputting week in year (e.g. "ww") in format(). It seems to use the settings of the system locale for firstDayOfWeek and minimalDaysInFirstWeek, which (depending on the year) may result in the incorrect week number being output. Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:  import java.util.Calendar; import java.util.Date; import java.util.Locale; import java.text.SimpleDateFormat;  import org.apache.commons.lang.time.FastDateFormat;  public class FastDateFormatWeekBugDemo {     public static void main(String[] args) {         Locale.setDefault(new Locale("en", "US"));         Locale locale = new Locale("sv", "SE");          Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome         cal.set(2010, 0, 1, 12, 0, 0);         Date d = cal.getTime();         System.out.println("Target date: " + d);          FastDateFormat fdf = FastDateFormat.getInstance("EEEE', week 'ww", locale);         SimpleDateFormat sdf = new SimpleDateFormat("EEEE', week 'ww", locale);         System.out.println("FastDateFormat:   " + fdf.format(d)); // will output "FastDateFormat:   fredag, week 01"         System.out.println("SimpleDateFormat: " + sdf.format(d)); // will output "SimpleDateFormat: fredag, week 53"     } }   If sv/SE is passed to Locale.setDefault() instead of en/US, both FastDateFormat and SimpleDateFormat output the correct week number.$$patch1-Lang-26-CoCoNut$$FastDateFormat copy constructor$$1
Mockito-29$$fixed a verify() call example in @Captor javadoc.$$None$$patch1-Mockito-29-CoCoNut$$Fix appendText ( ) with null values .$$1
Mockito-8$$1.10 regression (StackOverflowError) with interface where generic type has itself as upper bound.$$None$$patch1-Mockito-8-CoCoNut$$Fix contextualActualTypeParameters . put ( typeParameter , actualTypeArgument )$$1
Mockito-38$$Generate change list separated by types using labels$$As discussed on the mailing list instead of one big list of "Improvements" the change list for the release is divided into change types based on labels. It is required to specify which labels should be considered separately. Some other labels can be excluded (like "question" or "refactoring"). There is also headerForOtherChanges method to override default "Other" header.$$patch1-Mockito-38-CoCoNut$$Fix toStringEquals in ArgumentMatchingTool$$1
Mockito-5$$Mockito 1.10.x timeout verification needs JUnit classes (VerifyError, NoClassDefFoundError)$$If JUnit is not on the classpath and mockito is version 1.10.x (as of now 1.10.1 up to 1.10.19) and the code is using the timeout verification which is not supposed to be related to JUnit, then the JVM may fail with a VerifyError or a NoClassDefFoundError.$$patch1-Mockito-5-CoCoNut$$Fixed the build .$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-kPAR$$Fix typo in codeGenerator where ' c ' was accidentally passed .$$1
Closure-40$$smartNameRemoval causing compiler crash$$None$$patch1-Closure-40-kPAR$$Fix boolean flags$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-kPAR$$Fix nullability note in AbstractCategoryItemRenderer$$1
Chart-19$$None$$None$$patch1-Chart-19-kPAR$$Fix NPE in CategoryPlot . getRangeAxisIndex ( )$$1
Chart-4$$None$$None$$patch1-Chart-4-kPAR$$Fix X axis annotation presence$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-kPAR$$Added missing parameter .$$1
Math-89$$Bugs in Frequency API$$I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.$$patch1-Math-89-kPAR$$Add support for addValue ( )$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-kPAR$$Fix getFrequency ( ) to return precise value$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-kPAR$$fixed a typo in BisectionSolver$$1
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1-Lang-7-kPAR$$formatting is not supported by NumberUtils$$1
Closure-35$$assignment to object in conditional causes type error on function w/ record type return type$$None$$patch1-Closure-35-kPAR$$Make TypeInference . collapseUnion a union type .$$0
Closure-129$$Casting a function before calling it produces bad code and breaks plugin code$$None$$patch1-Closure-129-kPAR$$Allow recursion in CALL nodes$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-kPAR$$Fix try / finally .$$0
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-kPAR$$Remove spurious blank line$$0
Closure-109$$Constructor types that return all or unknown fail to parse$$None$$patch1-Closure-109-kPAR$$Add parseAndRecordTypeNode to the right type AST .$$0
Closure-63$$None$$None$$patch1-Closure-63-kPAR$$Remove spurious blank line$$0
Closure-46$$ClassCastException during TypeCheck pass$$None$$patch1-Closure-46-kPAR$$Fix jstype annotation matching of non - Record type .$$0
Closure-115$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1-Closure-115-kPAR$$Allow inline with function arguments$$0
Chart-7$$None$$None$$patch1-Chart-7-kPAR$$Fixed formatting mistake$$0
Chart-17$$cloning of TimeSeries$$It's just a minor bug!  When I clone a TimeSeries which has no items, I get an IllegalArgumentException ("Requires start <= end"). But I don't think the user should be responsible for checking whether the TimeSeries has any items or not.$$patch1-Chart-17-kPAR$$fix # 1796$$0
Chart-26$$None$$None$$patch1-Chart-26-kPAR$$Fix CategoryPlot not using a renderer$$0
Chart-15$$None$$None$$patch1-Chart-15-kPAR$$Fix an NPE if data is not loaded$$0
Chart-3$$None$$None$$patch1-Chart-3-kPAR$$Fixed a bug in TimeSeries where the data was not removed .$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-kPAR$$Remove oversampling .$$0
Chart-13$$None$$None$$patch1-Chart-13-kPAR$$Fix checkstyle issue$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-kPAR$$don ' t allow duplicates in XYSeries$$0
Chart-14$$None$$None$$patch1-Chart-14-kPAR$$Don ' t remove null marker$$0
Chart-25$$None$$None$$patch1-Chart-25-kPAR$$Fix getMeanValue ( )$$0
Math-104$$Special functions not very accurate$$The Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I'd expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.$$patch1-Math-104-kPAR$$Fix case$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-kPAR$$Fix typo in BaseSecantSolver$$0
Math-7$$event state not updated if an unrelated event triggers a RESET_STATE during ODE integration$$When an ODE solver manages several different event types, there are some unwanted side effects. If one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return. As a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20. The attached file reproduces the problem.$$patch1-Math-7-kPAR$$Added BracketingNthOrderBrentSolver .$$0
Math-42$$Negative value with restrictNonNegative$$Problem: commons-math-2.2 SimplexSolver. A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call: SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true); Function 1 * x + 1 * y + 0 Constraints: 1 * x + 0 * y = 1 Result: x = 1; y = -1; Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.$$patch1-Math-42-kPAR$$Fix hash code for SimplexTableau class$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-kPAR$$EigenDecompositionImpl should do a good step$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch1-Math-8-kPAR$$Fix Object . array to be compatible with Java 8$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-kPAR$$EigenDecompositionImpl was trying to handle both heaps .$$0
Math-43$$Statistics.setVarianceImpl makes getStandardDeviation produce NaN$$Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:  int[] scores = {1, 2, 3, 4}; SummaryStatistics stats = new SummaryStatistics(); stats.setVarianceImpl(new Variance(false)); //use "population variance" for(int i : scores) {   stats.addValue(i); } double sd = stats.getStandardDeviation(); System.out.println(sd);   A workaround suggested by Mikkel is:    double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());$$patch1-Math-43-kPAR$$Fix bug in SummaryStatistics$$0
Math-88$$Simplex Solver arrives at incorrect solution$$I have reduced the problem reported to me down to a minimal test case which I will attach.$$patch1-Math-88-kPAR$$Fix the case of one - to - one calls to tableau . getEntry ( ) in the$$0
Math-62$$Miscellaneous issues concerning the "optimization" package$$Revision 990792 contains changes triggered the following issues:  MATH-394 MATH-397 MATH-404  This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):  "BrentOptimizer": a specific convergence checker must be used. "LevenbergMarquardtOptimizer" also has specific convergence checks. Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical): 	 See "BrentOptimizer" and "LevenbergMarquardtOptimizer", the algorithm passes "points" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker). In "PowellOptimizer" the line search ("BrentOptimizer") tolerances depend on the tolerances within the main algorithm. Since tolerances come with "ConvergenceChecker" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals ("BrentOptimizer" field) to the enclosing class ("PowellOptimizer").   Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all. Class "DirectSearchOptimizer" (in package "optimization.direct") inherits from class "AbstractScalarOptimizer" (in package "optimization.general"). Some interfaces are defined in package "optimization" but their base implementations (abstract class that contain the boiler-plate code) are in package "optimization.general" (e.g. "DifferentiableMultivariateVectorialOptimizer" and "BaseAbstractVectorialOptimizer"). No check is performed to ensure the the convergence checker has been set (see e.g. "BrentOptimizer" and "PowellOptimizer"); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker. "NonLinearConjugateGradientOptimizer": Ugly workaround for the checked "ConvergenceException". Everywhere, we trail the checked "FunctionEvaluationException" although it is never used. There remains some duplicate code (such as the "multi-start loop" in the various "MultiStart..." implementations). The "ConvergenceChecker" interface is very general (the "converged" method can take any number of "...PointValuePair"). However there remains a "semantic" problem: One cannot be sure that the list of points means the same thing for the caller of "converged" and within the implementation of the "ConvergenceChecker" that was independently set. It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In "LevenbergMarquartdOptimizer" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations. In "AbstractLeastSquaresOptimizer" and "LevenbergMarquardtOptimizer", occurences of "OptimizationException" were replaced by the unchecked "ConvergenceException" but in some cases it might not be the most appropriate one. "MultiStartUnivariateRealOptimizer": in the other classes ("MultiStartMultivariate...") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval). The Javadoc utility raises warnings (see output of "mvn site") which I couldn't figure out how to correct. Some previously existing classes and interfaces have become no more than a specialisation of new "generics" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.$$patch1-Math-62-kPAR$$Fix a bug in MultiStartUnivariateRealOptimizer from side effects of adding additional bounds$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-kPAR$$Add missing @@$$0
Math-84$$MultiDirectional optimzation loops forver if started at the correct solution$$MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.$$patch1-Math-84-kPAR$$Fix a typo in MultiDirectional .$$0
Math-15$$FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53$$As reported by Jeff Hain: pow(double,double): Math.pow(-1.0,5.000000000000001E15) = -1.0 FastMath.pow(-1.0,5.000000000000001E15) = 1.0 ===> This is due to considering that power is an even integer if it is >= 2^52, while you need to test that it is >= 2^53 for it. ===> replace "if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)" with "if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)" and that solves it.$$patch1-Math-15-kPAR$$FastMath . pow ( - x , y ) doesn ' t handle exponent range in JS backend$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-kPAR$$Added DEFAULT_EPSILON to the min value of the tableau object .$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-kPAR$$Fix MathRuntimeException buildMessage$$0
Math-40$$BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary$$In some cases, the aging feature in BracketingNthOrderBrentSolver fails. It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket. In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).$$patch1-Math-40-kPAR$$fix the sign change index so it works$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-kPAR$$Remove warning ( added serialVersionUID )$$0
Time-17$$Bug on withLaterOffsetAtOverlap method$$On the last two brackets we can see that withLaterOffsetAtOverlap is not undoing withEarlierOffsetAtOverlap as it should ( and not even working at all ).$$patch1-Time-17-kPAR$$Fix an issue with DateTimeZone . getOffset ( ) where the difference was not being applied , because the$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-kPAR$$StrBuilder should add the right size , but not the left .$$0
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-kPAR$$Add null check in LocaleUtils$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-kPAR$$removed backslash$$0
Lang-58$$NumberUtils.createNumber throws NumberFormatException for one digit long$$NumberUtils.createNumber throws a NumberFormatException when parsing "1l", "2l" .. etc... It works fine if you try to parse "01l" or "02l".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for "1l"$$patch1-Lang-58-kPAR$$Fix null positives in NumberUtils$$0
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch1-Lang-20-kPAR$$Fix bug in StringUtils$$0
Lang-18$$FastDateFormat formats year differently than SimpleDateFormat in Java 7$$Starting with Java 7 does SimpleDateFormat format a year pattern of 'Y' or 'YYY' as '2003' instead of '03' as in former Java releases. According Javadoc this pattern should have been always been formatted as number, therefore the new behavior seems to be a bug fix in the JDK. FastDateFormat is adjusted to behave the same.$$patch1-Lang-18-kPAR$$FastDateFormat now accepts 2 - digit year number$$0
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-kPAR$$removed expPos$$0
Lang-16$$NumberUtils does not handle upper-case hex: 0X and -0X$$NumberUtils.createNumber() should work equally for 0x1234 and 0X1234; currently 0X1234 generates a NumberFormatException Integer.decode() handles both upper and lower case hex.$$patch1-Lang-16-kPAR$$removed extraneous space$$0
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-kPAR$$fixed try / case$$0
Lang-10$$FastDateParser does not handle white-space properly$$The SimpleDateFormat Javadoc does not treat white-space specially, however FastDateParser treats a single white-space as being any number of white-space characters. This means that FDP will parse dates that fail when parsed by SDP.$$patch1-Lang-10-kPAR$$FastDateParser ignores backslash which prevents me from processing the date value as literals$$0
Lang-21$$DateUtils.isSameLocalTime does not work correct$$Hi, I think I found a bug in the DateUtils class in the method isSameLocalTime. Example:  Calendar a = Calendar.getInstance(); a.setTimeInMillis(1297364400000L); Calendar b = Calendar.getInstance(); b.setTimeInMillis(1297321200000L); Assert.assertFalse(DateUtils.isSameLocalTime(a, b)); This is because the method compares  cal1.get(Calendar.HOUR) == cal2.get(Calendar.HOUR)  but I think it has to be  cal1.get(Calendar.HOUR_OF_DAY) == cal2.get(Calendar.HOUR_OF_DAY)$$patch1-Lang-21-kPAR$$Add missing modifier$$0
Lang-44$$NumberUtils createNumber thows a StringIndexOutOfBoundsException when only an "l" is passed in.$$Seems to be similar to LANG-300, except that if you don't place a digit in front of the "l" or "L" it throws a StringIndexOutOfBoundsException instead.$$patch1-Lang-44-kPAR$$Fix build$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-kPAR$$Don ' t strip backslash when creating a string in the message format$$0
Lang-53$$Dates.round() behaves incorrectly for minutes and seconds$$Get unexpected output for rounding by minutes or seconds. public void testRound() {     Calendar testCalendar = Calendar.getInstance(TimeZone.getTimeZone("GMT"));     testCalendar.set(2007, 6, 2, 8, 9, 50);     Date date = testCalendar.getTime();     System.out.println("Before round() " + date);     System.out.println("After round()  " + DateUtils.round(date, Calendar.MINUTE)); } --2.1 produces Before round() Mon Jul 02 03:09:50 CDT 2007 After round()  Mon Jul 02 03:10:00 CDT 2007  this is what I would expect --2.2 and 2.3 produces Before round() Mon Jul 02 03:09:50 CDT 2007 After round()  Mon Jul 02 03:01:00 CDT 2007  this appears to be wrong$$patch1-Lang-53-kPAR$$Don ' t round if ( ! round || millisecs < 500 )$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch1-Lang-63-kPAR$$Fix merge issues$$0
Lang-41$$ClassUtils.getShortClassName() will not work with an array;  it seems to add a semicolon to the end.$$A semicolon is introduced into the class name at the end for all arrays... String sArray[] = new String[2]; sArray[0] = "mark"; sArray[1] = "is cool"; String simpleString = "chris"; assertEquals("String", ClassUtils.getShortClassName(simpleString, null)); assertEquals("String;", ClassUtils.getShortClassName(sArray, null));$$patch1-Lang-41-kPAR$$Fix ClassUtils # getShortClassName / getShortCanonicalName$$0
Lang-24$$NumberUtils.isNumber(String)  is not right when the String is "1.1L"$$"1.1L"  is not a Java Number . but NumberUtils.isNumber(String) return true. perhaps change:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp;             }   to:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp && !hasDecPoint;             }$$patch1-Lang-24-kPAR$$allow trailing comma$$0
Closure-33$$weird object literal invalid property error on unrelated object prototype$$None$$patch1-Closure-33-Jaid$$Avoid matching against an unneeded property .$$1
Closure-18$$Dependency sorting with closurePass set to false no longer works.$$None$$patch1-Closure-18-Jaid$$"Fix the "" closure pass "" flag in Compiler . java"$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-Jaid$$Fix typo in codeGenerator where ' c ' was passed through , but was accidentally ignored .$$1
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-Jaid$$Fix tryMinimizeExitPoints for the case of finally blocks$$1
Closure-31$$Add support for --manage_closure_dependencies and --only_closure_dependencies with compilation level WHITESPACE_ONLY$$None$$patch1-Closure-31-Jaid$$Allow closure pass through$$1
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-Jaid$$Remove whitespaces from sourceExcerpt .$$1
Closure-63$$None$$None$$patch1-Closure-63-Jaid$$Remove whitespaces from sourceExcerpt$$1
Closure-70$$unexpected typed coverage of less than 100%$$None$$patch1-Closure-70-Jaid$$Fix typed scope creator for parameter types .$$1
Closure-40$$smartNameRemoval causing compiler crash$$None$$patch1-Closure-40-Jaid$$Fix boolean property access warning$$1
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-Jaid$$Fix an issue with TimeSeries . isEmptyRange ( )$$1
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch2-Chart-9-Jaid$$Fix an issue with time series end index < startIndex$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-Jaid$$Fix nullability note in AbstractCategoryItemRenderer$$1
Chart-26$$None$$None$$patch1-Chart-26-Jaid$$Fix null pointer check in axis title bar shows weird text$$1
Chart-26$$None$$None$$patch2-Chart-26-Jaid$$Fix null pointer check in Axis . get ( )$$1
Chart-26$$None$$None$$patch3-Chart-26-Jaid$$Fix null pointer check in Axis .$$1
Chart-24$$None$$None$$patch1-Chart-24-Jaid$$Fix value to be in the same range as the lowerBound ( and upperBound ) axes .$$1
Chart-24$$None$$None$$patch2-Chart-24-Jaid$$Fix value to be in the same range as the lowerBound ( this . upperBound )$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-Jaid$$Fix renegation of boolean methods$$1
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?$$patch1-Math-32-Jaid$$removed debug code$$1
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-Jaid$$Fixed a bug in EigenDecompositionImpl . java$$1
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch2-Math-80-Jaid$$Fixed a bug in EigenDecompositionImpl . java$$1
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch1-Math-53-Jaid$$Add NaN to Complex . add ( )$$1
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch2-Math-53-Jaid$$Add the isNaN check$$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-Jaid$$Remove a redundant check$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-Jaid$$removed epsilon label from tableau objective function$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-Jaid$$Add the inverse of Complex . INF to check for 0 . 0 precision .$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch2-Math-5-Jaid$$Add the inverse of Complex . INF as well .$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch3-Math-5-Jaid$$Fixed a minor typo in Complex . reciprocal ( )$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-Jaid$$missing break$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch6-Lang-51-Jaid$$missing bracket$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch5-Lang-51-Jaid$$Added missing case in BooleanUtils$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch3-Lang-51-Jaid$$missing break$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch4-Lang-51-Jaid$$missing test for Y$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch1-Lang-33-Jaid$$removed unnecessary loop$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch2-Lang-33-Jaid$$removed unnecessary copy of ClassUtils$$1
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-Jaid$$Fixed misc problems caused by the use of substringBefore and substringAfter methods in WordUtils$$1
Lang-38$$DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations$$If a Calendar object is constructed in certain ways a call to Calendar.setTimeZone does not correctly change the Calendars fields.  Calling Calenar.getTime() seems to fix this problem.  While this is probably a bug in the JDK, it would be nice if DateFormatUtils was smart enough to detect/resolve this problem. For example, the following unit test fails:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";      // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)     // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);       FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }   However, this unit test passes:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);     cal.getTime();      FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }$$patch1-Lang-38-Jaid$$Reset the calendar time to GMT on Sunday .$$1
Lang-38$$DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations$$If a Calendar object is constructed in certain ways a call to Calendar.setTimeZone does not correctly change the Calendars fields.  Calling Calenar.getTime() seems to fix this problem.  While this is probably a bug in the JDK, it would be nice if DateFormatUtils was smart enough to detect/resolve this problem. For example, the following unit test fails:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";      // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)     // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);       FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }   However, this unit test passes:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);     cal.getTime();      FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }$$patch2-Lang-38-Jaid$$Reset timeZone on format ( ) .$$1
Lang-38$$DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations$$If a Calendar object is constructed in certain ways a call to Calendar.setTimeZone does not correctly change the Calendars fields.  Calling Calenar.getTime() seems to fix this problem.  While this is probably a bug in the JDK, it would be nice if DateFormatUtils was smart enough to detect/resolve this problem. For example, the following unit test fails:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";      // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)     // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);       FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }   However, this unit test passes:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);     cal.getTime();      FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }$$patch3-Lang-38-Jaid$$Fix null pointer check in FastDateFormat$$1
Lang-38$$DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations$$If a Calendar object is constructed in certain ways a call to Calendar.setTimeZone does not correctly change the Calendars fields.  Calling Calenar.getTime() seems to fix this problem.  While this is probably a bug in the JDK, it would be nice if DateFormatUtils was smart enough to detect/resolve this problem. For example, the following unit test fails:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";      // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)     // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);       FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }   However, this unit test passes:    public void testFormat_CalendarIsoMsZulu() {     final String dateTime = "2009-10-16T16:42:16.000Z";     GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone("GMT-8"));     cal.clear();     cal.set(2009, 9, 16, 8, 42, 16);     cal.getTime();      FastDateFormat format = FastDateFormat.getInstance("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", TimeZone.getTimeZone("GMT"));     assertEquals("dateTime", dateTime, format.format(cal));   }$$patch4-Lang-38-Jaid$$Fix null pointer check in FastDateFormat$$1
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch1-Lang-55-Jaid$$Add a test to make sure that no one keeps being left in StopWatch after a running state ==$$1
Closure-33$$weird object literal invalid property error on unrelated object prototype$$None$$patch1-Closure-33-Jaid$$Added a missing if / else .$$0
Closure-33$$weird object literal invalid property error on unrelated object prototype$$None$$patch2-Closure-33-Jaid$$Allow property types to be inferred from boolean to string .$$0
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-Jaid$$don ' t add whitespace to source experpt$$0
Closure-63$$None$$None$$patch1-Closure-63-Jaid$$don ' t add whitespace to source experpt$$0
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-Jaid$$Fix bug for empty range$$0
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch2-Chart-9-Jaid$$Fix bug$$0
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch3-Chart-9-Jaid$$Fix bug # 944$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-Jaid$$Fix null pointer check in AbstractCategoryItemRenderer$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch2-Chart-1-Jaid$$Fix nullability assertion in AbstractCategoryItemRenderer$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch3-Chart-1-Jaid$$Fix legend item legend generation$$0
Chart-26$$None$$None$$patch1-Chart-26-Jaid$$Fix hotspot positioning .$$0
Chart-26$$None$$None$$patch2-Chart-26-Jaid$$Fix null histograms getting cut off .$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-Jaid$$Fix regula_FALSI case .$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch3-Math-50-Jaid$$Fix renegation of baseSecantSolver .$$0
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?$$patch1-Math-32-Jaid$$Fixed a bug in PolygonsSet . getAttribute$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-Jaid$$Fixed a bug in EigenDecompositionImpl . flipArrays$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch2-Math-80-Jaid$$Fixed a bug in EigenDecompositionImpl . java$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch3-Math-80-Jaid$$Fixed a bug in EigenDecompositionImpl . flipArrays$$0
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch1-Math-53-Jaid$$Add the NaN check , as the other one is also possible , as the numerator + rhs .$$0
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch2-Math-53-Jaid$$Add the NaN argument to Complex . add ( )$$0
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch3-Math-53-Jaid$$Add NaN to Complex . add ( )$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-Jaid$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch2-Math-85-Jaid$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch3-Math-85-Jaid$$Fix a false convergence check in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch4-Math-85-Jaid$$Fix a false reporting of convergence$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-Jaid$$@@ minRatio = rhs / entry ; if ( minRatio == rhs ) {$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch2-Math-82-Jaid$$Added a check for equality$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch3-Math-82-Jaid$$Fixed a bug in the minRatioSolver .$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch4-Math-82-Jaid$$@@ i = 1 ; if ( ! ( i == 1 ) ) {$$0
Lang-61$$StrBuilder.replaceAll and StrBuilder.deleteAll can throw ArrayIndexOutOfBoundsException.$$StrBuilder.replaceAll and StrBuilder.deleteAll can thrown ArrayIndexOutOfBoundsException's. Here are a couple of additions to the StrBuilderTest class that demonstrate this problem: StrBuilder.deleteAll() - added to testDeleteAll_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.deleteAll("\n%BLAH%");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the following error: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.deleteImpl(StrBuilder.java:1114) 	at org.apache.commons.lang.text.StrBuilder.deleteAll(StrBuilder.java:1188) 	at org.apache.commons.lang.text.StrBuilderTest.testDeleteAll_String(StrBuilderTest.java:606) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) StrBuilder.replaceAll() - added to testReplaceAll_String_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.replaceAll("\n%BLAH%", "");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the exception: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.replaceImpl(StrBuilder.java:1256) 	at org.apache.commons.lang.text.StrBuilder.replaceAll(StrBuilder.java:1339) 	at org.apache.commons.lang.text.StrBuilderTest.testReplaceAll_String_String(StrBuilderTest.java:763) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)$$patch1-Lang-61-Jaid$$Stop appending extra characters .$$0
Lang-61$$StrBuilder.replaceAll and StrBuilder.deleteAll can throw ArrayIndexOutOfBoundsException.$$StrBuilder.replaceAll and StrBuilder.deleteAll can thrown ArrayIndexOutOfBoundsException's. Here are a couple of additions to the StrBuilderTest class that demonstrate this problem: StrBuilder.deleteAll() - added to testDeleteAll_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.deleteAll("\n%BLAH%");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the following error: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.deleteImpl(StrBuilder.java:1114) 	at org.apache.commons.lang.text.StrBuilder.deleteAll(StrBuilder.java:1188) 	at org.apache.commons.lang.text.StrBuilderTest.testDeleteAll_String(StrBuilderTest.java:606) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) StrBuilder.replaceAll() - added to testReplaceAll_String_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.replaceAll("\n%BLAH%", "");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the exception: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.replaceImpl(StrBuilder.java:1256) 	at org.apache.commons.lang.text.StrBuilder.replaceAll(StrBuilder.java:1339) 	at org.apache.commons.lang.text.StrBuilderTest.testReplaceAll_String_String(StrBuilderTest.java:763) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)$$patch2-Lang-61-Jaid$$Don ' t compare string builders to buffer length - trailing space .$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-Jaid$$missing if ($$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch2-Lang-51-Jaid$$Fixed bug in BooleanUtils where empty string is not the case$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch3-Lang-51-Jaid$$missing if ($$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch4-Lang-51-Jaid$$missing one if ($$0
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-Jaid$$Fixed WordUtils . isEmpty ( ) .$$0
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch2-Lang-45-Jaid$$Fixed WordUtils . isEmpty ( ) .$$0
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch1-Lang-55-Jaid$$Add missing return statement$$0
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch2-Lang-55-Jaid$$Reset running state to 1$$0
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch3-Lang-55-Jaid$$Added missing return statement$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-JGenProg2015$$Fix renegation of baseSecantSolver$$1
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch1-Math-53-JGenProg2015$$Add the isNaN check in Complex . add ( )$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-JGenProg2015$$fixed erroneous loop$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-JGenProg2015$$Fix NaN - > INF in Complex$$1
Chart-7$$None$$None$$patch1-Chart-7-JGenProg2015$$Fixed formatting mistake .$$0
Chart-15$$None$$None$$patch1-Chart-15-JGenProg2015$$Added missing call to fireChartChanged ( ) .$$0
Chart-13$$None$$None$$patch6-Chart-13-Arja$$Fix border arrangement bug$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-JGenProg2015$$Fixed issue with XYSeries . add ( )$$0
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-Math-73-JGenProg2015$$changed 28 .$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-JGenProg2015$$Fix double put$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-JGenProg2015$$removed ceil$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch1-Time-4-JGenProg2015$$Fixed zero - is - max datetime field$$0
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-GenPat$$Fix typo in codeGenerator$$1
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch1-Closure-86-GenPat$$Add missing copy of NodeUtil$$1
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-GenPat$$Remove spurious blank line$$1
Closure-63$$None$$None$$patch1-Closure-63-GenPat$$Remove spurious blank line$$1
Closure-115$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1-Closure-115-GenPat$$Fixing the function infusion of side effects in JS stylesheet$$1
Closure-2$$combining @interface and multiple @extends can crash compiler$$None$$patch1-Closure-2-GenPat$$Added TypeCheck . checkForProxy ( ) to interfaceType$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-GenPat$$added missing copy$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-GenPat$$Fix null checking in AbstractCategoryItemRenderer$$1
Chart-24$$None$$None$$patch1-Chart-24-GenPat$$Fix bug in GrayPaintScale . getPaint ( double )$$1
Chart-4$$None$$None$$patch1-Chart-4-GenPat$$added missing annotation$$1
Math-79$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest { 	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ), 				new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size()); 	} }$$patch1-Math-79-GenPat$$Fix MathUtils distance$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-GenPat$$Added copy of bisectionSolver . solve ( )$$1
Math-4$$NPE when calling SubLine.intersection() with non-intersecting lines$$When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations. The attached patch fixes both implementations and adds the required test cases.$$patch1-Math-4-GenPat$$added null check in subLine copy$$1
Math-22$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time. The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned. [1] http://markmail.org/message/dxuxh7eybl7xejde$$patch1-Math-22-GenPat$$UniformRealDistribution copy ( )$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-GenPat$$Added missing copy of BooleanUtils$$1
Lang-60$$StrBuilder contains usages of thisBuf.length when they should use size$$While fixing LANG-294 I noticed that there are two other places in StrBuilder that reference thisBuf.length and unless I'm mistaken they shouldn't.$$patch1-Lang-60-GenPat$$StrBuilder . contains ( ) now uses the same level of storage as ArrayList .$$1
Lang-21$$DateUtils.isSameLocalTime does not work correct$$Hi, I think I found a bug in the DateUtils class in the method isSameLocalTime. Example:  Calendar a = Calendar.getInstance(); a.setTimeInMillis(1297364400000L); Calendar b = Calendar.getInstance(); b.setTimeInMillis(1297321200000L); Assert.assertFalse(DateUtils.isSameLocalTime(a, b)); This is because the method compares  cal1.get(Calendar.HOUR) == cal2.get(Calendar.HOUR)  but I think it has to be  cal1.get(Calendar.HOUR_OF_DAY) == cal2.get(Calendar.HOUR_OF_DAY)$$patch1-Lang-21-GenPat$$Fixed fall through in DateUtils . java$$1
Lang-47$$StrBuilder appendFixedWidth does not handle nulls$$Appending a null value with fixed width causes a null pointer exception if getNullText() has not been set.$$patch1-Lang-47-GenPat$$Fix an NPE in StrBuilder . toString ( )$$1
Mockito-22$$Can not Return deep stubs from generic method that returns generic type.$$if I try to mock a generic method which a generic returntype, where the returntype is derived from the generic type of the method using deep stubs I get a ClassCastException when calling when on it. When you don't use deep stubs and a raw Supplier mock to pass around it works:$$patch1-Mockito-22-GenPat$$added a bit more tidying of the areEqual method$$1
Closure-38$$Identifier minus a negative number needs a space between the "-"s$$None$$patch1-Closure-38-GenPat$$Fix negative zero error in code consumer$$0
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-GenPat$$Fix bug in TimeSeries . java$$0
Chart-7$$None$$None$$patch1-Chart-7-GenPat$$Fix copy ( )$$0
Chart-26$$None$$None$$patch1-Chart-26-GenPat$$Don ' t create a new plotRenderingInfo object$$0
Chart-14$$None$$None$$patch1-Chart-14-GenPat$$Changed return type of removeRangeMarker$$0
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1-Math-95-GenPat$$Fix FDistributionImpl . getDenominatorDegreesOfFreedom ( )$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-GenPat$$Added another copy of BaseSecantSolver$$0
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-GenPat$$added kmeans + plus clusterer copy$$0
Math-94$$MathUtils.gcd(u, v) fails when u and v both contain a high power of 2$$The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.         assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15))); Fix: Replace the test at the start of MathUtils.gcd()         if (u * v == 0) { by         if (u == 0 || v == 0) {$$patch1-Math-94-GenPat$$Fix gcd ( )$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-GenPat$$EigenDecompositionImpl flipIfWarranted ( n , 2 ) added eigen$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-GenPat$$Added missing case$$0
Math-72$$Brent solver returns the wrong value if either bracket endpoint is root$$The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.$$patch1-Math-72-GenPat$$Fix the bit between the two functions$$0
Math-101$$java.lang.StringIndexOutOfBoundsException in ComplexFormat.parse(String source, ParsePosition pos)$$The parse(String source, ParsePosition pos) method in the ComplexFormat class does not check whether the imaginary character is set or not which produces StringIndexOutOfBoundsException in the substring method : (line 375 of ComplexFormat) ...         // parse imaginary character         int n = getImaginaryCharacter().length();         startIndex = pos.getIndex();         int endIndex = startIndex + n;         if (source.substring(startIndex, endIndex).compareTo(             getImaginaryCharacter()) != 0) { ... I encoutered this exception typing in a JTextFied with ComplexFormat set to look up an AbstractFormatter. If only the user types the imaginary part of the complex number first, he gets this exception. Solution: Before setting to n length of the imaginary character, check if the source contains it. My proposal: ...         int n = 0;         if (source.contains(getImaginaryCharacter()))         n = getImaginaryCharacter().length(); ...		  F.S.$$patch1-Math-101-GenPat$$fix parseAndIgnoreWhitespace ( source , pos )$$0
Math-77$$getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)$$the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries. The current implementation in ArrayRealVector has a typo:      public double getLInfNorm() {         double max = 0;         for (double a : data) {             max += Math.max(max, Math.abs(a));         }         return max;     }   the += should just be an =. There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness). Worse, the implementation in OpenMapRealVector is not even positive semi-definite:          public double getLInfNorm() {         double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             max += iter.value();         }         return max;     }   I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():    public double getLInfNorm() {     double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null) {       norm = Math.max(norm, Math.abs(e.getValue()));     }     return norm;   }   Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.$$patch1-Math-77-GenPat$$Fix ArrayRealVector . getLInfNorm ( )$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch1-Time-4-GenPat$$Added zeroismaxdatetimefield copy$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-GenPat$$StrBuilder copy ( ) {$$0
Lang-58$$NumberUtils.createNumber throws NumberFormatException for one digit long$$NumberUtils.createNumber throws a NumberFormatException when parsing "1l", "2l" .. etc... It works fine if you try to parse "01l" or "02l".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for "1l"$$patch1-Lang-58-GenPat$$Fix parse error$$0
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch1-Lang-20-GenPat$$Fix toString method of StringUtils$$0
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-GenPat$$Fix copy ( )$$0
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-GenPat$$Fixed null pointer check in WordUtils$$0
Lang-44$$NumberUtils createNumber thows a StringIndexOutOfBoundsException when only an "l" is passed in.$$Seems to be similar to LANG-300, except that if you don't place a digit in front of the "l" or "L" it throws a StringIndexOutOfBoundsException instead.$$patch1-Lang-44-GenPat$$Manchester United FC = > NumberUtils copy ( )$$0
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-GenPat$$Fix copy / paste error$$0
Mockito-38$$Generate change list separated by types using labels$$As discussed on the mailing list instead of one big list of "Improvements" the change list for the release is divided into change types based on labels. It is required to specify which labels should be considered separately. Some other labels can be excluded (like "question" or "refactoring"). There is also headerForOtherChanges method to override default "Other" header.$$patch1-Mockito-38-GenPat$$Add null check in ArgumentMatchingTool$$0
Closure-18$$Dependency sorting with closurePass set to false no longer works.$$None$$patch1-Closure-18-SequenceR$$Don ' t inline closure passes when compiling dependency fields .$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-SequenceR$$Fix typo in codeGenerator . java$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch2-Closure-73-SequenceR$$Fix typo in codeGenerator . java$$1
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch1-Closure-86-SequenceR$$Fix NodeUtil . hasLocalResult ( )$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-SequenceR$$added p2 . getPathIterator ( null ) to fix looping over PathIterator$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-SequenceR$$Fix null pointer check in AbstractCategoryItemRenderer$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-SequenceR$$Fix int overflow$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-SequenceR$$GaussianFitter . fit ( ) now uses 1 . 18 . 0 ( and is using$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-SequenceR$$Fix getFrequency ( Object ) to return precise value instead of getCumPct ( Object )$$1
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles$$patch1-Math-30-SequenceR$$changed int to double .$$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-SequenceR$$Fix a numeric equality bug in UnivariateRealSolverUtils$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-SequenceR$$Fixed an example of the regression in LinearSolver . . .$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch2-Math-82-SequenceR$$Using less restrictive conditionals .$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-SequenceR$$NaN - > Complex . INF$$1
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-SequenceR$$StrBuilder . append ( obj , width )$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-SequenceR$$Fix bug in CharSequenceTranslator$$1
Closure-92$$bug with implicit namespaces across modules$$None$$patch8-Closure-92-SequenceR$$Fix closure closures ' definition .$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch1-Closure-92-SequenceR$$Fix closure closures ' positioning .$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch6-Closure-92-SequenceR$$Fix closure closures to error code .$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch7-Closure-92-SequenceR$$Fix a warning$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch9-Closure-92-SequenceR$$Fix a warning$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch5-Closure-92-SequenceR$$Fix a warning$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch2-Closure-92-SequenceR$$Fix closure closures ' null arguments ' error$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch10-Closure-92-SequenceR$$Fix closure closures ' positioning mistake$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch3-Closure-92-SequenceR$$Fix an issue with ' . ' in ProcessClosurePrimitives . java$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch4-Closure-92-SequenceR$$Fix an issue with that damn code style error .$$0
Closure-93$$None$$None$$patch8-Closure-93-SequenceR$$Fix closure closures ' definition .$$0
Closure-93$$None$$None$$patch1-Closure-93-SequenceR$$Fix closure closures ' positioning .$$0
Closure-93$$None$$None$$patch6-Closure-93-SequenceR$$Fix closure closures to error code .$$0
Closure-93$$None$$None$$patch9-Closure-93-SequenceR$$Fix a warning$$0
Closure-93$$None$$None$$patch5-Closure-93-SequenceR$$Fix a warning$$0
Closure-93$$None$$None$$patch2-Closure-93-SequenceR$$Fix closure closures ' null arguments ' error$$0
Closure-93$$None$$None$$patch10-Closure-93-SequenceR$$Fix closure closures ' positioning mistake$$0
Closure-93$$None$$None$$patch3-Closure-93-SequenceR$$Fix an issue with ' . ' in ProcessClosurePrimitives . java$$0
Closure-93$$None$$None$$patch4-Closure-93-SequenceR$$Fix an issue with that damn code style error .$$0
Closure-18$$Dependency sorting with closurePass set to false no longer works.$$None$$patch1-Closure-18-SequenceR$$Fix a bug in the compiler ' s dependency check$$0
Closure-18$$Dependency sorting with closurePass set to false no longer works.$$None$$patch5-Closure-18-SequenceR$$Fix a null pointer check that would trigger an NPE .$$0
Closure-18$$Dependency sorting with closurePass set to false no longer works.$$None$$patch3-Closure-18-SequenceR$$Fix a null pointer check that would trigger an error .$$0
Closure-18$$Dependency sorting with closurePass set to false no longer works.$$None$$patch4-Closure-18-SequenceR$$Fix null pointer check .$$0
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-SequenceR$$Fix typo in codeGenerator . java$$0
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch5-Closure-86-SequenceR$$Allow for an aliased constructor to be used in a few places$$0
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch2-Closure-86-SequenceR$$AllowImmutableValue check for value types .$$0
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch3-Closure-86-SequenceR$$Fix bug in NodeUtil . isFunctionLike ( )$$0
Closure-86$$side-effects analysis incorrectly removing function calls with side effects$$None$$patch4-Closure-86-SequenceR$$Add toStringMethodCall check$$0
Closure-38$$Identifier minus a negative number needs a space between the "-"s$$None$$patch1-Closure-38-SequenceR$$Fix negative zero logspam$$0
Closure-38$$Identifier minus a negative number needs a space between the "-"s$$None$$patch2-Closure-38-SequenceR$$Fix negative zero logspam$$0
Closure-38$$Identifier minus a negative number needs a space between the "-"s$$None$$patch3-Closure-38-SequenceR$$Fix negative zero presence in JSON$$0
Closure-123$$Generates code with invalid for/in left-hand assignment$$None$$patch1-Closure-123-SequenceR$$Fix the rhs context for the for - init CLAUSE token .$$0
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-SequenceR$$Fix an issue with TimeSeries . isEmptyRange ( ) .$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch2-Chart-1-SequenceR$$Fix nullability note in AbstractCategoryItemRenderer$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch3-Chart-1-SequenceR$$Fix null checking in AbstractCategoryItemRenderer$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch4-Chart-1-SequenceR$$Fix nullability note in AbstractCategoryItemRenderer$$0
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-SequenceR$$Fix NaN handling in FastMath . max ( )$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-SequenceR$$Fixed a bug in EigenDecompositionImpl . flipIfWarranted$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-SequenceR$$Fix a bug in MathUtils . equals$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch8-Math-85-SequenceR$$Fix a bug in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch6-Math-85-SequenceR$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch15-Math-85-SequenceR$$Fix a finite loop in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch12-Math-85-SequenceR$$Fix a finite loop in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch7-Math-85-SequenceR$$Fix a bug in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch9-Math-85-SequenceR$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch13-Math-85-SequenceR$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch14-Math-85-SequenceR$$Fix a numeric equality bug in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch11-Math-85-SequenceR$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch16-Math-85-SequenceR$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch5-Math-85-SequenceR$$Fix a bug in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch2-Math-85-SequenceR$$Fix a bug in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch17-Math-85-SequenceR$$Fix a false reporting of convergence$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch10-Math-85-SequenceR$$Fix a bug in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch3-Math-85-SequenceR$$Fix a bug in UnivariateRealSolverUtils$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch4-Math-85-SequenceR$$Fix a false reporting of convergence$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch3-Math-82-SequenceR$$Fix the example for the linear search method ' EEG '$$0
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch2-Lang-6-SequenceR$$Don ' t count surrogate pairs as we don ' t understand code points at the beginning of a$$0
Chart-3$$None$$None$$patch1-Chart-3-Arja$$Find bounds by iteration .$$1
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-Arja$$Set the dataset on the pie chart .$$1
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1_1-Chart-5-Arja$$Use the method provided for sorting XYSeries$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-Arja$$Fix renegation of baseSecantSolver$$1
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-Arja$$GaussianFitter . fit ( ) now uses parameter guesser$$1
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-Math-73-Arja$$Added verifyBracketing to BrentSolver$$1
Math-98$$RealMatrixImpl#operate gets result vector dimensions wrong$$org.apache.commons.math.linear.RealMatrixImpl#operate tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix. Thus line 640 in RealMatrixImpl.java should read double[] out = new double[nRows]; instead of double[] out = new double[v.length];$$patch1-Math-98-Arja$$Fix BigMatrixImpl to verify that vector has the same length$$1
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch1-Math-53-Arja$$Add one - line ifs$$1
Math-39$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.$$patch1-Math-39-Arja$$Fix step size = t - stepStart ;$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-Arja$$fixed erroneous loop$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-Arja$$Fix NaN - > INF in Complex$$1
Math-22$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time. The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned. [1] http://markmail.org/message/dxuxh7eybl7xejde$$patch1-Math-22-Arja$$Fix FDistribution and UniformRealDistribution . isSupportLowerBoundInclusive ( )$$1
Time-15$$possibly a bug in org.joda.time.field.FieldUtils.safeMultiply$$It seems to me that as currently written in joda-time-2.1.jar org.joda.time.field.FieldUtils.safeMultiply(long val1, int scalar) doesn't detect the overflow if the long val1 == Long.MIN_VALUE and the int scalar == -1.  The attached file demonstrates what I think is the bug and suggests a patch.  I looked at the Joda Time bugs list in SourceForge but couldn't see anything that looked relevant.$$patch1-Time-15-Arja$$Fix a bug in FieldUtils$$1
Lang-35$$ArrayUtils.add(T[] array, T element) can create unexpected ClassCastException$$ArrayUtils.add(T[] array, T element) can create an unexpected ClassCastException. For example, the following code compiles without a warning:  String[] sa = ArrayUtils.add(stringArray, aString);   and works fine, provided at least one of the parameters is non-null. However, if both parameters are null, the add() method returns an Object[] array, hence the Exception. If both parameters are null, it's not possible to determine the correct array type to return, so it seems to me this should be disallowed. I think the method ought to be changed to throw IllegalParameterException when both parameters are null.$$patch1-Lang-35-Arja$$Fix newly introduced switch case$$1
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch1-Lang-20-Arja$$Fix bug in StringUtils$$1
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-Arja$$Extend word utils upper bounds$$1
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-Arja$$don ' t append QUOTE if escaping is on$$1
Chart-7$$None$$None$$patch1-Chart-7-Arja$$Fixed formatting mistake .$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-Arja$$Fix category item label generator on initial refresh .$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch6-Chart-1-Arja$$Remove redundant return statement$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch5-Chart-1-Arja$$Remove unnecessary check for null dataset$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch2-Chart-1-Arja$$Fix category item renderer to reset the counts as well$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch3-Chart-1-Arja$$Fix category item renderer to save row count in case of data series changes$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch4-Chart-1-Arja$$Fix background annotations for category items$$0
Chart-19$$None$$None$$patch1-Chart-19-Arja$$Added throw exception if object is null$$0
Chart-15$$None$$None$$patch1-Chart-15-Arja$$don ' t draw no data message if dataset is empty$$0
Chart-15$$None$$None$$patch2-Chart-15-Arja$$don ' t draw no data message if dataset is empty ( no data message )$$0
Chart-15$$None$$None$$patch3-Chart-15-Arja$$don ' t drawOutline if dataset is empty ( ie , no data )$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch8-Chart-12-Arja$$Add a listener to MultiplePiePlot .$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-Arja$$Add a listener to the dataset group object , so we can use the group object to get the$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch6-Chart-12-Arja$$Add a listener to MultiplePiePlot .$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch7-Chart-12-Arja$$Add a listener to the dataset group object , so we can use it for any dataset$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch5-Chart-12-Arja$$Add a listener to the dataset group option on pie plot .$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch2-Chart-12-Arja$$Add a listener to the dataset$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch3-Chart-12-Arja$$Fixed dataExtractOrder$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch4-Chart-12-Arja$$Add a listener to MultiplePiePlot$$0
Chart-13$$None$$None$$patch1-Chart-13-Arja$$Fix border embarrassment .$$0
Chart-13$$None$$None$$patch6-Chart-13-Arja$$Fix border arrangement bug$$0
Chart-13$$None$$None$$patch5-Chart-13-Arja$$Fix border embarrassment in FAQ$$0
Chart-13$$None$$None$$patch2-Chart-13-Arja$$Remove redundant top border feature from border arranger$$0
Chart-13$$None$$None$$patch3-Chart-13-Arja$$Fix border arrangement$$0
Chart-13$$None$$None$$patch4-Chart-13-Arja$$Fix bottom border appearance bug$$0
Chart-25$$None$$None$$patch5-Chart-25-Arja$$Fixed a bug in DefaultStatisticalCategoryDataset$$0
Chart-25$$None$$None$$patch2-Chart-25-Arja$$Fixed a bug where DefaultStatisticalCategoryDataset . getColumnCount ( ) was called with no arguments$$0
Chart-25$$None$$None$$patch3-Chart-25-Arja$$Reset the data to empty .$$0
Chart-25$$None$$None$$patch4-Chart-25-Arja$$Fix a bug in DefaultStatisticalCategoryDataset$$0
Math-68$$LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it$$LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.$$patch1-Math-68-Arja$$Fix a regression in LevenbergMarquardtOptimizer . java$$0
Math-56$$MultidimensionalCounter.getCounts(int) returns wrong array of indices$$MultidimensionalCounter counter = new MultidimensionalCounter(2, 4); for (Integer i : counter) {     int[] x = counter.getCounts;     System.out.println(i + " " + Arrays.toString); } Output is: 0 [0, 0] 1 [0, 1] 2 [0, 2] 3 [0, 2]   <=== should be [0, 3] 4 [1, 0] 5 [1, 1] 6 [1, 2] 7 [1, 2]   <=== should be [1, 3]$$patch1-Math-56-Arja$$Fixed a bug in MultidimensionalCounter$$0
Math-56$$MultidimensionalCounter.getCounts(int) returns wrong array of indices$$MultidimensionalCounter counter = new MultidimensionalCounter(2, 4); for (Integer i : counter) {     int[] x = counter.getCounts;     System.out.println(i + " " + Arrays.toString); } Output is: 0 [0, 0] 1 [0, 1] 2 [0, 2] 3 [0, 2]   <=== should be [0, 3] 4 [1, 0] 5 [1, 1] 6 [1, 2] 7 [1, 2]   <=== should be [1, 3]$$patch2-Math-56-Arja$$removed some redundant code$$0
Math-60$$ConvergenceException in NormalDistributionImpl.cumulativeProbability()$$I get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity. For instance in the following code: 	@Test 	public void testCumulative() { 		final NormalDistribution nd = new NormalDistributionImpl(); 		for (int i = 0; i < 500; i++) { 			final double val = Math.exp; 			try  { 				System.out.println("val = " + val + " cumulative = " + nd.cumulativeProbability(val)); 			}  catch (MathException e)  { 				e.printStackTrace(); 				fail(); 			} 		} 	} In version 2.0, I get no exception.  My suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.$$patch1-Math-60-Arja$$Missing continue loop$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch8-Math-20-Arja$$Fix direct msampling to benefit from diagD = diag ( x , i )$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-Math-20-Arja$$Update the CMAESOptimizer ' s lambda function to the same value as the input array , with$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch6-Math-20-Arja$$Fix CMAESOptimizer log101 .$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch7-Math-20-Arja$$Remove erroneous loop$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch9-Math-20-Arja$$CACHE_DIRECT_MODE = 100 ; LPS - 77383 Remove interm$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch11-Math-20-Arja$$Remove intermittent inverse of x$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch5-Math-20-Arja$$Fix CMAESOptimizer ' s fix$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch2-Math-20-Arja$$Fix CMAESOptimizer ' s value range .$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch10-Math-20-Arja$$CACHE_NON_CONSTANTS_TO_LOGICAL - 1216$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch3-Math-20-Arja$$Fix CMAESOptimizer ' s line ending$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch8-Math-80-Arja$$EigenDecompositionImpl . java$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-Arja$$Fixed erroneous loop in EigenDecompositionImpl . java$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch6-Math-80-Arja$$Fixed a bug in the EigenDecompositionImpl . flipArray ( ) method where the last$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch15-Math-80-Arja$$EigenDecompositionImpl . flipValues ( ) didn ' t do anything due to local variable$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch12-Math-80-Arja$$EigenDecompositionImpl . minValueOf ( double , double , double ) added missing min$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch7-Math-80-Arja$$Fixed a bug in EigenDecompositionImpl . flip ( ) .$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch9-Math-80-Arja$$Fixed a bug in EigenDecompositionImpl . flip ( ) .$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch13-Math-80-Arja$$Fixed EigenDecompositionImpl . flipRows ( ) .$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch14-Math-80-Arja$$Fixed a bug in EigenDecompositionImpl . flipRows$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch11-Math-80-Arja$$EigenDecompositionImpl . innerProduct is not working correctly$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch5-Math-80-Arja$$Fix EigenDecompositionImpl . g = 0 . 25 ;$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch2-Math-80-Arja$$Fixed a bug in EigenDecompositionImpl . flipArrays$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch10-Math-80-Arja$$Fixed a bug in EigenDecompositionImpl . flipArrays$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch3-Math-80-Arja$$EigenDecompositionImpl . flipRows ( ) didn ' t do anything due to local variable$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch4-Math-80-Arja$$EigenDecompositionImpl . innerProduct is not working correctly$$0
Math-74$$Wrong parameter for first step size guess for Embedded Runge Kutta methods$$In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator. Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...) The problem comes from the array "scale" that is used as a parameter in the call off initializeStep(..) Following the theory described by Hairer in his book "Solving Ordinary Differential Equations 1 : Nonstiff Problems", the scaling should be : sci = Atol i + |y0i| * Rtoli Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation "sci = Atol i + |y0i| * Rtoli  " when he performs the call to the same method initializeStep(..) In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user. But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...) To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator For exemple :  final double[] scale= new double[y0.length];;           if (vecAbsoluteTolerance == null) {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;               }             } else {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;               }             }           hNew = initializeStep(equations, forward, getOrder(), scale,                            stepStart, y, yDotK[0], yTmp, yDotK[1]); Sorry for the length of this message, looking forward to hearing from you soon Vincent Morand$$patch1-Math-74-Arja$$Fix lost time value in AdamsMoultonIntegator . reinitialize ( ) .$$0
Math-6$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.      @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }$$patch1-Math-6-Arja$$Fix an issue with BaseOptimizer . getIterations ( )$$0
Math-6$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.      @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }$$patch2-Math-6-Arja$$Fix typo in BaseOptimizer . getIterations$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch8-Math-28-Arja$$added some debug code$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-Arja$$@@ add minRatioPositions . add ( i )$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch6-Math-28-Arja$$I had left it harwired to the minRatioSolver$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch7-Math-28-Arja$$Add incrementIterationsCounter ( ) to the minRow so it can be used for sorting$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch9-Math-28-Arja$$Remove a redundant check$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch11-Math-28-Arja$$removed some code that was accidentally left in the last commit$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch5-Math-28-Arja$$Fixed missing minRow return$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch2-Math-28-Arja$$Fixed a bug in SimplexSolver . java$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch10-Math-28-Arja$$removed a redundant check$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch3-Math-28-Arja$$Fix the bug in SimplexSolver . java$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch4-Math-28-Arja$$Improved the linear search code$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch1-Math-8-Arja$$throw exception if sampleSize <= 0$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch6-Math-8-Arja$$Remove sample from the DiscreteDistribution example .$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch7-Math-8-Arja$$Remove sample from DiscreteDistribution$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch5-Math-8-Arja$$Remove sample from DiscreteDistribution$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch2-Math-8-Arja$$Keep sample in looping over DiscreteDistribution$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch3-Math-8-Arja$$throw exception if sampleSize < = 0$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch4-Math-8-Arja$$throw exception if sampleSize < 0$$0
Math-86$$testing for symmetric positive definite matrix in CholeskyDecomposition$$I used this matrix:         double[][] cv = {  {0.40434286, 0.09376327, 0.30328980, 0.04909388} ,  {0.09376327, 0.10400408, 0.07137959, 0.04762857} ,  {0.30328980, 0.07137959, 0.30458776, 0.04882449},             {0.04909388, 0.04762857, 0.04882449, 0.07543265}         };  And it works fine, because it is symmetric positive definite  I tried this matrix:          double[][] cv = {             {0.40434286, -0.09376327, 0.30328980, 0.04909388},             {-0.09376327, 0.10400408, 0.07137959, 0.04762857},             {0.30328980, 0.07137959, 0.30458776, 0.04882449} ,             {0.04909388, 0.04762857, 0.04882449, 0.07543265}         }; And it should throw an exception but it does not.  I tested the matrix in R and R's cholesky decomposition method returns that the matrix is not symmetric positive definite. Obviously your code is not catching this appropriately. By the way (in my opinion) the use of exceptions to check these conditions is not the best design or use for exceptions.  If you are going to force the use to try and catch these exceptions at least provide methods  to test the conditions prior to the possibility of the exception.$$patch1-Math-86-Arja$$throw exception if abs / max is not greater than threshold$$0
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch1-Math-31-Arja$$Missing throw in ContinuedFraction .$$0
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch6-Math-31-Arja$$Missing throw in ContinuedFraction . log$$0
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch7-Math-31-Arja$$Missing throw in ContinuedFraction .$$0
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch5-Math-31-Arja$$Missing throw in ContinuedFraction .$$0
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch2-Math-31-Arja$$Missing throw in ContinuedFraction .$$0
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch3-Math-31-Arja$$Missing throw in ContinuedFraction .$$0
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch4-Math-31-Arja$$Missing throw in ContinuedFraction .$$0
Math-71$$ODE integrator goes past specified end of integration range$$End of integration range in ODE solving is handled as an event. In some cases, numerical accuracy in events detection leads to error in events location. The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.    public void testMissedEvent() throws IntegratorException, DerivativeException {           final double t0 = 1878250320.0000029;           final double t =  1878250379.9999986;           FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {                          public int getDimension() {                 return 1;             }                          public void computeDerivatives(double t, double[] y, double[] yDot)                 throws DerivativeException {                 yDot[0] = y[0] * 1.0e-6;             }         };          DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,                                                                                1.0e-10, 1.0e-10);          double[] y = { 1.0 };         integrator.setInitialStepSize(60.0);         double finalT = integrator.integrate(ode, t0, y, t, y);         Assert.assertEquals(t, finalT, 1.0e-6);     }$$patch1-Math-71-Arja$$Fix CombinedEventsManager . evaluateStep ( interpolator )$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-Arja$$Added missing state to OpenIntToDoubleHashMap .$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch6-Math-49-Arja$$Change the index sign for Map . put ( ) .$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch7-Math-49-Arja$$Change the index sign in OpenIntToDoubleHashMap .$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch5-Math-49-Arja$$Change the index sign in OpenIntToDoubleHashMap . java$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch2-Math-49-Arja$$added shouldGrowTable ( )$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch3-Math-49-Arja$$removed count count from OpenIntToDoubleHashMap$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch4-Math-49-Arja$$Change the index sign when FULL is true$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch8-Math-2-Arja$$Remove erroneous legacy support for AbstractIntegerDistribution$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-Arja$$Remove an unnecessary ceil$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch6-Math-2-Arja$$solve inverse cumulative probability in AbstractIntegerDistribution$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch7-Math-2-Arja$$throw exception if p < 0 || p > 1$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch5-Math-2-Arja$$If p == 1 . 0 then return the upper bound ; otherwise return the lower bound .$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch2-Math-2-Arja$$Remove an unnecessary ceil$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch3-Math-2-Arja$$Remove unnecessary ceil ( tmp ) in AbstractIntegerDistribution$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch4-Math-2-Arja$$Fix warning$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch1-Time-11-Arja$$Fixing duplicate recurrent name key in DateTimeZoneBuilder$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch6-Time-11-Arja$$Fix loop$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch7-Time-11-Arja$$Added missing debug message$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch5-Time-11-Arja$$added debug level out of zone info map$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch2-Time-11-Arja$$Fix NPE in DateTimeZoneBuilder$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch3-Time-11-Arja$$added debug output$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch4-Time-11-Arja$$Fixing duplicate recurrent name key .$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch1-Time-4-Arja$$Fixed zero - is - max datetime field$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch6-Time-4-Arja$$Fixed zero - is - maxDateTimeField minimization$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch7-Time-4-Arja$$Fixed the build .$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch5-Time-4-Arja$$Add an exception$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch2-Time-4-Arja$$Add an exception$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch3-Time-4-Arja$$Fix # 135$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch4-Time-4-Arja$$Fixed zero - is - max datetime field minimization$$0
Time-14$$Unable to add days to a MonthDay set to the ISO leap date$$It's not possible to add days to a MonthDay set to the ISO leap date (February 29th). This is even more bizarre given the exact error message thrown.$$patch1-Time-14-Arja$$MonthDay doesn ' t wrap month - day partials in MonthDay .$$0
Lang-61$$StrBuilder.replaceAll and StrBuilder.deleteAll can throw ArrayIndexOutOfBoundsException.$$StrBuilder.replaceAll and StrBuilder.deleteAll can thrown ArrayIndexOutOfBoundsException's. Here are a couple of additions to the StrBuilderTest class that demonstrate this problem: StrBuilder.deleteAll() - added to testDeleteAll_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.deleteAll("\n%BLAH%");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the following error: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.deleteImpl(StrBuilder.java:1114) 	at org.apache.commons.lang.text.StrBuilder.deleteAll(StrBuilder.java:1188) 	at org.apache.commons.lang.text.StrBuilderTest.testDeleteAll_String(StrBuilderTest.java:606) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) StrBuilder.replaceAll() - added to testReplaceAll_String_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.replaceAll("\n%BLAH%", "");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the exception: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.replaceImpl(StrBuilder.java:1256) 	at org.apache.commons.lang.text.StrBuilder.replaceAll(StrBuilder.java:1339) 	at org.apache.commons.lang.text.StrBuilderTest.testReplaceAll_String_String(StrBuilderTest.java:763) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)$$patch1-Lang-61-Arja$$Add ensureCapacity ( ) to the StrBuilder to prevent using the arrayCopy ( ) method .$$0
Lang-61$$StrBuilder.replaceAll and StrBuilder.deleteAll can throw ArrayIndexOutOfBoundsException.$$StrBuilder.replaceAll and StrBuilder.deleteAll can thrown ArrayIndexOutOfBoundsException's. Here are a couple of additions to the StrBuilderTest class that demonstrate this problem: StrBuilder.deleteAll() - added to testDeleteAll_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.deleteAll("\n%BLAH%");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the following error: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.deleteImpl(StrBuilder.java:1114) 	at org.apache.commons.lang.text.StrBuilder.deleteAll(StrBuilder.java:1188) 	at org.apache.commons.lang.text.StrBuilderTest.testDeleteAll_String(StrBuilderTest.java:606) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) StrBuilder.replaceAll() - added to testReplaceAll_String_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.replaceAll("\n%BLAH%", "");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the exception: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.replaceImpl(StrBuilder.java:1256) 	at org.apache.commons.lang.text.StrBuilder.replaceAll(StrBuilder.java:1339) 	at org.apache.commons.lang.text.StrBuilderTest.testReplaceAll_String_String(StrBuilderTest.java:763) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)$$patch2-Lang-61-Arja$$added ensureCapacity ( )$$0
Lang-61$$StrBuilder.replaceAll and StrBuilder.deleteAll can throw ArrayIndexOutOfBoundsException.$$StrBuilder.replaceAll and StrBuilder.deleteAll can thrown ArrayIndexOutOfBoundsException's. Here are a couple of additions to the StrBuilderTest class that demonstrate this problem: StrBuilder.deleteAll() - added to testDeleteAll_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.deleteAll("\n%BLAH%");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the following error: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.deleteImpl(StrBuilder.java:1114) 	at org.apache.commons.lang.text.StrBuilder.deleteAll(StrBuilder.java:1188) 	at org.apache.commons.lang.text.StrBuilderTest.testDeleteAll_String(StrBuilderTest.java:606) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) StrBuilder.replaceAll() - added to testReplaceAll_String_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.replaceAll("\n%BLAH%", "");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the exception: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.replaceImpl(StrBuilder.java:1256) 	at org.apache.commons.lang.text.StrBuilder.replaceAll(StrBuilder.java:1339) 	at org.apache.commons.lang.text.StrBuilderTest.testReplaceAll_String_String(StrBuilderTest.java:763) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)$$patch3-Lang-61-Arja$$expanded buffer to include str . length ( ) + 4 .$$0
Lang-61$$StrBuilder.replaceAll and StrBuilder.deleteAll can throw ArrayIndexOutOfBoundsException.$$StrBuilder.replaceAll and StrBuilder.deleteAll can thrown ArrayIndexOutOfBoundsException's. Here are a couple of additions to the StrBuilderTest class that demonstrate this problem: StrBuilder.deleteAll() - added to testDeleteAll_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.deleteAll("\n%BLAH%");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the following error: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.deleteImpl(StrBuilder.java:1114) 	at org.apache.commons.lang.text.StrBuilder.deleteAll(StrBuilder.java:1188) 	at org.apache.commons.lang.text.StrBuilderTest.testDeleteAll_String(StrBuilderTest.java:606) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) StrBuilder.replaceAll() - added to testReplaceAll_String_String():         sb = new StrBuilder("\n%BLAH%\nDo more stuff\neven more stuff\n%BLAH%\n");         sb.replaceAll("\n%BLAH%", "");         assertEquals("\nDo more stuff\neven more stuff\n", sb.toString()); this causes the exception: java.lang.ArrayIndexOutOfBoundsException 	at java.lang.System.arraycopy(Native Method) 	at org.apache.commons.lang.text.StrBuilder.replaceImpl(StrBuilder.java:1256) 	at org.apache.commons.lang.text.StrBuilder.replaceAll(StrBuilder.java:1339) 	at org.apache.commons.lang.text.StrBuilderTest.testReplaceAll_String_String(StrBuilderTest.java:763) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:585) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult$1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)$$patch4-Lang-61-Arja$$added ensureCapacity ( )$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch8-Lang-59-Arja$$StrBuilder ( ) uses char[] instead of char [ ] for the char array .$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-Arja$$StrBuilder . appendFixedWidthPadRight ( ) needs buffer .$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch6-Lang-59-Arja$$Add more ensureCapacity ( ) for string builder$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch7-Lang-59-Arja$$Add 4 more ensureCapacity .$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch9-Lang-59-Arja$$Add more ensureCapacity .$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch5-Lang-59-Arja$$StrBuilder should add 4 + 4 + 4 + 4 + 4 + 4 + 4 + 4 +$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch2-Lang-59-Arja$$StrBuilder should add 4 spaces for appendFixedWidthPadRight ( )$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch3-Lang-59-Arja$$StrBuilder ( ) uses char[] instead of char[] .$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch4-Lang-59-Arja$$StrBuilder . appendFixedWidthPadRight ( ) will reset the array$$0
Lang-50$$FastDateFormat getDateInstance() and getDateTimeInstance() assume Locale.getDefault() won't change$$The FastDateFormat getDateInstance() and getDateTimeInstance()  methods create the HashMap key from various items including the locale. If the locale is null, then it is not made part of the key, but the stored object is created using the current default locale. If the Locale is changed subsequently, then the wrong locale is applied. Patch for test case to follow.$$patch1-Lang-50-Arja$$Fix cDateTimeInstanceCache . put ( key , formatter ) .$$0
Lang-50$$FastDateFormat getDateInstance() and getDateTimeInstance() assume Locale.getDefault() won't change$$The FastDateFormat getDateInstance() and getDateTimeInstance()  methods create the HashMap key from various items including the locale. If the locale is null, then it is not made part of the key, but the stored object is created using the current default locale. If the Locale is changed subsequently, then the wrong locale is applied. Patch for test case to follow.$$patch6-Lang-50-Arja$$Allow null locale for date formats .$$0
Lang-50$$FastDateFormat getDateInstance() and getDateTimeInstance() assume Locale.getDefault() won't change$$The FastDateFormat getDateInstance() and getDateTimeInstance()  methods create the HashMap key from various items including the locale. If the locale is null, then it is not made part of the key, but the stored object is created using the current default locale. If the Locale is changed subsequently, then the wrong locale is applied. Patch for test case to follow.$$patch5-Lang-50-Arja$$Allow null locale for date formats .$$0
Lang-50$$FastDateFormat getDateInstance() and getDateTimeInstance() assume Locale.getDefault() won't change$$The FastDateFormat getDateInstance() and getDateTimeInstance()  methods create the HashMap key from various items including the locale. If the locale is null, then it is not made part of the key, but the stored object is created using the current default locale. If the Locale is changed subsequently, then the wrong locale is applied. Patch for test case to follow.$$patch2-Lang-50-Arja$$FastDateFormat should not cache null pattern for locale$$0
Lang-50$$FastDateFormat getDateInstance() and getDateTimeInstance() assume Locale.getDefault() won't change$$The FastDateFormat getDateInstance() and getDateTimeInstance()  methods create the HashMap key from various items including the locale. If the locale is null, then it is not made part of the key, but the stored object is created using the current default locale. If the Locale is changed subsequently, then the wrong locale is applied. Patch for test case to follow.$$patch3-Lang-50-Arja$$Fix cached date format instance .$$0
Lang-50$$FastDateFormat getDateInstance() and getDateTimeInstance() assume Locale.getDefault() won't change$$The FastDateFormat getDateInstance() and getDateTimeInstance()  methods create the HashMap key from various items including the locale. If the locale is null, then it is not made part of the key, but the stored object is created using the current default locale. If the Locale is changed subsequently, then the wrong locale is applied. Patch for test case to follow.$$patch4-Lang-50-Arja$$Prevent null pattern exception from being cached .$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-Arja$$Handle case when String is empty or null$$0
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch2-Lang-51-Arja$$Handle case when String is empty or null$$0
Lang-60$$StrBuilder contains usages of thisBuf.length when they should use size$$While fixing LANG-294 I noticed that there are two other places in StrBuilder that reference thisBuf.length and unless I'm mistaken they shouldn't.$$patch1-Lang-60-Arja$$StrBuilder . contains ( ) uses the array copy constructor$$0
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch1-Lang-20-Arja$$Fix cruise ( string is not UTF - 8 )$$0
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch2-Lang-20-Arja$$Fix cruise ( array ) and cruise ( array )$$0
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch3-Lang-20-Arja$$Updated StringUtils to use StringBuilder . append ( separator ) instead of StringBuilder . append ( 1 )$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1-Lang-7-Arja$$removed extra chars$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch6-Lang-7-Arja$$Improve error handling for ""-- "" and "" ( areas with the same length ) - removed 0x$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch7-Lang-7-Arja$$Handle hex number format in NumberUtils$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch5-Lang-7-Arja$$formatting fixes$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch2-Lang-7-Arja$$Remove a check for '-- '$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch3-Lang-7-Arja$$Fixed erroneous parseShort ( String )$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch4-Lang-7-Arja$$Allow null string input$$0
Lang-16$$NumberUtils does not handle upper-case hex: 0X and -0X$$NumberUtils.createNumber() should work equally for 0x1234 and 0X1234; currently 0X1234 generates a NumberFormatException Integer.decode() handles both upper and lower case hex.$$patch1-Lang-16-Arja$$Fixed NO_ISSUE in NumberUtils . createInteger ( ) .$$0
Lang-16$$NumberUtils does not handle upper-case hex: 0X and -0X$$NumberUtils.createNumber() should work equally for 0x1234 and 0X1234; currently 0X1234 generates a NumberFormatException Integer.decode() handles both upper and lower case hex.$$patch2-Lang-16-Arja$$Fixed erroneous error in NumberUtils$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-Arja$$don ' t append QUOTE to the appendTo var$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch2-Lang-43-Arja$$don ' t append QUOTE if it is backslashed$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch3-Lang-43-Arja$$don ' t skip backslash$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch4-Lang-43-Arja$$Fix lost backslash$$0
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-Arja$$start = textIndex + searchList [ replaceIndex ] . length ( )$$0
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch1-Lang-55-Arja$$Added missing return statement$$0
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch5-Lang-55-Arja$$Add some error code$$0
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch2-Lang-55-Arja$$Add some error code$$0
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch3-Lang-55-Arja$$Remove unnecessary assignment$$0
Lang-55$$StopWatch: suspend() acts as split(), if followed by stop()$$In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:         StopWatch sw = new StopWatch();         sw.start();         Thread.sleep(1000);         sw.suspend();         // Time 1 (ok)         System.out.println(sw.getTime());         Thread.sleep(2000);         // Time 1 (again, ok)         System.out.println(sw.getTime());         sw.resume();         Thread.sleep(3000);         sw.suspend();         // Time 2 (ok)         System.out.println(sw.getTime());         Thread.sleep(4000);         // Time 2 (again, ok)         System.out.println(sw.getTime());         Thread.sleep(5000);         sw.stop();         // Time 2 (should be, but is Time 3 => NOT ok)         System.out.println(sw.getTime()); suspend/resume is like a pause, where time counter doesn't continue. So a following stop()-call shouldn't increase the time counter, should it?$$patch4-Lang-55-Arja$$Fix splitState after stopTime$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch1-Lang-63-Arja$$Fixed bug for calendar systems .$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch5-Lang-63-Arja$$Fixed formatting mistake$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch2-Lang-63-Arja$$Fix bug in DurationFormatUtils$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch3-Lang-63-Arja$$Remove over - aggressive add in DurationFormatUtils$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch4-Lang-63-Arja$$Fixed the merge bug in DurationFormatUtils$$0
Lang-41$$ClassUtils.getShortClassName() will not work with an array;  it seems to add a semicolon to the end.$$A semicolon is introduced into the class name at the end for all arrays... String sArray[] = new String[2]; sArray[0] = "mark"; sArray[1] = "is cool"; String simpleString = "chris"; assertEquals("String", ClassUtils.getShortClassName(simpleString, null)); assertEquals("String;", ClassUtils.getShortClassName(sArray, null));$$patch1-Lang-41-Arja$$Fix ClassUtils . getShortClassName / getShortCanonicalName$$0
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch1-Lang-22-Arja$$Fix greatestCommonDivisor from Lee Butts$$0
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch2-Lang-22-Arja$$Fix Fraction . greatestCommonDivisor ( )$$0
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch3-Lang-22-Arja$$Fix Fraction . greatestCommonDivisor ( )$$0
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch4-Lang-22-Arja$$Fix Fraction . greatestCommonDivisor ( )$$0
Closure-92$$bug with implicit namespaces across modules$$None$$patch1-Closure-92-ConFix$$Fix the build .$$1
Closure-93$$None$$None$$patch1-Closure-93-ConFix$$Fix the build .$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-ConFix$$Fix typo in codeGenerator . java$$1
Closure-38$$Identifier minus a negative number needs a space between the "-"s$$None$$patch1-Closure-38-ConFix$$Fix whitespace in code consumer$$1
Closure-109$$Constructor types that return all or unknown fail to parse$$None$$patch1-Closure-109-ConFix$$Fix whitespace in closure109 .$$1
Closure-14$$bogus 'missing return' warning$$None$$patch1-Closure-14-ConFix$$Fix control flow analysis$$1
Chart-11$$JCommon 1.0.12 ShapeUtilities.equal(path1,path2)$$The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.$$patch1-Chart-11-ConFix$$Fix bug in chart11$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-ConFix$$Fix null pointer check in AbstractCategoryItemRenderer$$1
Chart-10$$None$$None$$patch1-Chart-10-ConFix$$Fix ToolTipGenerator . generateToolTipFragment ( )$$1
Chart-24$$None$$None$$patch1-Chart-24-ConFix$$Fix build$$1
Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.$$patch1-Math-34-ConFix$$added iterator on non - empty chromosomes$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-ConFix$$reduce imports$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-ConFix$$Fix typo in Frequency . getPct$$1
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles$$patch1-Math-30-ConFix$$Fix warning$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-ConFix$$fixed missing import$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-ConFix$$fix a bug in Complex . java$$1
Time-19$$Inconsistent interpretation of ambiguous time during DST$$The inconsistency appears for timezone Europe/London.  These three DateTime objects should all represent the same moment in time even if they are ambiguous. Now, it always returns the earlier instant (summer time) during an overlap.$$patch1-Time-19-ConFix$$fixed typo .$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-ConFix$$Missing comment$$1
Lang-51$$BooleanUtils.toBoolean() - invalid drop-thru in case statement causes StringIndexOutOfBoundsException$$The method BooleanUtils.toBoolean() has a case statement; case 3 drops through to case 4; this can cause StringIndexOutOfBoundsException, for example with the test: assertEquals(false, BooleanUtils.toBoolean("tru")); The end of case 3 should return false. Patch to follow for source and unit test.$$patch1-Lang-51-ConFix$$missing break$$1
Lang-6$$StringIndexOutOfBoundsException in CharSequenceTranslator$$I found that there is bad surrogate pair handling in the CharSequenceTranslator This is a simple test case for this problem. \uD83D\uDE30 is a surrogate pair.  @Test public void testEscapeSurrogatePairs() throws Exception {     assertEquals("\uD83D\uDE30", StringEscapeUtils.escapeCsv("\uD83D\uDE30")); }   You'll get the exception as shown below.  java.lang.StringIndexOutOfBoundsException: String index out of range: 2 	at java.lang.String.charAt(String.java:658) 	at java.lang.Character.codePointAt(Character.java:4668) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:95) 	at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:59) 	at org.apache.commons.lang3.StringEscapeUtils.escapeCsv(StringEscapeUtils.java:556)   Patch attached, the method affected:  public final void translate(CharSequence input, Writer out) throws IOException$$patch1-Lang-6-ConFix$$Fix unused import .$$1
Lang-26$$FastDateFormat.format() outputs incorrect week of year because locale isn't respected$$FastDateFormat apparently doesn't respect the locale it was sent on creation when outputting week in year (e.g. "ww") in format(). It seems to use the settings of the system locale for firstDayOfWeek and minimalDaysInFirstWeek, which (depending on the year) may result in the incorrect week number being output. Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:  import java.util.Calendar; import java.util.Date; import java.util.Locale; import java.text.SimpleDateFormat;  import org.apache.commons.lang.time.FastDateFormat;  public class FastDateFormatWeekBugDemo {     public static void main(String[] args) {         Locale.setDefault(new Locale("en", "US"));         Locale locale = new Locale("sv", "SE");          Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome         cal.set(2010, 0, 1, 12, 0, 0);         Date d = cal.getTime();         System.out.println("Target date: " + d);          FastDateFormat fdf = FastDateFormat.getInstance("EEEE', week 'ww", locale);         SimpleDateFormat sdf = new SimpleDateFormat("EEEE', week 'ww", locale);         System.out.println("FastDateFormat:   " + fdf.format(d)); // will output "FastDateFormat:   fredag, week 01"         System.out.println("SimpleDateFormat: " + sdf.format(d)); // will output "SimpleDateFormat: fredag, week 53"     } }   If sv/SE is passed to Locale.setDefault() instead of en/US, both FastDateFormat and SimpleDateFormat output the correct week number.$$patch1-Lang-26-ConFix$$Fix bug in FastDateFormat$$1
Lang-24$$NumberUtils.isNumber(String)  is not right when the String is "1.1L"$$"1.1L"  is not a Java Number . but NumberUtils.isNumber(String) return true. perhaps change:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp;             }   to:              if (chars[i] == 'l'                 || chars[i] == 'L') {                 // not allowing L with an exponent                 return foundDigit && !hasExp && !hasDecPoint;             }$$patch1-Lang-24-ConFix$$allow L with a decimal point$$1
Closure-59$$Cannot exclude globalThis checks through command line$$None$$patch1-Closure-59-ConFix$$Fix checkSuspiciousCode$$0
Closure-133$$Exception when parsing erroneous jsdoc: /**@return {@code foo} bar   *    baz. */$$None$$patch1-Closure-133-ConFix$$Allow non - type annotations .$$0
Closure-89$$Compiler removes function properties that it should not$$None$$patch1-Closure-89-ConFix$$Fix CollapseProperties report$$0
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-21-ConFix$$Fix check side effects for calls to the closure context$$0
Closure-119$$catch(e) yields JSC_UNDEFINED_NAME warning when e is used in catch in advanced mode$$None$$patch1-Closure-119-ConFix$$Add some fix .$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-ConFix$$Fix try / catch blocks$$0
Closure-55$$Exception when emitting code containing getters$$None$$patch1-Closure-55-ConFix$$Fix maybeGetSingleReturnRValue ( )$$0
Closure-108$$precondition crash: goog.scope local with aliased in the type declaration$$None$$patch1-Closure-108-ConFix$$Fix check for same name in ScopedAliases$$0
Closure-90$$@this emits warning when used with a typedef$$None$$patch1-Closure-90-ConFix$$Added check for enum initializers$$0
Closure-46$$ClassCastException during TypeCheck pass$$None$$patch1-Closure-46-ConFix$$Fix record type resolve bug$$0
Closure-79$$RuntimeException when compiling with extern prototype$$None$$patch1-Closure-79-ConFix$$Fix findbugs issue$$0
Closure-83$$Cannot see version with --version$$None$$patch1-Closure-83-ConFix$$Fix checkstyle issues$$0
Closure-125$$IllegalStateException at com.google.javascript.rhino.jstype.FunctionType.getInstanceType$$None$$patch1-Closure-125-ConFix$$Fix TypeCheck . java$$0
Closure-2$$combining @interface and multiple @extends can crash compiler$$None$$patch1-Closure-2-ConFix$$Fix TypeCheck . checkForExtraProperties$$0
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-22-ConFix$$Allow non - expr results in closure closure check .$$0
Chart-9$$Error on TimeSeries createCopy() method$$The test case at the end fails with :  java.lang.IllegalArgumentException: Requires start <= end.  The problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7$$patch1-Chart-9-ConFix$$Fix bug in TimeSeries indexing$$0
Chart-7$$None$$None$$patch1-Chart-7-ConFix$$Fix bug in TimePeriodValues . getMaxMiddleIndex$$0
Chart-26$$None$$None$$patch1-Chart-26-ConFix$$Fix bug in CategoryPlot$$0
Chart-15$$None$$None$$patch1-Chart-15-ConFix$$Fix null info object$$0
Chart-3$$None$$None$$patch1-Chart-3-ConFix$$fix copy bug$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-ConFix$$Fix bug in AbstractDataset . hasListener$$0
Chart-13$$None$$None$$patch1-Chart-13-ConFix$$Fix border embarrassment in chart$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-ConFix$$AddOrUpdate ( ) now works fine$$0
Chart-25$$None$$None$$patch1-Chart-25-ConFix$$Remove unused line$$0
Math-61$$Dangerous code in "PoissonDistributionImpl"$$In the following excerpt from class "PoissonDistributionImpl": PoissonDistributionImpl.java     public PoissonDistributionImpl(double p, NormalDistribution z) {         super();         setNormal(z);         setMean(p);     }   (1) Overridable methods are called within the constructor. (2) The reference "z" is stored and modified within the class. I've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the "setter" methods inside the constructor but I think that a more elegant solution would be to remove the "setters" altogether (i.e. make the classes immutable). Problem (2) can also create unexpected behaviour. Is it really necessary to pass the "NormalDistribution" object; can't it be always created within the class?$$patch1-Math-61-ConFix$$Fixed a bug in PoissonDistributionImpl .$$0
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1-Math-95-ConFix$$fix bug in double precision functions$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-ConFix$$Fix bug in renegation of max abs ( x ) .$$0
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-ConFix$$Fix typo in KMeansPlusPlusClusterer . java$$0
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?$$patch1-Math-32-ConFix$$Fix bug in AbstractRegion$$0
Math-56$$MultidimensionalCounter.getCounts(int) returns wrong array of indices$$MultidimensionalCounter counter = new MultidimensionalCounter(2, 4); for (Integer i : counter) {     int[] x = counter.getCounts;     System.out.println(i + " " + Arrays.toString); } Output is: 0 [0, 0] 1 [0, 1] 2 [0, 2] 3 [0, 2]   <=== should be [0, 3] 4 [1, 0] 5 [1, 1] 6 [1, 2] 7 [1, 2]   <=== should be [1, 3]$$patch1-Math-56-ConFix$$Fixed bug in multidimensionalCounter$$0
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.$$patch1-Math-58-ConFix$$Fix regression in GaussianFitter .$$0
Math-94$$MathUtils.gcd(u, v) fails when u and v both contain a high power of 2$$The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.         assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15))); Fix: Replace the test at the start of MathUtils.gcd()         if (u * v == 0) { by         if (u == 0 || v == 0) {$$patch1-Math-94-ConFix$$Fix gcd with System . currentTimeMillis ( )$$0
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.$$patch1-Math-20-ConFix$$cmAESOptimizer now uses the max iterations$$0
Math-18$$CMAESOptimizer with bounds fits finely near lower bound and coarsely near upper bound.$$When fitting with bounds, the CMAESOptimizer fits finely near the lower bound and coarsely near the upper bound.  This is because it internally maps the fitted parameter range into the interval [0,1].  The unit of least precision (ulp) between floating point numbers is much smaller near zero than near one.  Thus, fits have much better resolution near the lower bound (which is mapped to zero) than the upper bound (which is mapped to one).  I will attach a example program to demonstrate.$$patch1-Math-18-ConFix$$Fix early warning$$0
Math-7$$event state not updated if an unrelated event triggers a RESET_STATE during ODE integration$$When an ODE solver manages several different event types, there are some unwanted side effects. If one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return. As a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20. The attached file reproduces the problem.$$patch1-Math-7-ConFix$$Add configuration line$$0
Math-29$$Bugs in RealVector.ebeMultiply(RealVector) and ebeDivide(RealVector)$$OpenMapRealVector.ebeMultiply(RealVector) and OpenMapRealVector.ebeDivide(RealVector) return wrong values when one entry of the specified RealVector is nan or infinity. The bug is easy to understand. Here is the current implementation of ebeMultiply      public OpenMapRealVector ebeMultiply(RealVector v) {         checkVectorDimensions(v.getDimension());         OpenMapRealVector res = new OpenMapRealVector(this);         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             res.setEntry(iter.key(), iter.value() * v.getEntry(iter.key()));         }         return res;     }   The assumption is that for any double x, x * 0d == 0d holds, which is not true. The bug is easy enough to identify, but more complex to solve. The only solution I can come up with is to loop through all entries of v (instead of those entries which correspond to non-zero entries of this). I'm afraid about performance losses.$$patch1-Math-29-ConFix$$Fix bug in RealVector interface$$0
Math-42$$Negative value with restrictNonNegative$$Problem: commons-math-2.2 SimplexSolver. A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call: SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true); Function 1 * x + 1 * y + 0 Constraints: 1 * x + 0 * y = 1 Result: x = 1; y = -1; Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.$$patch1-Math-42-ConFix$$Fix a bug in the similarity function$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-ConFix$$Fixed bug in EigenDecompositionImpl .$$0
Math-74$$Wrong parameter for first step size guess for Embedded Runge Kutta methods$$In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator. Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...) The problem comes from the array "scale" that is used as a parameter in the call off initializeStep(..) Following the theory described by Hairer in his book "Solving Ordinary Differential Equations 1 : Nonstiff Problems", the scaling should be : sci = Atol i + |y0i| * Rtoli Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation "sci = Atol i + |y0i| * Rtoli  " when he performs the call to the same method initializeStep(..) In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user. But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...) To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator For exemple :  final double[] scale= new double[y0.length];;           if (vecAbsoluteTolerance == null) {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;               }             } else {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;               }             }           hNew = initializeStep(equations, forward, getOrder(), scale,                            stepStart, y, yDotK[0], yTmp, yDotK[1]); Sorry for the length of this message, looking forward to hearing from you soon Vincent Morand$$patch1-Math-74-ConFix$$Fix missing import$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-ConFix$$Fix minRatioPositions for TFJ - 236$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch1-Math-8-ConFix$$Fix bug in DiscreteDistribution . java$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-ConFix$$added np - 6 - 2 back into work$$0
Math-44$$Incomplete reinitialization with some events handling$$I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way. I can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.$$patch1-Math-44-ConFix$$Restore accidentally removed import$$0
Math-88$$Simplex Solver arrives at incorrect solution$$I have reduced the problem reported to me down to a minimal test case which I will attach.$$patch1-Math-88-ConFix$$Fix warning$$0
Math-62$$Miscellaneous issues concerning the "optimization" package$$Revision 990792 contains changes triggered the following issues:  MATH-394 MATH-397 MATH-404  This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):  "BrentOptimizer": a specific convergence checker must be used. "LevenbergMarquardtOptimizer" also has specific convergence checks. Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical): 	 See "BrentOptimizer" and "LevenbergMarquardtOptimizer", the algorithm passes "points" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker). In "PowellOptimizer" the line search ("BrentOptimizer") tolerances depend on the tolerances within the main algorithm. Since tolerances come with "ConvergenceChecker" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals ("BrentOptimizer" field) to the enclosing class ("PowellOptimizer").   Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all. Class "DirectSearchOptimizer" (in package "optimization.direct") inherits from class "AbstractScalarOptimizer" (in package "optimization.general"). Some interfaces are defined in package "optimization" but their base implementations (abstract class that contain the boiler-plate code) are in package "optimization.general" (e.g. "DifferentiableMultivariateVectorialOptimizer" and "BaseAbstractVectorialOptimizer"). No check is performed to ensure the the convergence checker has been set (see e.g. "BrentOptimizer" and "PowellOptimizer"); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker. "NonLinearConjugateGradientOptimizer": Ugly workaround for the checked "ConvergenceException". Everywhere, we trail the checked "FunctionEvaluationException" although it is never used. There remains some duplicate code (such as the "multi-start loop" in the various "MultiStart..." implementations). The "ConvergenceChecker" interface is very general (the "converged" method can take any number of "...PointValuePair"). However there remains a "semantic" problem: One cannot be sure that the list of points means the same thing for the caller of "converged" and within the implementation of the "ConvergenceChecker" that was independently set. It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In "LevenbergMarquartdOptimizer" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations. In "AbstractLeastSquaresOptimizer" and "LevenbergMarquardtOptimizer", occurences of "OptimizationException" were replaced by the unchecked "ConvergenceException" but in some cases it might not be the most appropriate one. "MultiStartUnivariateRealOptimizer": in the other classes ("MultiStartMultivariate...") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval). The Javadoc utility raises warnings (see output of "mvn site") which I couldn't figure out how to correct. Some previously existing classes and interfaces have become no more than a specialisation of new "generics" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.$$patch1-Math-62-ConFix$$Fix warning$$0
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-ConFix$$Fix bug in MathUtils . equals$$0
Math-79$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest { 	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ), 				new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size()); 	} }$$patch1-Math-79-ConFix$$Fix LOGICAL_REVID = 100881$$0
Math-84$$MultiDirectional optimzation loops forver if started at the correct solution$$MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.$$patch1-Math-84-ConFix$$Fix typo in MultiDirectional .$$0
Math-3$$ArrayIndexOutOfBoundsException in MathArrays.linearCombination$$When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line: double prodHighNext = prodHigh[1]; linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.$$patch1-Math-3-ConFix$$Fix MathArrays . prodHigh to work with bit 64 .$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-ConFix$$Fix a warning$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-ConFix$$Added missing closing parenthesis in tableau .$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-ConFix$$Fix error in OpenMapRealVector put method .$$0
Math-40$$BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary$$In some cases, the aging feature in BracketingNthOrderBrentSolver fails. It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket. In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).$$patch1-Math-40-ConFix$$Fix swapped sign exchange error in BracketingNthOrderBrentSolver . java$$0
Math-78$$during ODE integration, the last event in a pair of very close event may not be detected$$When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let's say this step spans from 90.0 to 153.0. The switching function switches once again in this step. If the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative. This bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.$$patch1-Math-78-ConFix$$Fix check interval$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-ConFix$$Fix double precision thingie in AbstractIntegerDistribution$$0
Time-9$$Ensure there is a max/min valid offset$$DateTimeZone does not apply a max/min value for an offset. However the parse method is limited to 23:59. Make 23:59:59.999 the maximum.$$patch1-Time-9-ConFix$$Fixed missing import .$$0
Time-11$$NPE in DateTimeZoneBuilder$$When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create(). When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().  The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler ...will initialize cVerbose only for the first thread and not for the subsequent ones. The NPE is caused by the autoboxing in: . A better approach could be to remove the initialization and test for null:$$patch1-Time-11-ConFix$$Fix bug in tz archive$$0
Time-7$$DateTimeFormat.parseInto sometimes miscalculates year (2.2)$$The following code (which can be added to org.joda.time.format.TestDateTimeFormatter) breaks, because the input mutable date time's millis appear to be mishandled and the year for the parse is changed to 1999.$$patch1-Time-7-ConFix$$Add withUTC suffix .$$0
Time-17$$Bug on withLaterOffsetAtOverlap method$$On the last two brackets we can see that withLaterOffsetAtOverlap is not undoing withEarlierOffsetAtOverlap as it should ( and not even working at all ).$$patch1-Time-17-ConFix$$Fix bug in time zone ISO 8601 format$$0
Time-4$$Constructing invalid Partials$$Partials can be constructed by invoking a constructor Partial(DateTimeFieldType[], int[]) or by merging together a set of partials using with, each constructed by calling Partial(DateTimeFieldType, int). However, the above doesn't work in all cases: I suppose the Partials should not allow to be constructed in either case. Is that right?  There's also a related issue (probably stems from the fact that the Partial is invalid):$$patch1-Time-4-ConFix$$Fixing the build .$$0
Lang-59$$Bug in method appendFixedWidthPadRight of class StrBuilder causes an ArrayIndexOutOfBoundsException$$There's a bug in method appendFixedWidthPadRight of class StrBuilder: public StrBuilder appendFixedWidthPadRight(Object obj, int width, char padChar) {         if (width > 0) {             ensureCapacity(size + width);             String str = (obj == null ? getNullText() : obj.toString());             int strLen = str.length();             if (strLen >= width)  {  ==>            str.getChars(0, strLen, buffer, size);   <==== BUG: it should be str.getChars(0, width, buffer, size);             }  else {                 int padLen = width - strLen;                 str.getChars(0, strLen, buffer, size);                 for (int i = 0; i < padLen; i++)  {                     buffer[size + strLen + i] = padChar;                 }             }             size += width;         }         return this;     } This is causing an ArrayIndexOutOfBoundsException, so this method is unusable when strLen > width. It's counterpart method appendFixedWidthPadLeft seems to be ok.$$patch1-Lang-59-ConFix$$add missing backslash$$0
Lang-60$$StrBuilder contains usages of thisBuf.length when they should use size$$While fixing LANG-294 I noticed that there are two other places in StrBuilder that reference thisBuf.length and unless I'm mistaken they shouldn't.$$patch1-Lang-60-ConFix$$Added missing 1111$$0
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-ConFix$$Fix typo in first line of text file$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1-Lang-7-ConFix$$Fix checkstyle$$0
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-ConFix$$reduce TFJ - 619 search path$$0
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-ConFix$$Fix cruise ( IP address format )$$0
Lang-31$$StringUtils methods do not handle Unicode 2.0+ supplementary characters correctly.$$StringUtils.containsAny methods incorrectly matches Unicode 2.0+ supplementary characters. For example, define a test fixture to be the Unicode character U+20000 where U+20000 is written in Java source as "\uD840\uDC00" 	private static final String CharU20000 = "\uD840\uDC00"; 	private static final String CharU20001 = "\uD840\uDC01"; You can see Unicode supplementary characters correctly implemented in the JRE call: 	assertEquals(-1, CharU20000.indexOf(CharU20001)); But this is broken: 	assertEquals(false, StringUtils.containsAny(CharU20000, CharU20001)); 	assertEquals(false, StringUtils.containsAny(CharU20001, CharU20000)); This is fine: 	assertEquals(true, StringUtils.contains(CharU20000 + CharU20001, CharU20000)); 	assertEquals(true, StringUtils.contains(CharU20000 + CharU20001, CharU20001)); 	assertEquals(true, StringUtils.contains(CharU20000, CharU20000)); 	assertEquals(false, StringUtils.contains(CharU20000, CharU20001)); because the method calls the JRE to perform the match. More than you want to know:  http://java.sun.com/developer/technicalArticles/Intl/Supplementary/$$patch1-Lang-31-ConFix$$Fix bug in StringUtils$$0
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-ConFix$$Fix typo in StringUtils$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch1-Lang-63-ConFix$$Fix bug in duration format utils$$0
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch1-Lang-22-ConFix$$Fix greatestCommonDivisor result$$0
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-21-jKali$$Fix bug in closure patch$$1
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-jKali$$try minimize exits on finally blocks$$1
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-22-jKali$$Updated patch$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-jKali$$Fix a bug in the patch .$$1
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1-Lang-7-jKali$$Fix 0x mistake in patch$$1
Lang-10$$FastDateParser does not handle white-space properly$$The SimpleDateFormat Javadoc does not treat white-space specially, however FastDateParser treats a single white-space as being any number of white-space characters. This means that FDP will parse dates that fail when parsed by SDP.$$patch1-Lang-10-jKali$$don ' t backslash backslash in regex so it won ' t be backslashed$$1
Closure-45$$Assignment removed when used as an expression result to Array.push$$None$$patch1-Closure-45-jKali$$fix bug$$0
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-jKali$$don ' t add extra space to the end of a line$$0
Closure-101$$--process_closure_primitives can't be set to false$$None$$patch1-Closure-101-jKali$$closure fix bug$$0
Closure-63$$None$$None$$patch1-Closure-63-jKali$$don ' t add extra space to the end of a line$$0
Closure-46$$ClassCastException during TypeCheck pass$$None$$patch1-Closure-46-jKali$$Fix closure patching$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-jKali$$Allow null dataset in chart$$0
Chart-13$$None$$None$$patch1-Chart-13-jKali$$Fixed issue with left over chart$$0
Chart-5$$XYSeries.addOrUpdate() should add if duplicates are allowed$$I've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data.$$patch1-Chart-5-jKali$$Fix bug in chart$$0
Chart-25$$None$$None$$patch1-Chart-25-jKali$$Fixed bug in chart patch$$0
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?$$patch1-Math-32-jKali$$Fixed issue with BSP tree size .$$0
Math-29$$Bugs in RealVector.ebeMultiply(RealVector) and ebeDivide(RealVector)$$OpenMapRealVector.ebeMultiply(RealVector) and OpenMapRealVector.ebeDivide(RealVector) return wrong values when one entry of the specified RealVector is nan or infinity. The bug is easy to understand. Here is the current implementation of ebeMultiply      public OpenMapRealVector ebeMultiply(RealVector v) {         checkVectorDimensions(v.getDimension());         OpenMapRealVector res = new OpenMapRealVector(this);         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             res.setEntry(iter.key(), iter.value() * v.getEntry(iter.key()));         }         return res;     }   The assumption is that for any double x, x * 0d == 0d holds, which is not true. The bug is easy enough to identify, but more complex to solve. The only solution I can come up with is to loop through all entries of v (instead of those entries which correspond to non-zero entries of this). I'm afraid about performance losses.$$patch1-Math-29-jKali$$Fix buggy$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-jKali$$fix a bug in qd array not shown in the same time frame$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-jKali$$added fixed patch$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch1-Math-8-jKali$$Remove sample from patch collection$$0
Math-71$$ODE integrator goes past specified end of integration range$$End of integration range in ODE solving is handled as an event. In some cases, numerical accuracy in events detection leads to error in events location. The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.    public void testMissedEvent() throws IntegratorException, DerivativeException {           final double t0 = 1878250320.0000029;           final double t =  1878250379.9999986;           FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {                          public int getDimension() {                 return 1;             }                          public void computeDerivatives(double t, double[] y, double[] yDot)                 throws DerivativeException {                 yDot[0] = y[0] * 1.0e-6;             }         };          DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,                                                                                1.0e-10, 1.0e-10);          double[] y = { 1.0 };         integrator.setInitialStepSize(60.0);         double finalT = integrator.integrate(ode, t0, y, t, y);         Assert.assertEquals(t, finalT, 1.0e-6);     }$$patch1-Math-71-jKali$$Fixed a constructor of ConvergingAlgorithmImpl$$0
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)$$patch1-Math-49-jKali$$Fixed a bug in the patch .$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-jKali$$fixed in patch 1 . 0 . 2 - jKali .$$0
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-jKali$$removed expPos from mant patch$$0
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch1-Lang-22-jKali$$Fix false alarm in JKali patch$$0
Closure-57$$compiler crashes when  goog.provide used with non string$$None$$patch1-Closure-57-SimFix$$Fixing closure code conventions$$1
Closure-73$$Codepoint U+007f appears raw in output$$None$$patch1-Closure-73-SimFix$$Fix typo in codeGenerator where ' c ' was passed through , but was accidentally ignored .$$1
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-SimFix$$Remove whitespaces from sourceExcerpt .$$1
Closure-63$$None$$None$$patch1-Closure-63-SimFix$$Remove whitespaces from sourceExcerpt$$1
Closure-115$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1-Closure-115-SimFix$$Allow null arguments for static closures .$$1
Closure-14$$bogus 'missing return' warning$$None$$patch1-Closure-14-SimFix$$Fix finally map .$$1
Chart-20$$None$$None$$patch1-Chart-20-SimFix$$Fix ValueMarker constructor to ignore the outline paint for now .$$1
Chart-7$$None$$None$$patch1-Chart-7-SimFix$$Fixed formatting mistake$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-SimFix$$Fix nullability note in AbstractCategoryItemRenderer$$1
Chart-3$$None$$None$$patch1-Chart-3-SimFix$$Find bounds by iteration$$1
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.$$patch1-Math-59-SimFix$$Fix NaN in FastMath . max ( a , b )$$1
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-SimFix$$Fix renegation of baseSecantSolver$$1
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-SimFix$$Fix int overflow$$1
Math-35$$Need range checks for elitismRate in ElitisticListPopulation constructors.$$There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.$$patch1-Math-35-SimFix$$ElitisticListPopulation constructor should throw an error if the population size was too large$$1
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.$$patch1-Math-33-SimFix$$reduce error in SimplexTableau$$1
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }$$patch1-Math-75-SimFix$$Fix getFrequency ( Object ) to return precise value instead of getCumPct ( Object )$$1
Math-98$$RealMatrixImpl#operate gets result vector dimensions wrong$$org.apache.commons.math.linear.RealMatrixImpl#operate tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix. Thus line 640 in RealMatrixImpl.java should read double[] out = new double[nRows]; instead of double[] out = new double[v.length];$$patch1-Math-98-SimFix$$Fix BigMatrixImpl to work with dense matrix impl .$$1
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).$$patch1-Math-53-SimFix$$Add the isNaN check in Complex . add ( )$$1
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.$$patch1-Math-63-SimFix$$Fix MathUtils . equals ( double , double , int )$$1
Math-79$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest { 	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ), 				new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size()); 	} }$$patch1-Math-79-SimFix$$removed int because it wasn ' t working as expected$$1
Math-41$$One of Variance.evaluate() methods does not work correctly$$The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset. Similar method in Mean class seems to work. I did not check other methods taking the part of the array; they may have the same problem. Workaround: I had to shrink my arrays and use the method without the length.$$patch1-Math-41-SimFix$$Fix the for loop$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-SimFix$$fixed a typo in BisectionSolver$$1
Math-71$$ODE integrator goes past specified end of integration range$$End of integration range in ODE solving is handled as an event. In some cases, numerical accuracy in events detection leads to error in events location. The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.    public void testMissedEvent() throws IntegratorException, DerivativeException {           final double t0 = 1878250320.0000029;           final double t =  1878250379.9999986;           FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {                          public int getDimension() {                 return 1;             }                          public void computeDerivatives(double t, double[] y, double[] yDot)                 throws DerivativeException {                 yDot[0] = y[0] * 1.0e-6;             }         };          DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,                                                                                1.0e-10, 1.0e-10);          double[] y = { 1.0 };         integrator.setInitialStepSize(60.0);         double finalT = integrator.integrate(ode, t0, y, t, y);         Assert.assertEquals(t, finalT, 1.0e-6);     }$$patch1-Math-71-SimFix$$Fix an issue with the nonstiff algorithm ' s value .$$1
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $$$patch1-Math-5-SimFix$$Fix NaN - > INF in Complex$$1
Time-7$$DateTimeFormat.parseInto sometimes miscalculates year (2.2)$$The following code (which can be added to org.joda.time.format.TestDateTimeFormatter) breaks, because the input mutable date time's millis appear to be mishandled and the year for the parse is changed to 1999.$$patch1-Time-7-SimFix$$Fix copy / paste error$$1
Lang-50$$FastDateFormat getDateInstance() and getDateTimeInstance() assume Locale.getDefault() won't change$$The FastDateFormat getDateInstance() and getDateTimeInstance()  methods create the HashMap key from various items including the locale. If the locale is null, then it is not made part of the key, but the stored object is created using the current default locale. If the Locale is changed subsequently, then the wrong locale is applied. Patch for test case to follow.$$patch1-Lang-50-SimFix$$Fix getLocaleKey in FastDateFormat$$1
Lang-58$$NumberUtils.createNumber throws NumberFormatException for one digit long$$NumberUtils.createNumber throws a NumberFormatException when parsing "1l", "2l" .. etc... It works fine if you try to parse "01l" or "02l".  The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for "1l"$$patch1-Lang-58-SimFix$$Fix parse error ( l - > long )$$1
Lang-60$$StrBuilder contains usages of thisBuf.length when they should use size$$While fixing LANG-294 I noticed that there are two other places in StrBuilder that reference thisBuf.length and unless I'm mistaken they shouldn't.$$patch1-Lang-60-SimFix$$Removing bad merge of one - line builder .$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch1-Lang-33-SimFix$$removed null check$$1
Lang-16$$NumberUtils does not handle upper-case hex: 0X and -0X$$NumberUtils.createNumber() should work equally for 0x1234 and 0X1234; currently 0X1234 generates a NumberFormatException Integer.decode() handles both upper and lower case hex.$$patch1-Lang-16-SimFix$$convert string to lower case since it ' s a number$$1
Lang-43$$ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes$$When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur. Example that will cause error: ExtendedMessageFormatTest.java  private static Map<String, Object> formatRegistry = new HashMap<String, Object>();         static {         formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());     }          public static void main(String[] args) {         ExtendedMessageFormat mf = new ExtendedMessageFormat("it''s a {dummy} 'test'!", formatRegistry);         String formattedPattern = mf.format(new String[] {"great"});         System.out.println(formattedPattern);     } }    The following change starting at line 421 on the 2.4 release seems to fix the problem: ExtendedMessageFormat.java CURRENT (Broken): if (escapingOn && c[start] == QUOTE) {         return appendTo == null ? null : appendTo.append(QUOTE); }  WORKING: if (escapingOn && c[start] == QUOTE) {         next(pos);         return appendTo == null ? null : appendTo.append(QUOTE); }$$patch1-Lang-43-SimFix$$don ' t append QUOTE if it is backslashed$$1
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-SimFix$$still need more matches in StringUtils$$1
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.$$patch1-Math-73-SimFix$$changed param name in BrentSolver$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-SimFix$$EigenDecompositionImpl . flipArrays ( ) needed by intel .$$0
Math-6$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.      @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }$$patch1-Math-6-SimFix$$Fix typo in BaseOptimizer . getIterations$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-SimFix$$Using lower maxUlps value in linear search$$0
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.$$patch1-Math-8-SimFix$$Fix DiscreteDistribution . sample ( )$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-SimFix$$EigenDecompositionImpl . maxIter = b2 * b1 ; LPS - 15$$0
Math-88$$Simplex Solver arrives at incorrect solution$$I have reduced the problem reported to me down to a minimal test case which I will attach.$$patch1-Math-88-SimFix$$Fixed a bug in the linear search results$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-SimFix$$Fix a false reporting of convergence$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-SimFix$$Fixed an issue with the min value being greater than the max .$$0
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-SimFix$$Fix parse error$$0
Lang-45$$WordUtils.abbreviate bug when lower is greater than str.length$$In WordUtils.abbreviate, upper is adjusted to the length of the string, then to lower. But lower is never adjusted to the length of the string, so if lower is greater than str.lengt(), upper will be too... Then, str.substring(0, upper) throw a StringIndexOutOfBoundsException The fix is to adjust lower to the length of the string$$patch1-Lang-45-SimFix$$make str . length ( ) > upper$$0
Lang-44$$NumberUtils createNumber thows a StringIndexOutOfBoundsException when only an "l" is passed in.$$Seems to be similar to LANG-300, except that if you don't place a digit in front of the "l" or "L" it throws a StringIndexOutOfBoundsException instead.$$patch1-Lang-44-SimFix$$Fix erroneous fallthrough in NumberUtils$$0
Lang-63$$DurationFormatUtils returns wrong result$$DurationFormatUtils returns wrong result.  oddly, it is only when Date is set to Dec 31, 2005 The following code will result in a String of -2 which is way off. I've tested against 2.1 and 2.2.         Calendar cal = Calendar.getInstance();         cal.set(Calendar.MONTH, Calendar.DECEMBER);         cal.set(Calendar.DAY_OF_MONTH, 31);         cal.set(Calendar.YEAR, 2005);         cal.set(Calendar.HOUR_OF_DAY, 0);         cal.set(Calendar.MINUTE, 0);         cal.set(Calendar.SECOND, 0);         cal.set(Calendar.MILLISECOND, 0);         String result = DurationFormatUtils.formatPeriod(cal.getTimeInMillis(), System.currentTimeMillis(), "MM");         System.out.println(result);$$patch1-Lang-63-SimFix$$Fix merge conflict between end and start$$0
Lang-41$$ClassUtils.getShortClassName() will not work with an array;  it seems to add a semicolon to the end.$$A semicolon is introduced into the class name at the end for all arrays... String sArray[] = new String[2]; sArray[0] = "mark"; sArray[1] = "is cool"; String simpleString = "chris"; assertEquals("String", ClassUtils.getShortClassName(simpleString, null)); assertEquals("String;", ClassUtils.getShortClassName(sArray, null));$$patch1-Lang-41-SimFix$$Fix ClassUtils . getShortClassName / getShortCanonicalName$$0
Chart-16$$Bug propgated from v1.0.5 on to present$$The method getRowCount() in class org.jfree.data.category.DefaultIntervalCategoryDataset says that it "Returns the number of series in the dataset (possibly zero)."  The implementation from v1.0.5 on no longer checks for a null condition (which would then return a zero) on the seriesKeys as it did in v1.0.4 and previous. This now throws a Null Pointer if seriesKeys never got initialized and the getRowCount() method is called.$$patch1-Chart-16-VFix$$Fix bug in chart 16$$1
Chart-16$$Bug propgated from v1.0.5 on to present$$The method getRowCount() in class org.jfree.data.category.DefaultIntervalCategoryDataset says that it "Returns the number of series in the dataset (possibly zero)."  The implementation from v1.0.5 on no longer checks for a null condition (which would then return a zero) on the seriesKeys as it did in v1.0.4 and previous. This now throws a Null Pointer if seriesKeys never got initialized and the getRowCount() method is called.$$patch2-Chart-16-VFix$$Fix bug in chart - 16$$1
Chart-26$$None$$None$$patch1-Chart-26-VFix$$Fix bug$$1
Chart-4$$None$$None$$patch1-Chart-4-VFix$$added missing patch$$1
Chart-15$$None$$None$$patch1-Chart-15-VFix$$Add NPE protection$$1
Chart-14$$None$$None$$patch1-Chart-14-VFix$$Allow null to be removed from backgroundDomainMarkers$$1
Chart-14$$None$$None$$patch2-Chart-14-VFix$$Fix bug$$1
Chart-14$$None$$None$$patch3-Chart-14-VFix$$Allow null to be removed from background domain markers$$1
Chart-14$$None$$None$$patch4-Chart-14-VFix$$Fix bug$$1
Chart-25$$None$$None$$patch1-Chart-25-VFix$$Add null check$$1
Chart-25$$None$$None$$patch2-Chart-25-VFix$$Add null check in patch2$$1
Chart-25$$None$$None$$patch3-Chart-25-VFix$$Improved patching of standard deviations with different y values .$$1
Chart-25$$None$$None$$patch4-Chart-25-VFix$$Improved patching of standard deviations$$1
Math-4$$NPE when calling SubLine.intersection() with non-intersecting lines$$When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations. The attached patch fixes both implementations and adds the required test cases.$$patch1-Math-4-VFix$$added missing if ($$1
Math-4$$NPE when calling SubLine.intersection() with non-intersecting lines$$When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations. The attached patch fixes both implementations and adds the required test cases.$$patch2-Math-4-VFix$$Add null check in patch2$$1
Lang-57$$NullPointerException in isAvailableLocale(Locale)$$FindBugs pointed out:    UwF: Field not initialized in constructor: org.apache.commons.lang.LocaleUtils.cAvailableLocaleSet cAvailableSet is used directly once in the source - and if availableLocaleSet() hasn't been called it will cause a NullPointerException.$$patch1-Lang-57-VFix$$Add null check$$1
Lang-33$$ClassUtils.toClass(Object[]) throws NPE on null array element$$see summary$$patch1-Lang-33-VFix$$removed null check$$1
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch1-Lang-20-VFix$$fixed bug$$1
Lang-20$$StringUtils.join throws NPE when toString returns null for one of objects in collection$$Try    StringUtils.join(new Object[]{         new Object() {           @Override           public String toString() {             return null;           }         }     }, ',');   ToString should probably never return null, but it does in javax.mail.internet.InternetAddress$$patch2-Lang-20-VFix$$fixed bug$$1
Lang-39$$StringUtils replaceEach - Bug or Missing Documentation$$The following Test Case for replaceEach fails with a null pointer exception. I have expected that all StringUtils methods are "null-friendly" The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null. I admit the use case is not perfect, because it is unclear what happens on the replace. I outlined three expectations in the test case, of course only one should be met. If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string  import static org.junit.Assert.assertEquals;  import org.apache.commons.lang.StringUtils; import org.junit.Test;   public class StringUtilsTest {  	@Test 	public void replaceEach(){ 		String original = "Hello World!"; 		String[] searchList = {"Hello", "World"}; 		String[] replacementList = {"Greetings", null}; 		String result = StringUtils.replaceEach(original, searchList, replacementList); 		assertEquals("Greetings !", result); 		//perhaps this is ok as well                 //assertEquals("Greetings World!", result);                 //or even 		//assertEquals("Greetings null!", result); 	}  	 }$$patch1-Lang-39-VFix$$fix bug$$1
Lang-47$$StrBuilder appendFixedWidth does not handle nulls$$Appending a null value with fixed width causes a null pointer exception if getNullText() has not been set.$$patch1-Lang-47-VFix$$Fix bug$$1
Lang-47$$StrBuilder appendFixedWidth does not handle nulls$$Appending a null value with fixed width causes a null pointer exception if getNullText() has not been set.$$patch2-Lang-47-VFix$$Fix bug$$1
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.$$patch1-Math-70-VFix$$Added a fix for null constraint$$0
Closure-62$$Column-indicating caret is sometimes not in error output$$None$$patch1-Closure-62-jMutRepair$$Fixed bug in Closure Repair fix$$1
Closure-63$$None$$None$$patch1-Closure-63-jMutRepair$$Fixed bug in Closure ' s excerpt patch ( charno = sourceExpert . length ( ) )$$1
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-jMutRepair$$Fix nullability assertion .$$1
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-jMutRepair$$Fix a bug in the patch collection$$1
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1-Math-82-jMutRepair$$Fixed epsilon$$1
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-21-jMutRepair$$Fix closure closure bug$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1-Closure-126-jMutRepair$$minimize the closure closure closure closure patch .$$0
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch1-Closure-22-jMutRepair$$JSDoc info removed from closure patch$$0
Chart-7$$None$$None$$patch1-Chart-7-jMutRepair$$Fix chart error$$0
Chart-26$$None$$None$$patch1-Chart-26-jMutRepair$$Fix chart area sizes$$0
Chart-25$$None$$None$$patch1-Chart-25-jMutRepair$$Fixed getMeanValue$$0
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.$$patch1-Math-50-jMutRepair$$Fix bug in REGULA_FALSI .$$0
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.$$patch1-Math-57-jMutRepair$$fixed bug in point comparison$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1-Math-80-jMutRepair$$Fixed issue with ping fix$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1-Math-28-jMutRepair$$Fixed the minRatioPositions for tableau$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1-Math-81-jMutRepair$$Fixed incorrect patch$$0
Math-88$$Simplex Solver arrives at incorrect solution$$I have reduced the problem reported to me down to a minimal test case which I will attach.$$patch1-Math-88-jMutRepair$$added fixed case for tableau sizes$$0
Math-52$$numerical problems in rotation creation$$building a rotation from the following vector pairs leads to NaN: u1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377 u2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10 v1 = 1, 0, 0 v2 = 0, 0, 1 The constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:  <v1'|v1'> == <u1|u1>  <v2'|v2'> == <u2|u2>  <u1 |u2>  == <v1'|v2'> Once the (v1', v2') pair has been computed, we compute the cross product:   k = (v1' - u1)^(v2' - u2) and the scalar product:   c = <k | (u1^u2)> By construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)]. c should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm. However, there are numerical problems with the vector above with the way these computations are done, as shown by the following comparisons, showing the result we get from our Java code and the result we get from manual computation with the same formulas but with enhanced precision: commons math:   k = 38514476.5,            -84.,                           -1168590144 high precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208... and it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get: commons math    c = -1.2397173627587605E20 high precision: c =  558382746168463196.7079627... We have lost ALL significant digits in cancellations, and even the sign is wrong!$$patch1-Math-52-jMutRepair$$Fixed a bug in the cross product test$$0
Math-84$$MultiDirectional optimzation loops forver if started at the correct solution$$MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.$$patch1-Math-84-jMutRepair$$added a missing if / else$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-jMutRepair$$fixed bug in 1 . 0 . 2$$0
Lang-27$$NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing "e" and "E" is passed in$$NumberUtils createNumber throws a StringIndexOutOfBoundsException instead of NumberFormatException when a String containing both possible exponent indicators is passed in. One example of such a String is "1eE".$$patch1-Lang-27-jMutRepair$$Fix incorrect patch$$0
Lang-22$$org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)$$The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator. Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method. FractionTest.java 	// additional test cases 	public void testReducedFactory_int_int() { 		// ... 		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2); 		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator()); 		assertEquals(1, f.getDenominator());  	public void testReduce() { 		// ... 		f = Fraction.getFraction(Integer.MIN_VALUE, 2); 		result = f.reduce(); 		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator()); 		assertEquals(1, result.getDenominator());$$patch1-Lang-22-jMutRepair$$Fix a bug in JMutRepair where abs ( u ) > v$$0
Closure-126$$Break in finally block isn't optimized properly$$None$$patch1_1-Closure-126-GenProgA$$Remove tryMinimizeExits from tryMinimizeExits$$1
Closure-115$$Erroneous optimization in ADVANCED_OPTIMIZATIONS mode$$None$$patch1_1-Closure-115-GenProgA$$Allow side effects for closure arguments$$1
Closure-59$$Cannot exclude globalThis checks through command line$$None$$patch1_1-Closure-59-GenProgA$$"Add "" addVerboseWarnings ( ) "" to closure compiler warnings"$$0
Closure-67$$Advanced compilations renames a function and then deletes it, leaving a reference to a renamed but non-existent function$$None$$patch1_1-Closure-67-GenProgA$$Remove unused prototype properties fix .$$0
Closure-129$$Casting a function before calling it produces bad code and breaks plugin code$$None$$patch1_1-Closure-129-GenProgA$$Remove unused old fix$$0
Closure-10$$Wrong code generated if mixing types in ternary operator$$None$$patch1_1-Closure-10-GenProgA$$Fix PeepholeFoldConstants case$$0
Closure-21$$Classify non-rightmost expressions as problematic$$None$$patch1_1-Closure-21-GenProgA$$Updated code$$0
Closure-75$$closure compiled swfobject error$$None$$patch1-Closure-75-GenProgA$$Remove ' POS ' fallthrough in isStrWhiteSpaceChar case .$$0
Closure-55$$Exception when emitting code containing getters$$None$$patch1_1-Closure-55-GenProgA$$Remove unneeded call to NodeTraversal . traverse ( ) .$$0
Closure-130$$arguments is moved to another scope$$None$$patch1_1-Closure-130-GenProgA$$Remove stray ' if ' in name . getRefs ( )$$0
Closure-112$$Template types on methods incorrectly trigger inference of a template on the class if that template type is unknown$$None$$patch1_1-Closure-112-GenProgA$$Remove hard coded resolve of template types in closure context$$0
Closure-124$$Different output from RestAPI and command line jar$$None$$patch1_1-Closure-124-GenProgA$$Remove old and unused patch .$$0
Closure-114$$Crash on the web closure compiler$$None$$patch1-Closure-114-GenProgA$$Allow closure to override local variable name$$0
Closure-78$$division by zero wrongly throws JSC_DIVIDE_BY_0_ERROR$$None$$patch1_1-Closure-78-GenProgA$$Do not increment local variable$$0
Closure-22$$Classify non-rightmost expressions as problematic$$None$$patch1_1-Closure-22-GenProgA$$Remove stray return statement from a checksideeffects patch .$$0
Chart-1$$Potential NPE in AbstractCategoryItemRender.getLegendItems()$$Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java. The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read "if (dataset == null)", not "if (dataset != null)".$$patch1-Chart-1-GenProgA$$Fix NPE in CategoryPlot$$0
Chart-3$$None$$None$$patch1_1-Chart-3-GenProgA$$Fix bug$$0
Chart-12$$Fix for MultiplePiePlot$$When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.$$patch1-Chart-12-GenProgA$$Fix NPE in MultiplePiePlot .$$0
Chart-13$$None$$None$$patch1_1-Chart-13-GenProgA$$Fix border embarrassment in JDK6$$0
Chart-25$$None$$None$$patch1_1-Chart-25-GenProgA$$Remove a unused variable$$0
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.$$patch1-Math-95-GenProgA$$Fix a warning$$0
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }$$patch1_1-Math-80-GenProgA$$Fixed erroneous loop in EigenDecompositionImpl .$$0
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR$$patch1_1-Math-28-GenProgA$$fixed a small bug$$0
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.$$patch1_1-Math-81-GenProgA$$updated EigenDecompositionImpl . a2 = b2 ;$$0
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.$$patch1-Math-31-GenProgA$$Fix continuedFraction patch$$0
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)$$patch1-Math-85-GenProgA$$Fix a bug in UnivariateRealSolverUtils$$0
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).$$patch1_1-Math-82-GenProgA$$Delete old minValue = null ; patch for linear search$$0
Math-40$$BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary$$In some cases, the aging feature in BracketingNthOrderBrentSolver fails. It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket. In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).$$patch1-Math-40-GenProgA$$fix the ReducingException$$0
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values  the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean()  instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.$$patch1-Math-2-GenProgA$$Fix issue with FuzzyKMeansClusterer empty cluster in FJ$$0
Lang-7$$NumberUtils#createNumber - bad behaviour for leading "--"$$NumberUtils#createNumber checks for a leading "--" in the string, and returns null if found. This is documented as a work round for a bug in BigDecimal. Returning nulll is contrary to the Javadoc and the behaviour for other methods which would throw NumberFormatException. It's not clear whether the BigDecimal problem still exists with recent versions of Java. However, if it does exist, then the check needs to be done for all invocations of BigDecimal, i.e. needs to be moved to createBigDecimal.$$patch1_1-Lang-7-GenProgA$$Fix a regression in NumberUtils$$0
